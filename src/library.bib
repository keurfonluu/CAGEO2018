Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rumpf2015,
abstract = {To analyse and invert refraction seismic travel time data, different approaches and techniques have been proposed. One common approach is to invert first-break travel times employing local optimization approaches. However, these approaches result in a single velocity model, and it is difficult to assess the quality and to quantify uncertainties and non-uniqueness of the found solution. To address these problems, we propose an inversion strategy relying on a global optimization approach known as particle swarm optimization. With this approach we generate an ensemble of ac- ceptable velocity models, i.e., models explaining our data equally well. We test and evaluate our approach using synthetic seismic travel times and field data collect- ed across a creeping hillslope in the Austrian Alps. Our synthetic study mimics a layered near-surface environment, including a sharp velocity increase with depth and complex refractor topography. Analysing the generated ensemble of acceptable so- lutions using different statistical measures demonstrates that our inversion strategy is able to reconstruct the input velocity model, including reasonable, quantitative estimates of uncertainty. Our field data set is inverted, employing the same strategy, and we further compare our results with the velocity model obtained by a stan- dard local optimization approach and the information from a nearby borehole. This comparison shows that both inversion strategies result in geologically reasonable models (in agreement with the borehole information). However, analysing the model variability of the ensemble generated using our global approach indicates that the result of the local optimization approach is part of this model ensemble. Our results show the benefit of employing a global inversion strategy to generate near-surface velocity models from refraction seismic data sets, especially in cases where no de- tailed apriori information regarding subsurface structures and velocity variations is available.},
author = {Rumpf, Michael and Tronicke, Jens},
nodoi = {10.1111/1365-2478.12240},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Prospecting/Rumpf, Tronicke{\_}2015.pdf:pdf},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Inversion,Seismic refraction,Uncertainty,pso},
mendeley-tags = {pso},
month = {sep},
number = {5},
pages = {1188--1197},
title = {{Assessing uncertainty in refraction seismic traveltime inversion using a global inversion strategy}},
nourl = {http://nodoi.wiley.com/10.1111/1365-2478.12240},
volume = {63},
year = {2015}
}
@inproceedings{Luu2016,
abstract = {Seismic traveltime tomography is an optimization problem that requires large computational efforts. Therefore, linearized techniques are commonly used for their low computational cost. These local optimization methods are likely to get trapped in a local minimum as they critically depend on the initial model. On the other hand, common global optimization techniques such as Genetic Algorithm (GA) or Simulated Annealing (SA) are insensitive to the initial model but are computationally expensive and require many controlling parameters. Particle Swarm Optimization (PSO) is a rather new global optimization approach with few parameters that has shown excellent convergence rates and is straightforwardly parallelizable, allowing a good distribution of the workload. However, while it can traverse several local minima of the evaluated misfit function, classical implementation of PSO can get trapped in local minima at later iterations as particles inertia dim. We propose a Competitive PSO (CPSO) to allow "worst" particles to explore the model parameter space and eventually find a better minimum. A tomography algorithm based on CPSO is successfully applied on a 3D synthetic case corresponding to a typical calibration shot geometry in a hydraulic fracturing context.},
author = {Luu, Keurfon and Noble, Mark and Gesret, Alexandrine},
booktitle = {SEG Technical Program Expanded Abstracts 2016},
nodoi = {10.1190/segam2016-13840267.1},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2016/Luu, Noble, Gesret{\_}2016.pdf:pdf},
keywords = {cpso,inversion,microseismic,pso},
mendeley-tags = {cpso,inversion,microseismic,pso},
month = {sep},
pages = {2740--2744},
publisher = {Society of Exploration Geophysicists},
title = {{A competitive particle swarm optimization for nonlinear first arrival traveltime tomography}},
nourl = {http://library.seg.org/nodoi/10.1190/segam2016-13840267.1},
year = {2016}
}
@inproceedings{Shi1998,
abstract = {Evolutionary computation techniques, genetic algorithms, evolutionary strategies and genetic programming are motivated by the evolution of nature. A population of individuals, which encode the problem solutions are manipulated according to the rule of survival of the fittest through {\&}ldquo;genetic{\&}rdquo; operations, such as mutation, crossover and reproduction. A best solution is evolved through the generations. In contrast to evolutionary computation techniques, Eberhart and Kennedy developed a different algorithm through simulating social behavior (R.C. Eberhart et al., 1996; R.C. Eberhart and J. Kennedy, 1996; J. Kennedy and R.C. Eberhart, 1995; J. Kennedy, 1997). As in other algorithms, a population of individuals exists. This algorithm is called particle swarm optimization (PSO) since it resembles a school of flying birds. In a particle swarm optimizer, instead of using genetic operators, these individuals are {\&}ldquo;evolved{\&}rdquo; by cooperation and competition among the individuals themselves through generations. Each particle adjusts its flying according to its own flying experience and its companions' flying experience. We introduce a new parameter, called inertia weight, into the original particle swarm optimizer. Simulations have been done to illustrate the significant and effective impact of this new parameter on the particle swarm optimizer},
author = {Shi, Yuhui and Eberhart, R C},
booktitle = {1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)},
nodoi = {10.1109/ICEC.1998.699146},
file = {:D$\backslash$:/OneDrive/Articles/1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat.98TH8360)/Shi, Eberhart{\_}1998.pdf:pdf},
isbn = {0-7803-4869-9},
issn = {1549-9596},
keywords = {competition,cooperation,evolutionary computation techniques,evolutionary strategies,flying birds,flying experience,genetic algorithms,genetic programming,inertia weight,iterative methods,modified particle swarm optimizer,particle swarm optimization,pso,search problems,social behavior simulation,survival of the fittest},
mendeley-tags = {pso},
pages = {69--73},
pmid = {19366194},
publisher = {IEEE},
title = {{A modified particle swarm optimizer}},
nourl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=699146 http://ieeexplore.ieee.org/document/699146/},
year = {1998}
}
@article{Hansen2011a,
abstract = {This paper investigates the behavior of PSO (particle swarm optimization) and CMA-ES (covariance matrix adaptation evolution strategy) on ill-conditioned functions. The paper also highlights momentum as important common concept used in both algorithms and reviews important invariance properties. On separable, ill-conditioned functions, PSO performs very well and outperforms CMA-ES by a factor of up to five. On the same but rotated functions, the performance of CMA-ES is unchanged, while the performance of PSO declines dramatically: on non-separable, ill-conditioned functions we find the search costs (number of function evaluations) of PSO increasing roughly proportional with the condition number and CMA-ES outperforms PSO by orders of magnitude. The strong dependency of PSO on rotations originates from random events that are only independent within the given coordinate system. The CMA-ES adapts the coordinate system where the independent events take place and is rotational invariant. We argue that invariance properties, like rotational invariance, are desirable, because they increase the predictive power of performance results by inducing problem equivalence classes. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Hansen, Nikolaus and Ros, Raymond and Mauny, Nikolas and Schoenauer, Marc and Auger, Anne},
nodoi = {10.1016/j.asoc.2011.03.001},
file = {:D$\backslash$:/OneDrive/Articles/Applied Soft Computing Journal/Hansen et al.{\_}2011.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {CMA-ES,Covariance matrix adaptation,Evolution strategy,Ill-conditioned problems,Invariance,Non-separable problems,PSO,Particle swarm optimization,Performance assessment,cmaes},
mendeley-tags = {cmaes},
number = {8},
pages = {5755--5769},
title = {{Impacts of invariance in search: When CMA-ES and PSO face ill-conditioned and non-separable problems}},
volume = {11},
year = {2011}
}
@article{Maeda1985,
author = {Maeda, Naoki},
nodoi = {10.4294/zisin1948.38.3_365},
file = {:D$\backslash$:/OneDrive/Articles/Zisin (Journal of the Seismological Society of Japan. 2nd ser.)/Maeda{\_}1985.pdf:pdf},
issn = {0037-1114},
journal = {Zisin (Journal of the Seismological Society of Japan. 2nd ser.)},
number = {3},
pages = {365--379},
title = {{A Method for Reading and Checking Phase Time in Auto-Processing System of Seismic Wave Data}},
nourl = {https://scholar.google.com/scholar{\_}lookup?title=A method for reading and checking phase times in auto-processing system of seismic data{\&}author=N. Maeda{\&}publication{\_}year=1985{\&}journal=Zisin{\%}3DJishin{\&}volume=38{\&}pages=365-379 https://www.jstage.jst.go.jp/artic},
volume = {38},
year = {1985}
}
@article{Eberhart2001a,
abstract = {A fuzzy system is implemented to dynamically adapt the inertia weight of the particle swarm optimization algorithm (PSO). Three benchmark functions with asymmetric initial range settings are selected as the test functions. The same fuzzy system has been applied to all three test functions with different dimensions. The experimental results illustrate that the fuzzy adaptive PSO is a promising optimization method, which is especially useful for optimization problems with a dynamic environment},
author = {Eberhart, R.C.},
nodoi = {10.1109/CEC.2001.934377},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)/Eberhart{\_}2001.pdf:pdf},
isbn = {0-7803-6657-3},
issn = {1879-2472},
journal = {Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)},
keywords = {pso},
mendeley-tags = {pso},
pages = {101--106},
pmid = {22192158},
title = {{Fuzzy adaptive particle swarm optimization}},
nourl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=934377},
volume = {1},
year = {2001}
}
@article{Baillard2014,
abstract = {We present an automatic P- and S-wave onset-picking algorithm, using kurtosis-derived characteristic functions (CF) and eigenvalue decompositions on three-component seismic data. We modified the kurtosis CF (Saragiotis et al., 2002) to improve pick precision by computing the CF over several frequency bandwidths, window sizes, and smoothing parameters. Once phases are picked, our algorithm determines the onset type (P or S) using polarization parameters, removes bad picks using a clustering procedure and the signal-to-noise ratio (SNR) and assigns a pick quality index based on the SNR. We tested our algorithm on data from two different networks: (1) a 30-station, 100x100 km array of mostly onland wideband seismometers in a subduction context and (2) a four-station, 7x4 km array of ocean-bottom seismometers over a midocean ridge volcano. We compared picks from the automatic algorithm with manual and short-term average/long-term average (STA/LTA)-based automatic picks on subsets of each dataset. For the larger array, the automatic algorithm resulted in more locations than manual picking (133 versus 93 locations out of 163 total events detected), picking as many P onsets and twice as many S onsets as with manual picking or the STA/LTA algorithm. The difference between manual and automatic pick times for P-wave onsets was 0.01{\{}+/-{\}}0.08 s overall, compared with -0.18{\{}+/-{\}}0.19 s using the STA/LTA picker. For S-wave onsets, the difference was -0.09{\{}+/-{\}}0.23 s, which is comparable to the STA/LTA picker, but our picker provided nearly twice as many picks. The pick accuracy was constant over the range of event magnitudes (0.7-3.7 Ml). For the smaller array, the time difference between our algorithm and manual picks is 0.04{\{}+/-{\}}0.17 s for P waves and 0.07{\{}+/-{\}}0.08 s for S waves. Misfit between the automatic and manual picks is significantly lower using our procedure than using the STA/LTA algorithm.},
author = {Baillard, Christian and Crawford, Wayne C. and Ballu, Val{\'{e}}rie and Hibert, Cl{\'{e}}ment and Mangeney, Anne},
nodoi = {10.1785/0120120347},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Baillard et al.{\_}2014.pdf:pdf},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
keywords = {picking},
mendeley-tags = {picking},
month = {feb},
number = {1},
pages = {394--409},
title = {{An Automatic Kurtosis-Based P- and S-Phase Picker Designed for Local Seismic Networks}},
nourl = {http://www.bssaonline.org/cgi/nodoi/10.1785/0120120347},
volume = {104},
year = {2014}
}
@article{Poormirzaee2015,
author = {Poormirzaee, Rashed and Moghadam, Rasoul Hamidzadeh and Zarean, Ahmad},
nodoi = {10.1007/s12517-014-1662-x},
file = {:D$\backslash$:/OneDrive/Articles/Arabian Journal of Geosciences/Poormirzaee, Moghadam, Zarean{\_}2015.pdf:pdf},
issn = {1866-7511},
journal = {Arabian Journal of Geosciences},
keywords = {Inversion,P-wave velocity,PSO,Seismic refraction data,pso},
mendeley-tags = {pso},
month = {aug},
number = {8},
pages = {5981--5989},
title = {{Inversion seismic refraction data using particle swarm optimization: a case study of Tabriz, Iran}},
nourl = {http://link.springer.com/10.1007/s12517-014-1662-x},
volume = {8},
year = {2015}
}
@article{Pugh2016,
abstract = {Earthquake source inversion is highly dependent on location determination and velocity models. Uncertainties in both the model parameters and the observations need to be rigorously incorporated into an inversion approach. Here, we show a probabilistic Bayesian method that allows formal inclusion of the uncertainties in the moment tensor inversion. This method allows the combination of different sets of far-field observations, such as P-wave and S-wave polarities and amplitude ratios, into one inversion. Additional observations can be included by deriving a suitable likelihood function from the uncertainties. This inversion produces samples from the source posterior probability distribution, including a best-fitting solution for the source mechanism and associated probability. The inversion can be constrained to the double-couple space or allowed to explore the gamut of moment tensor solutions, allowing volumetric and other non-double-couple components. The posterior probability of the double-couple and full moment tensor source models can be evaluated from the Bayesian evidence, using samples from the likelihood distributions for the two source models, producing an estimate of whether or not a source is double-couple. Such an approach is ideally suited to microseismic studies where there are many sources of uncertainty and it is often difficult to produce reliability estimates of the source mechanism, although this can be true of many other cases. Using full-waveform synthetic seismograms, we also show the effects of noise, location, network distribution and velocity model uncertainty on the source probability density function. The noise has the largest effect on the results, especially as it can affect other parts of the event processing. This uncertainty can lead to erroneous non-double-couple source probability distributions, even when no other uncertainties exist. Although including amplitude ratios can improve the constraint on the source probability distribution, the measurements are often systematically affected by noise, leading to deviation from their noise-free true values and consequently adversely affecting the source probability distribution, especially for the full moment tensor model. As an example of the application of this method, four events from the Krafla volcano in Iceland are inverted, which show clear differentiation between non-double-couple and double-couple sources, reflected in the posterior probability distributions for the source models.},
author = {Pugh, D. J. and White, R. S. and Christie, P. A. F.},
nodoi = {10.1093/gji/ggw186},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Pugh, White, Christie{\_}2016.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Earthquake source observations,Numerical solutions,Probability distributions,Volcano seismology,moment tensor},
mendeley-tags = {moment tensor},
month = {aug},
number = {2},
pages = {1009--1038},
title = {{A Bayesian method for microseismic source inversion}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggw186},
volume = {206},
year = {2016}
}
@article{Got1994,
abstract = {Dense microearthquake swarms occur in the upper south flank of Kilauea, providing multiplets composed of hundreds of events. The similarity of their waveforms and the quality of the data have been sufficient to provide accurate relative relocations of their hypocenters. A simple and efficient method has been developed which allowed the relative relocation of more than 250 events with an average precision of about 50 m horizontally and 75 m vertically. Relocation of these events greatly improves the definition of the seismic image of the fault that generates them. Indeed, relative relocations define a plane dipping about 6° northward, although corresponding absolute locations are widely dispersed in the swarm. A composite focal mechanism, built from events providing a correct spatial sampling of the multiplet, also gives a well-constrained northward dip of about 5° to the near-horizontal plane. This technique thus collapses the clouds of hypocenters of single-event locations to a plane coinciding with the slip plane revealed by previous focal mechanism studies. We cannot conclude that all south flank earthquakes collapse to a single plane. There may locally be several planes, perhaps with different dips and depths throughout the south flank volume. The 6° northward-dipping plane we found is too steep to represent the overall flexure of the oceanic crust under the load of the island of Hawaii. This plane is probably an important feature that characterizes the basal slip layer below the upper south flank of Kilauea volcano. Differences in seismicity rate and surface deformations between the upper and lower south flank could be related to the geometry of this deep fault plane. The present work illustrates how high precision relative relocations of similar events in dense swarms, combined with the analysis of geodetic measurements, can help to describe deep fault plane geometry. Systematic selection and extensive relative relocation of similar earthquakes could be attempted in other well-instrumented, highly seismic areas to provide reliable basic information, especially useful for understanding of earthquake generation processes.},
author = {Got, J.-L. and Fr{\'{e}}chet, J. and Klein, Fred W.},
nodoi = {10.1029/94JB00577},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research/Got, Fr{\'{e}}chet, Klein{\_}1994.pdf:pdf},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
number = {B8},
pages = {15375--15386},
title = {{Deep fault plane geometry inferred from multiplet relative relocation beneath the south flank of Kilauea}},
volume = {99},
year = {1994}
}
@article{Schaff2004,
abstract = {Earthquake location using relative arrival time measurements can lead to dramatically reduced location errors and a view of fault-zone processes with unprecedented detail. There are two principal reasons why this approach reduces location errors. The first is that the use of differenced arrival times to solve for the vector separation of earthquakes removes from the earthquake location problem much of the error due to unmodeled velocity structure. The second reason, on which we focus in this article, is that waveform cross correlation can substantially reduce measurement error. While cross correlation has long been used to determine relative arrival times with subsample precision, we extend correlation measurements to less similar waveforms, and we introduce a general quantitative means to assess when correlation data provide an improvement over catalog phase picks. We apply the technique to local earthquake data from the Calaveras Fault in northern California. Tests for an example streak of 243 earthquakes demonstrate that relative arrival times with normalized cross correlation coefficients as low as similar to70{\%}, interevent separation distances as large as to 2 km, and magnitudes up to 3.5 as recorded on the Northern California Seismic Network are more precise than relative arrival times determined from catalog phase data. Also discussed are improvements made to the correlation technique itself. We find that for large time offsets, our implementation of time-domain cross correlation is often more robust and that it recovers more observations than the cross spectral approach. Longer time windows give better results than shorter ones. Finally, we explain how thresholds and empirical weighting functions may be derived to optimize the location procedure for any given region of interest, taking advantage of the respective strengths of diverse correlation and catalog phase data on different length scales.},
author = {Schaff, David P. and Bokelmann, G{\"{o}}tz H R and Ellsworth, William L. and Zanzerkia, Eva and Waldhauser, Felix and Beroza, Gregory C.},
nodoi = {10.1785/0120020238},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Schaff et al.{\_}2004.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
number = {2},
pages = {705--721},
title = {{Optimizing correlation techniques for improved earthquake location}},
volume = {94},
year = {2004}
}
@incollection{Sambridge2011,
abstract = {Monte Carlo method. A computational technique making use of random numbers to solve problems that are either probabilistic or deterministic in nature. Named after the famous Casino in Monaco. Monte Carlo inversion method. A method for sampling a parameter space of variables representing unknowns, governed by probabilistic rules. Markov chain Monte Carlo (McMC). A probabilistic method for generating vectors or parameter variables whose values follow a prescribed density function.},
author = {Sambridge, Malcolm and Gallagher, Kerry},
booktitle = {Encyclopedia of Solid Earth Geophysics},
nodoi = {10.1007/978-90-481-8702-7_192},
file = {:D$\backslash$:/OneDrive/Articles/Encyclopedia of Solid Earth Geophysics/Sambridge, Gallagher{\_}2011.pdf:pdf},
isbn = {978-90-481-8701-0},
pages = {639--644},
title = {{Inverse Theory, Monte Carlo Method}},
nourl = {http://link.springer.com/10.1007/978-90-481-8702-7{\_}192},
volume = {1},
year = {2011}
}
@article{Hingee2011,
abstract = {There is significant seismic activity in the region around Australia, largely due to the plate boundaries to the north and to the east of the mainland. This activity results in serious seismic and tsunami hazard in the coastal areas of Australia. Hence seismicity is and will be monitored in real time by Geoscience Australia (GA), which uses a network of permanent broadband seismometers. Seismic moment tensor (MT) solutions are currently determined using 1-D, radially symmetric models of Earth and this requires augmentation by recording stations located outside of Australia. A 3-D model of the Australian continent developed recently using full waveform tomography now offers the opportunity to significantly improve the determination of MT solutions of earthquakes from tectonically active regions. A complete-waveform, time-domain MT inversion method has been developed using a point-source approximation. A series of synthetic tests using first a 1-D and then a 3-D structural model has been performed. The feasibility of deploying 3-D versus 1-D Earth structure for the inversion of seismic data has been studied and the advantages of using the 3-D structural model were illustrated with examples. The 3-D model is superior to the 1-D model, as a number of sensitivity tests show. The ultimate goal of this work is an automated MT inversion system in Australia relying on GA and other international stations, although more work remains to be done before the full implementation of such a scheme in real time.},
author = {Hingee, Myall and Tkal{\v{c}}i{\'{c}}, Hrvoje and Fichtner, Andreas and Sambridge, Malcolm},
nodoi = {10.1111/j.1365-246X.2010.04897.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Hingee et al.{\_}2011.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Early warning,Earthquake source observations,Wave propagation},
month = {feb},
number = {2},
pages = {949--964},
title = {{Seismic moment tensor inversion using a 3-D structural model: applications for the Australian region}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.2010.04897.x},
volume = {184},
year = {2011}
}
@article{Buchen1996,
abstract = {The theory of Love- and Rayleigh-wave dispersion for plane-layered earth models has undergone a number of developments since the initial work of Thomson and Haskell. Most of these were concerned with computational difficulties associated with numerical overflow and loss of precision at high frequencies in the original Thomson-Haskell formalism. Several seemingly distinct approaches have been followed, including the delta matrix, reduced delta matrix, Schwab-Knopoff, fast Schwab-Knopoff, Kennett's Reflection-Transmission Matrix and Abo-Zena methods. This paper analyses all these methods in detail and finds explicit transformations connecting them. It is shown that they are essentially equivalent and, contrary to some claims made, each solves the loss of precision problem equally well. This is demonstrated both theoretically and computationally. By extracting the best computational features of the various methods, we develop a new algorithm (sec Appendix A5), called the fast delta matrix algorithm. To date, this is the simplest and most efficient algorithm for surface-wave dispersion computations (see Fig. 4). The theory given in this paper provides a complete review of the principal methods developed for Love- and Rayleigh-wave dispersion of free modes in plane-layered perfectly elastic, isotropic earth models and puts to rest controversies that have arisen with regard to computational stability.},
author = {Buchen, P W and Ben-Hador, R.},
nodoi = {10.1111/j.1365-246X.1996.tb05642.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Buchen, Ben-Hador{\_}1996.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {elastic-wave theory,guided waves,love waves,rayleigh waves,surface,surface wave,thomson haskell},
mendeley-tags = {surface wave,thomson haskell},
month = {mar},
number = {3},
pages = {869--887},
title = {{Free-mode surface-wave computations}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.1996.tb05642.x},
volume = {124},
year = {1996}
}
@article{Sambridge2014,
abstract = {Non-linear inverse problems in the geosciences often involve probabilistic sampling of multimodal density functions or global optimization and sometimes both. Efficient algorithmic tools for carrying out sampling or optimization in challenging cases are of major interest. Here results are presented of some numerical experiments with a technique, known as Parallel Tempering, which originated in the field of computational statistics but is finding increasing numbers of applications in fields ranging from Chemical Physics to Astronomy. To date, experience in use of Parallel Tempering within earth sciences problems is very limited. In this paper, we describe Parallel Tempering and compare it to related methods of Simulated Annealing and Simulated Tempering for optimization and sampling, respectively. A key feature of Parallel Tempering is that it satisfies the detailed balance condition required for convergence of Markov chain Monte Carlo (McMC) algorithms while improving the efficiency of probabilistic sampling. Numerical results are presented on use of Parallel Tempering for trans-dimensional inversion of synthetic seismic receiver functions and also the simultaneous fitting of multiple receiver functions using global optimization. These suggest that its use can significantly accelerate sampling algorithms and improve exploration of parameter space in optimization. Parallel Tempering is a meta-algorithm which may be used together with many existing McMC sampling and direct search optimization techniques. It's generality and demonstrated performance suggests that there is significant},
author = {Sambridge, Malcolm},
nodoi = {10.1093/gji/ggt342},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Sambridge{\_}2014.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Inverse theory,Numerical solutions,sa},
mendeley-tags = {sa},
month = {jan},
number = {1},
pages = {357--374},
title = {{A Parallel Tempering algorithm for probabilistic sampling and multimodal optimization}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggt342},
volume = {196},
year = {2014}
}
@article{Hansen2011,
abstract = {CMA Tutorial},
archivePrefix = {arXiv},
arxivId = {1604.00772},
author = {Hansen, Nikolaus},
nodoi = {10.1007/11007937_4},
eprint = {1604.00772},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Hansen{\_}2011.pdf:pdf},
isbn = {3540290060},
issn = {14349922},
keywords = {cmaes},
mendeley-tags = {cmaes},
number = {2006},
pages = {1--34},
title = {{The CMA evolution strategy: A tutorial}},
nourl = {http://www.lri.fr/{~}hansen/cmatutorial110628.pdf},
volume = {102},
year = {2011}
}
@inproceedings{Shahzad2009,
abstract = {This paper presents an Opposition-based PSO(OVCPSO) which uses Velocity Clamping to accelerate its convergence speed and to avoid premature convergence of algorithm. Probabilistic opposition-based learning for particles has been used in the proposed method which uses velocity clamping to control the speed and direction of particles. Experiments have been performed upon various well known benchmark optimization problems and results have shown that OVCPSO can deal with difficult unimodal and multimodal optimization problems efficiently and effectively. The numbers of function calls (NFC) are significantly less than other PSO variants i.e. basic PSO with inertia weight, PSO with inertia weight and velocity clamping (VCPSO) and opposition based PSO with Cauchy Mutation (OPSOCM).},
author = {Shahzad, Farrukh and Baig, A. Rauf and Masood, Sohail and Kamran, Muhammad and Naveed, Nawazish},
booktitle = {Advances in Computational Intelligence},
nodoi = {10.1007/978-3-642-03156-4},
file = {:D$\backslash$:/OneDrive/Articles/Advances in Computational Intelligence/Shahzad et al.{\_}2009.pdf:pdf},
isbn = {978-3-642-03156-4},
keywords = {opposite number,optimization,pso,swarm intelligence},
number = {1},
pages = {339--348},
title = {{Opposition-Based Particle Swarm Optimization with Velocity Clamping (OVCPSO)}},
nourl = {http://link.springer.com/10.1007/978-3-642-03156-4{\_}34},
year = {2009}
}
@article{Napoles2012,
abstract = {Particle Swarm Optimization (PSO) is a bioinspired meta–heuristic for solving complex global optimization problems. In standard PSO, the particle swarm frequently gets attracted by suboptimal solutions, causing premature convergence of the algorithm and swarm stagnation. Once the particles have been attracted to a local optimum, they continue the search process within a minuscule region of the solution space, and escaping from this local optimum may be difficult. This paper presents a modified variant of constricted PSO that uses random samples in variable neighborhoods for dispersing the swarm whenever a premature convergence (or stagnation) state is detected, offering an escaping alternative from local optima. The performance of the proposed algorithm is discussed and experimental results show its ability to approximate to the global minimum in each of the nine well–known studied benchmark functions.},
author = {N{\'{a}}poles, Gonzalo and Grau, Isel and Bello, Rafael},
nodoi = {10.17562/PB-46-1},
file = {:D$\backslash$:/OneDrive/Articles/Polibits/N{\'{a}}poles, Grau, Bello{\_}2012.pdf:pdf},
isbn = {1870-9044},
issn = {2395-8618},
journal = {Polibits},
keywords = {global optima,local optima,particle swarm optimization,premature convergence,pso,random samples,variable neighborhoods},
mendeley-tags = {pso},
month = {dec},
number = {46},
pages = {5--11},
title = {{Constricted Particle Swarm Optimization based Algorithm for Global Optimization}},
nourl = {http://www.polibits.cidetec.ipn.mx/ojs/index.php/polibits/article/view/1788},
volume = {46},
year = {2012}
}
@article{Rawlinson2010,
abstract = {The goal of this paper is to provide an overview of the current state of the art in seismic tomography, and trace its origins from pioneering work in the early 1970s to its present status as the pre-eminent tool for imaging the Earth's interior at a variety of scales. Due to length limitations, we cannot hope to cover every aspect of this diverse topic or include mathematical derivations of the underlying principles; rather, we will provide a largely descriptive coverage of the methodology that is targeted at readers not intimately familiar with the topic. The relative merits of local versus global parameterization, ray tracing versus wavefront tracking, backprojection versus gradient based inversion and synthetic testing versus model covariance are explored. A variety of key application areas are also discussed, including body wave traveltime tomography, surface wave tomography, attenuation tomography and ambient noise tomography. Established and emerging trends, many of which are driven by the ongoing rapid increases in available computing power, will also be examined, including finite frequency tomography, full waveform tomography and joint tomography using multiple datasets. Several practical applications of seismic tomography, including body wave traveltime, attenuation and surface waveform, are presented in order to reinforce prior discussion of theory. ?? 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1402.3243},
author = {Rawlinson, N. and Pozgay, S. and Fishwick, S.},
nodoi = {10.1016/j.pepi.2009.10.002},
eprint = {1402.3243},
file = {:D$\backslash$:/OneDrive/Articles/Physics of the Earth and Planetary Interiors/Rawlinson, Pozgay, Fishwick{\_}2010.pdf:pdf},
isbn = {0148-0227},
issn = {00319201},
journal = {Physics of the Earth and Planetary Interiors},
keywords = {Body wave,Earth structure,Inversion,Ray tracing,Seismic tomography,Surface wave},
month = {feb},
number = {3-4},
pages = {101--135},
pmid = {24553238},
title = {{Seismic tomography: A window into deep Earth}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/S0031920109002106},
volume = {178},
year = {2010}
}
@inproceedings{Erbas2007,
abstract = {Generating multiple history-matched reservoir models by stochastic sampling to quantify the uncertainty in oil recovery predictions has recently aroused interest in the industry. Coupling a stochastic sampling algorithm with a Bayesian analysis potentially allows incorporation of all sources of uncertainties including data, simulation and interpolation errors. However, the accuracy of the uncertainty estimations strongly depends on the sampling performance. In order to improve the robustness of the coupled Bayesian methodology, the factors that affect the accuracy of the estimations must be examined. This paper investigates how different sampling strategies affect the estimation of uncertainty in prediction of reservoir production. The sampling strategy involves the choice of algorithm and selection of algorithm parameters in sampling the high-dimensional parameter space. We present examples of using both the Neighbourhood Algorithm (NA) and a Genetic Algorithm (GA) to generate history-matched reservoir models for a real field case from the North Sea.},
author = {Erbas, Demet and Christie, Michael A.},
booktitle = {SPE Reservoir Simulation Symposium},
nodoi = {10.2118/106229-MS},
file = {:D$\backslash$:/OneDrive/Articles/SPE Reservoir Simulation Symposium/Erbas, Christie{\_}2007.pdf:pdf},
keywords = {uncertainty quantification},
mendeley-tags = {uncertainty quantification},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Effect of Sampling Strategies on Prediction Uncertainty Estimation}},
nourl = {http://www.onepetro.org/nodoi/10.2118/106229-MS},
year = {2007}
}
@article{Jones2008,
abstract = {Acoustic emission (AE) monitoring is a non-invasive method of monitoring fracturing both in situ, and in experimental rock deformation studies. Until recently, the major impediment for imaging brittle failure within a rock mass is the accuracy at which the hypocenters may be located. However, recent advances in the location of regional scale earthquakes have successfully reduced hypocentral uncertainties by an order of magnitude. The least-squares Geiger, master event relocation, and double difference methods have been considered in a series of synthetic experiments which investigate their ability to resolve AE hypocentral locations. The effect of AE hypocenter location accuracy due to seismic velocity perturbations, uncertainty in the first arrival pick, array geometry and the inversion of a seismically anisotropic structure with an isotropic velocity model were tested. Hypocenters determined using the Geiger procedure for a homogeneous, isotropic sample with a known velocity model gave a RMS error for the hypocenter locations of 2.6 mm; in contrast the double difference method is capable of reducing the location error of these hypocenters by an order of magnitude. We test uncertainties in velocity model of up to +/- 10{\%} and show that the double difference method can attain the same RMS error as using the standard Geiger procedure with a known velocity model. The double difference method is also capable of precise locations even in a 40{\%} anisotropic velocity structure using an isotropic model for location and attains a RMS mislocation error of 2.6 mm that is comparable to a RMS mislocation error produced with an isotropic known velocity model using the Geiger approach. We test the effect of sensor geometry on location accuracy and find that, even when sensors are missing, the double difference method is capable of a 1.43 mm total RMS mislocation compared to 4.58 mm for the Geiger method. The accuracy of automatic picking algorithms used for AE studies is +/- 0.5 mu s (1 time sample when the sampling rate is 0.2 mu s). We investigate how AE locations are effected by the accuracy of first arrival picking by randomly delaying the actual first arrival by up to 5 time samples. We find that even when noise levels are set to 5 time samples the double difference method successfully relocates the synthetic AE.},
author = {Jones, G. A. and Nippress, S. E J and Rietbrock, A. and Reyes-Montes, J. M.},
nodoi = {10.1007/s00024-008-0303-2},
file = {:D$\backslash$:/OneDrive/Articles/Pure and Applied Geophysics/Jones et al.{\_}2008.pdf:pdf},
isbn = {0033-4553$\backslash$r1420-9136},
issn = {00334553},
journal = {Pure and Applied Geophysics},
keywords = {Acoustic emission,Earthquake,Relocation,Seismic anisotropy,location},
mendeley-tags = {location},
number = {2},
pages = {235--254},
pmid = {22311540},
title = {{Accurate location of synthetic acoustic emissions and location sensitivity to relocation methods, velocity perturbations, and seismic anisotropy}},
volume = {165},
year = {2008}
}
@article{Ross2014,
abstract = {We develop a set of algorithms for automatic detection and picking of direct P and S waves, as well as fault zone head waves (FZHW), generated by earthquakes on faults that separate different lithologies and recorded by local seismic networks. The S-wave picks are performed using polarization analysis and related filters to remove P-wave energy from the seismograms, and utilize STA/LTA and kurtosis detectors in tandem to lock on the phase arrival. The early portions of P waveforms are processed with STA/LTA, kurtosis and skewness detectors for possible first-arriving FZHW. Identification and picking of direct P and FZHW is performed by a multistage algorithm that accounts for basic characteristics (motion polarities, time difference, sharpness and amplitudes) of the two phases. The algorithm is shown to perform well on synthetic seismograms produced by a model with a velocity contrast across the fault, and observed data generated by earthquakes along the Parkfield section of the San Andreas fault and the Hayward fault. The developed techniques can be used for systematic processing of large seismic waveform data sets recorded near major faults.},
author = {Ross, Z. E. and Ben-Zion, Y.},
nodoi = {10.1093/gji/ggu267},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Ross, Ben-Zion{\_}2014.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Body waves,Earthquake source observations,Interface waves,Time-series analysis,Wave propagation,picking},
mendeley-tags = {picking},
month = {aug},
number = {1},
pages = {368--381},
title = {{Automatic picking of direct P, S seismic phases and fault zone head waves}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggu267},
volume = {199},
year = {2014}
}
@article{Leys2013,
abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software. ?? 2013 Elsevier Inc.},
author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
nodoi = {10.1016/j.jesp.2013.03.013},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Experimental Social Psychology/Leys et al.{\_}2013.pdf:pdf},
isbn = {0022-1031},
issn = {00221031},
journal = {Journal of Experimental Social Psychology},
keywords = {MAD,Median absolute deviation,Outlier,outliers},
mendeley-tags = {outliers},
number = {4},
pages = {764--766},
pmid = {20228874},
publisher = {Elsevier Inc.},
title = {{Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median}},
nourl = {http://dx.nodoi.org/10.1016/j.jesp.2013.03.013},
volume = {49},
year = {2013}
}
@incollection{Alvarez-Benitez2005a,
abstract = {In extending the Particle Swarm Optimisation methodology tomulti-objective problems it is unclear how global guidesfor particles should be selected. Previous work hasrelied on metric information in objective space, althoughthis is at variance with the notion of dominance which isused to assess the quality of solutions. Here we proposemethods based exclusively on dominance for selectingguides from a non-dominated archive. The methods are evaluated on standard test problems and we find thatprobabilistic selection favouring archival particles thatdominate few particles provides good convergence towards$\backslash$nand coverage of the Pareto front. We demonstrate that thescheme is robust to changes in objective scaling. Wepropose and evaluate methods for confining particles to the feasible region, and find that allowing particles toexplore regions close to the constraint boundaries isimportant to ensure convergence to the Pareto front.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Alvarez-Benitez, Julio E and Everson, Richard M and Fieldsend, Jonathan E},
booktitle = {Evolutionary Multi-Criterion Optimization},
nodoi = {10.1007/978-3-540-31880-4_32},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/OneDrive/Articles/Evolutionary Multi-Criterion Optimization/Alvarez-Benitez, Everson, Fieldsend{\_}2005.pdf:pdf},
isbn = {978-3-540-24983-2},
issn = {03029743},
keywords = {pso},
mendeley-tags = {pso},
pages = {459--473},
pmid = {25246403},
title = {{A MOPSO Algorithm Based Exclusively on Pareto Dominance Concepts}},
nourl = {http://dx.nodoi.org/10.1007/978-3-540-31880-4{\_}32 http://link.springer.com/10.1007/978-3-540-31880-4{\_}32},
volume = {3410},
year = {2005}
}
@article{Wilken2012,
abstract = {We investigate different aspects concerning the application of swarm intelligence optimization to the inversion of Scholte-wave phase-slowness frequency (pf) spectra with respect to shear wave velocity structure. Besides human influence due to the dependence on a priori information for starting models and interpretation of pf spectra as well as noise, the model resolution of the inversion problem is strongly influenced by the multimodality of the misfit function. We thus tested the efficiency of global, stochastic optimization approaches with focus on swarm intelligence methods that can explore the multiple minima of the misfit function. A comparison among different PSO schemes by applying them to synthetic Scholte-wave spectra led to a hybrid of Particle Swarm Optimization and Downhill Simplex providing the best resolution of inverted shear wave velocity depth models. The results showed a very low spread of best fitting solutions of 7 per cent in shear wave velocity and an average of 9 per cent for noisy synthetic data and a very good fit to the true synthetic model used for computation of the input data. To classify this method we also compared the probability of finding a good fit in synthetic spectra inversion with that of Evolutionary Algorithm, Simulated Annealing, Neighbourhood Algorithm and Artificial Bee Colony algorithm. Again the hybrid optimization scheme showed its predominance. The usage of stochastic algorithms furthermore allowed a new way of misfit definition in terms of dispersion curve slowness residuals making the inversion scheme independent on Scholte-wave mode identification and allowing joint inversion of fundamental mode and higher mode information. Finally we used the hybrid optimization scheme and the misfit calculation for the inversion of 2-D shear wave velocity profiles from two locations in the North- and Baltic Sea. The models show acceptable resolution and a very good structural correlation to high resolution reflection seismic data.},
author = {Wilken, D. and Rabbel, W.},
nodoi = {10.1111/j.1365-246X.2012.05500.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Wilken, Rabbel{\_}2012.pdf:pdf},
isbn = {0956-540X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Europe,Inverse theory,Surface waves and free oscillations,pso},
mendeley-tags = {pso},
month = {jul},
number = {1},
pages = {580--594},
title = {{On the application of Particle Swarm Optimization strategies on Scholte-wave inversion}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.2012.05500.x},
volume = {190},
year = {2012}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:D$\backslash$:/OneDrive/Articles/Icml/Blundell et al.{\_}2015.pdf:pdf},
isbn = {9781510810587},
journal = {Icml},
keywords = {neural network},
mendeley-tags = {neural network},
month = {may},
pages = {1613--1622},
title = {{Weight Uncertainty in Neural Networks}},
nourl = {http://arxiv.org/abs/1505.05424{\%}5Cnhttp://www.arxiv.org/pdf/1505.05424.pdf http://arxiv.org/abs/1505.05424},
volume = {37},
year = {2015}
}
@inproceedings{Benaichouche2015,
abstract = {Classical algorithms used for first arrival traveltime tomography are not necessarily well-suited for handling very large seismic data sets, densely parameterized velocity models and for taking advantage of current supercomputers architecture. Several authors have recently revisited the classical approach of first arrival traveltime tomography by proposing to use a gradient-type approach based on the adjoint-state of the eikonal equation. With the adjoint-state technique the gradient of the objective function can be obtained without the explicit estimation of the Fr{\'{e}}chet derivative matrix, which is computationally prohibitive for large-scale problems. We propose a new robust, efficient numerical implementation of the adjoint state technique combined with the fast marching method. This scheme allows us to compute the gradient of the misfit function with respect to velocity, it also enable us to build up an approximation of the Hessian. This approach is a very effective solution for processing huge and dense surveys with no compromise in terms of amount of data and model description.},
author = {Benaichouche, A. and Noble, M. and Greset, A.},
booktitle = {77th EAGE Conference {\&} Exhibition},
file = {:D$\backslash$:/OneDrive/Articles/77th EAGE Conference {\&} Exhibition/Benaichouche, Noble, Greset{\_}2015.pdf:pdf},
pages = {1--4},
title = {{First Arrival Traveltime Tomography Using the Fast Marching Method and the Adjoint State Technique}},
year = {2015}
}
@article{Sambridge1992a,
abstract = {Summary Recently a new class of methods, to solve non-linear optimization problems, has generated considerable interest in the field of Artificial Intelligence. These methods, known as genetic algorithms, are able to solve highly non-linear and non-local optimization ... $\backslash$n},
author = {Sambridge, Malcolm and Drijkoningen, Guy},
nodoi = {10.1111/j.1365-246X.1992.tb00100.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Sambridge, Drijkoningen{\_}1992(2).pdf:pdf},
isbn = {0956-540X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {ga,genetic algorithms,global optimization,inversion,waveform inversion},
mendeley-tags = {ga,inversion},
month = {may},
number = {2},
pages = {323--342},
title = {{Genetic algorithms in seismic waveform inversion}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.1992.tb00100.x},
volume = {109},
year = {1992}
}
@article{Poliannikov2012,
abstract = {With high-permeability hydrocarbon reservoirs exhausting their potential, developing low-permeability reservoirs is becoming of increasing importance. In order to be produced economically, these reservoirs need to be stimulated to increase their permeability. Hydraulic fracturing is a technique used to do this. A mixture of water, additives, and proppants is injected under high pressure into the subsurface; this fluid fractures the rock, creating additional pathways for the oil or gas. Understanding the nature of the resulting fracture system, including the geometry, size, and orientation of individual fractures, as well as the distance from one fracture to the next, is key to answering important practical questions such as: What is the affected reservoir volume? Where should we fracture next? What are the optimal locations for future production wells?},
author = {Poliannikov, Oleg V. and Malcolm, Alison E. and Prange, Michael and Djikpesse, Hugues},
nodoi = {10.1190/tle31121490.1},
file = {:D$\backslash$:/OneDrive/Articles/The Leading Edge/Poliannikov et al.{\_}2012.pdf:pdf},
issn = {1070-485X,},
journal = {The Leading Edge},
number = {12},
pages = {1490--1494},
title = {{Checking up on the neighbors: Quantifying uncertainty in relative event location}},
nourl = {http://tle.geoscienceworld.org/content/31/12/1490{\%}5Cnhttp://tle.geoscienceworld.org/content/31/12/1490.full.pdf{\%}5Cnhttp://tle.geoscienceworld.org/content/31/12/1490.abstract},
volume = {31},
year = {2012}
}
@article{Lomax2009,
abstract = {Article Outline Glossary I. Definition of the Subject and Its Importance II. Introduction III. The Earthquake Location Problem An inherently nonlinear problem The observed data The velocity or slowness model The travel-time calculation A complete solution -probabilistic location Experimental design methods – choosing receiver locations IV. Location methods Linearized location methods Direct-search location methods V. Illustrative Examples Example 1: An ideal location Examples 2-5: Station distribution Example 6: Incorrect picks and phase identification -outlier data Example 7: Earthquake early-warning scenario Example 8: Incorrect velocity model VI. Future directions VII. Bibliography Books and Reviews Primary Literature Figures Glossary Arrival time The time of the first measurable energy of a seismic phase on a seismogram. Centroid The coordinates of the spatial or temporal average of some characteristic of an earthquake, such as surface shaking intensity or moment release. 26/10/2007 Page 2 Data space If the data are described by a vector d, then the data space D is the set of all possible values of d. Direct search A search or inversion technique that does not explicitly use derivatives. Earthquake early-warning The goal of earthquake early-warning is to estimate the shaking hazard of a large earthquake at a nearby population centre or other critical site before destructive S and surface waves have reached the site. This requires that useful, probabilistic constraint on the location and size of an earthquake is obtained very rapidly. Earthquake location An earthquake location specifies a spatial position and time of occurrence for an earthquake. The location may refer to the earthquake hypocentre and corresponding origin time, a mean or centroid of some spatial or temporal characteristic of the earthquake, or another property of the earthquake that can be spatially and temporally localized. This term also refers to the process of locating an earthquake.},
author = {Lomax, A. and Michelini, A. and Curtis, A.},
nodoi = {10.1007/978-0-387-30440-3},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Lomax, Michelini, Curtis{\_}2009.pdf:pdf},
isbn = {978-0-387-75888-6},
keywords = {location},
mendeley-tags = {location},
pages = {2--449},
title = {{Earthquake location, direct, global-search methods}},
nourl = {http://geosys.usc.edu/projects/seatree/export/604/trunk/modules/seismo/nonlinloc/nll/doc/EarthqkLoc-Direct-Search{\_}v2.0.pdf},
year = {2009}
}
@article{Riahi2013,
abstract = {We perform a time-lapse analysis of Rayleigh and Love wave anisotropy above an underground gas storage facility in the Paris Basin. The data were acquired with a three-component seismic array deployed during several days in April and November 2010. Phase velocity and back azimuth of Rayleigh and Love waves are measured in the frequency range 0.2–1.1 Hz using a three-component beamforming algorithm. In both snapshots, higher-surface wave modes start dominating the signal above 0.4 Hz with a concurrent increase in back azimuth ranges. We fit anisotropy parameters to the array detections above 0.4 Hz using a bootstrap approach which also provides estimation uncertainty and enables significance testing. The isotropic phase velocity dispersion for Love and Rayleigh waves match for both snapshots. We also observe a stable fast direction of NNW-SSE for Love and Rayleigh waves which is aligned with the preferred orientation of known shallow ({\textless}300 m) and deeper (∼1000 m) fault systems in the area, as well as the maximum horizontal stress orientation. At lower frequencies corresponding to deeper parts of the basin, the anisotropic parameters exhibit higher magnitude in the November data. This may perhaps be caused by the higher pore pressure changes in the gas reservoir in that depth range.},
author = {Riahi, Nima and Bokelmann, G{\"{o}}tz and Sala, Paola and Saenger, Erik H.},
nodoi = {10.1002/jgrb.50375},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Riahi et al.{\_}2013.pdf:pdf},
issn = {21699313},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {anisotropy,array processing,hypothesis testing,seismic noise,surface waves,time-lapse},
month = {oct},
number = {10},
pages = {5339--5351},
title = {{Time-lapse analysis of ambient surface wave anisotropy: A three-component array study above an underground gas storage}},
nourl = {http://nodoi.wiley.com/10.1002/jgrb.50375},
volume = {118},
year = {2013}
}
@inproceedings{Ji2011,
abstract = {Finding the least‐absolute (ℓ1 norm) error solution to solve an optimized problem is known to give a better answer than the classical least‐squares (ℓ2 norm) method does. It is because the robust property of the median value is not much affected by outlier values and the solution of the least ℓ1‐norm error corresponds to the solution of minimum median error. Several variants of the ℓ1 norm such as the Huber norm and the Hybrid norm have the same robust properties. These ℓ1‐norm‐based optimization methods obtain their robustness by reducing the influence of outliers significanly, although never ignoring it. Therefore, if the proportion of outliers increases, most of the ℓ1 ‐norm‐based methods may begin to be affected by the outliers. In such a case, other types of robust measures such as Tukey's Biweight (Bisquare weight) norm, which excludes outliers in computing the misfit measure, could perform better. This paper describes the application of the Biweight norm using the IRLS (iteratively reweighted least squares) method as a robust inversion and shows its possible improvement in robustness when dealing with data having complex outliers.},
author = {Ji, Jun},
booktitle = {SEG Technical Program Expanded Abstracts 2011},
nodoi = {10.1190/1.3627758},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2011/Ji{\_}2011.pdf:pdf},
keywords = {inversion,least squares,noise,nonlinear},
month = {jan},
pages = {2717--2721},
publisher = {Society of Exploration Geophysicists},
title = {{Robust inversion using Biweight norm}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/1.3627758},
year = {2011}
}
@inproceedings{Dong2008,
abstract = {The particle swarm optimization algorithm (PSO) has successfully been applied to many engineering optimization problems. However, the most of existing improved PSO algorithms work well only for small-scale problems on low-dimensional space. In this new self-adaptive PSO, a special function, which is defined in terms of the particle fitness, swarm size and the dimension size of solution space, is introduced to adjust the inertia weight adaptively. In a given generation, the inertia weight for particles with good fitness is decreased to accelerate the convergence rate, whereas the inertia weight for particles with inferior fitness is increased to enhance the global exploration abilities. When the swarm size is large, a smaller inertia weight is utilized to enhance the local search capability for fast convergence rate. If the swarm size is small, a larger inertia weight is employed to improve the global search capability for finding the global optimum. For an optimization problem on multi-dimension complex solution space, a larger inertia weight is employed to strengthen the ability to escape from local optima. In case of small dimension size of solution space, a smaller inertia weight is used for reinforcing the local search capability. This novel self-adaptive PSO can greatly accelerate the convergence rate and improve the capability to reach the global minimum for large-scale problems. Moreover, this new self-adaptive PSO exhibits a consistent methodology: a larger swarm size leads to a better performance.},
author = {Dong, Chen and Wang, Gaofeng and Chen, Zhenyi and Yu, Zuqiang},
booktitle = {Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008},
nodoi = {10.1109/CSSE.2008.295},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008/Dong et al.{\_}2008.pdf:pdf},
isbn = {9780769533360},
keywords = {Dimension size,Fitness value,Inertia weight,PSO,Self-adaptive,Swarm size,pso},
mendeley-tags = {pso},
pages = {1195--1198},
title = {{A method of self-adaptive inertia weight for PSO}},
volume = {1},
year = {2008}
}
@article{VanDenBergh2006,
abstract = {Particle swarm optimization (PSO) has shown to be an efficient, robust and simple optimization algorithm. Most of the PSO studies are empirical, with only a few theoretical analyses that concentrate on understanding particle trajectories. These theoretical studies concentrate mainly on simplified PSO systems. This paper overviews current theoretical studies, and extend these studies to investigate particle trajectories for general swarms to include the influence of the inertia term. The paper also provides a formal proof that each particle converges to a stable point. An empirical analysis of multi-dimensional stochastic particles is also presented. Experimental results are provided to support the conclusions drawn from the theoretical findings. ?? 2005 Elsevier Inc. All rights reserved.},
author = {{Van Den Bergh}, F. and Engelbrecht, A. P.},
nodoi = {10.1016/j.ins.2005.02.003},
file = {:D$\backslash$:/OneDrive/Articles/Information Sciences/Van Den Bergh, Engelbrecht{\_}2006.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Convergence,Equilibrium,Particle swarm optimization,Particle trajectories,pso},
mendeley-tags = {pso},
number = {8},
pages = {937--971},
title = {{A study of particle swarm optimization particle trajectories}},
volume = {176},
year = {2006}
}
@article{Bazin2010,
abstract = {On November 21, 2004 an Mw6.3 intraplate earthquake occurred at sea in the French Caribbean. The aftershock sequence continues to this day and is the most extensive sequence in a French territory in more than a century. We recorded aftershocks from day 25 to day 66 of this sequence, using a rapidly-deployed temporary array of ocean bottom seismometers (OBS). We invert P- and S-wave arrivals for a tomographic velocity model and improve aftershock locations. The velocity model shows anomalies related to tectonic and geologic structures beneath the Les Saintes graben. 3D relocated aftershocks outline faults whose scarps were identified as active in recent high-resolution marine data. The aftershocks distribution suggests that both the main November 21 event and its principal aftershock, on February 14, 2005, ruptured Roseau fault, which is the largest of the graben, extending from Dominica Island to the Les Saintes archipelago. Aftershocks cluster in the lower part of the Roseau fault plane (between 8 and 12.6km depth) that did not rupture during the main event. Shallower aftershocks occur in the Roseau fault footwall, probably along smaller antithetic faults. We calculate a strong negative Vp anomaly, between 4 and 8km depth, within the graben, along the Roseau fault plane. This low Vp anomaly is associated with a high Vp/Vs ratio and may reflect a strongly fracturated body filled with fluids. We infer from several types of observation that fault lubrication is the driving mechanism for this long-lasting aftershock sequence. {\textcopyright} 2010 Elsevier B.V.},
author = {Bazin, S. and Feuillet, N. and Duclos, C. and Crawford, W. and Nercessian, A. and Bengoubou-Val{\'{e}}rius, M. and Beauducel, F. and Singh, S. C.},
nodoi = {10.1016/j.tecto.2010.04.005},
file = {:D$\backslash$:/OneDrive/Articles/Tectonophysics/Bazin et al.{\_}2010.pdf:pdf},
isbn = {nodoi:10.1016/j.tecto.2010.04.005},
issn = {00401951},
journal = {Tectonophysics},
keywords = {Aftershock sequence,Fault-fluid interactions,Les Saintes,Lesser Antilles,Local earthquake tomography,Normal-fault earthquake,Ocean bottom seismometer},
number = {1-4},
pages = {91--103},
publisher = {Elsevier B.V.},
title = {{The 2004-2005 Les Saintes (French West Indies) seismic aftershock sequence observed with ocean bottom seismometers}},
nourl = {http://dx.nodoi.org/10.1016/j.tecto.2010.04.005},
volume = {489},
year = {2010}
}
@article{Sabbione2010,
abstract = {We have developed three methods for the automatic picking of first breaks that can be used for marine, dynamite, or vibroseis shot records: a modified Coppens's method, an entropy-based method, and a variogram fractal-dimension method. The techniques are based on the fact that the transition between noise and noise plus signal can be automatically identified by detecting rapid changes in a certain attribute (energy ratio, entropy, or fractal dimension), which we calculate within moving windows along the seismic trace. The application of appropriate edge-preserving smoothing operators to enhance these transitions allowed us to develop an automated strategy that can be used to easily signal the precise location of the first-arrival onset. Furthermore, we propose a mispick-correcting technique to exploit the benefits of the data present in the entire shot record, which allows us to adjust the trace-by-trace picks and to discard picks associated with bad or dead traces. As a result, the consistency of the first-break picks is significantly improved. The methods are robust under noisy conditions, computationally efficient, and easy to apply. Results using dynamite and vibroseis field data show that accurate and consistent picks can be obtained in an automated manner even under the presence of correlated noise, bad traces, pulse changes, and indistinct first breaks.},
author = {Sabbione, Juan I and Velis, Danilo},
nodoi = {10.1190/1.3463703},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Sabbione, Velis{\_}2010.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {picking},
mendeley-tags = {picking},
month = {jul},
number = {4},
pages = {V67--V76},
title = {{Automatic first-breaks picking: New strategies and algorithms}},
nourl = {http://geophysics.geoscienceworld.org/content/75/4/V67.abstract http://library.seg.org/nodoi/10.1190/1.3463703},
volume = {75},
year = {2010}
}
@phdthesis{Luu2015,
author = {Luu, Keurfon},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Luu{\_}2015.pdf:pdf},
keywords = {ambient noise,interferometry,monte-carlo inversion,passive seismic,raphy,seismic processing,shear wave velocity,surface waves,tomog-},
pages = {1--30},
title = {{Passive seismic tomography: feasibility on onshore domain}},
year = {2015}
}
@article{Mohamed2010,
abstract = {History matching and uncertainty quantification are two important research topics in reservoir simulation currently. In the Bayesian approach, we start with prior information about a reservoir - for example from analogue outcrop data - and update our reservoir models with observations, for example from production data or time lapse seismic. The goal of this activity is often to generate multiple models that match the history and use the models to quantify uncertainties in predictions of reservoir performance. A critical aspect of generating multiple history matched models is the sampling algorithm used to generate the models. Algorithms that have been studied include gradient methods, genetic algorithms, the Ensemble Kalman Filter, and others. This paper investigates the efficiency of three stochastic sampling algorithms: Hamiltonian Monte Carlo (HMC) algorithm, Particle Swarm Optimization (PSO) algorithm and the Neighborhood Algorithm (NA). HMC is a Markov Chain Monte Carlo (MCMC) technique that uses Hamiltonian dynamics to achieve larger jumps than are possible with other MCMC techniques. PSO is a swarm intelligence algorithm that uses similar dynamics to HMC to guide the search, but incorporates acceleration and damping parameters to provide rapid convergence to possible multiple minima. The Neighbourhood Algorithm is a sampling technique that uses the properties of Voronoi cells in high dimensions to achieve multiple history matched models. The algorithms are compared by generating multiple history matched reservoir models, and comparing the plO - p. 50 - p. 90 uncertainty bounds produced by each algorithm. We show that all the algorithms are able to find equivalent match qualities for this example, but that some algorithms are able to find good fitting results quickly, whereas others are able to find a more diverse set of models in parameter space. The effects of the different sampling of model parameter space are compared in terms of the p. 10 - p. 50 - p. 90 uncertainty bounds in forecast oil rate. These results show that algorithms based on Hamiltonian dynamics and swarm intelligence concepts have the potential to be effective tools in uncertainty quantification in the oil industry. Copyright 2009, Society of Petroleum Engineers.},
author = {Mohamed, L. and Christie, M. and Demyanov, V.},
nodoi = {10.2118/119139-PA},
file = {:D$\backslash$:/OneDrive/Articles/SPE Journal/Mohamed, Christie, Demyanov{\_}2010.pdf:pdf},
isbn = {9781605607771},
issn = {1086055X},
journal = {SPE Journal},
keywords = {pso,uncertainty quantification},
mendeley-tags = {pso,uncertainty quantification},
number = {March},
pages = {31--38},
title = {{Comparison of Stochastic Sampling Algorithms for Uncertainty Quantification}},
volume = {15},
year = {2010}
}
@article{Lan2014,
abstract = {In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create wormholes connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a residual energy function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.},
archivePrefix = {arXiv},
arxivId = {1306.0063},
author = {Lan, Shiwei and Streets, Jeffrey and Shahbaba, Babak},
eprint = {1306.0063},
file = {:D$\backslash$:/OneDrive/Articles/AAAI/Lan, Streets, Shahbaba{\_}2014.pdf:pdf},
isbn = {9781577356790},
journal = {AAAI},
keywords = {machine learning,mcmc},
mendeley-tags = {machine learning,mcmc},
month = {may},
pages = {1--18},
title = {{Wormhole Hamiltonian Monte Carlo}},
nourl = {http://arxiv.org/abs/1306.0063},
year = {2014}
}
@article{Rawlinson2006,
abstract = {We demonstrate the potential of a recently developed grid-based eikonal solver for tracking phases comprising reflection branches, transmission branches, or a combination of these, in 3D heterogeneous layered media. The scheme is based on a multi-stage fast marching approach that reinitialises the wavefront from each interface it encounters as either a reflection or transmission. The use of spherical coordinates allows wavefronts and traveltimes to be computed at local, regional, and semi-global scales. Traveltime datasets for a large variety of seismic experiments can be predicted, including reflection, wide-angle reflection and refraction, local earthquake, and teleseismic. A series of examples are presented to demonstrate potential applications of the method. These include: (1) tracking active and passive source wavefronts in the presence of a complex subduction zone; (2) earthquake hypocentre relocation in a laterally heterogeneous 3D medium; (3) joint inversion of wide-angle and teleseismic datasets for P-wave velocity structure in the crust and upper mantle. Results from these numerical experiments show that the new scheme is highly flexible, robust and efficient, a combination seldom found in either grid- or ray-based traveltime solvers. The ability to track arrivals for multiple data classes such as wide-angle and teleseismic is of particular importance, given the recent momentum in the seismic imaging community towards combining active and passive source datasets in a single tomographic inversion.},
author = {Rawlinson, N. and de Kool, M. and Sambridge, M.},
nodoi = {10.1071/EG06322},
file = {:D$\backslash$:/OneDrive/Articles/Exploration Geophysics/Rawlinson, de Kool, Sambridge{\_}2006.pdf:pdf},
issn = {0812-3985},
journal = {Exploration Geophysics},
keywords = {fast marching method,tomography},
number = {4},
pages = {322},
title = {{Seismic wavefront tracking in 3D heterogeneous media: applications with multiple data classes}},
nourl = {http://www.publish.csiro.au/?paper=EG06322},
volume = {37},
year = {2006}
}
@inproceedings{Akimoto2014,
abstract = {We propose a novel natural gradient based stochastic search algorithm, VD-CMA, for the optimization of high dimensional numerical functions. The algorithm is comparisonbased and hence invariant to monotonic transformations of the objective function. It adapts a multivariate normal distribution with a restricted covariance matrix with twice the dimension as degrees of freedom, representing an arbitrarily oriented long axis and additional axis-parallel scaling. We derive the different components of the algorithm and show linear internal time and space complexity. We find empirically that the algorithm adapts its covariance matrix to the inverse Hessian on convex-quadratic functions with an Hessian with one short axis and different scaling on the diagonal. We then evaluate VD-CMA on test functions and compare it to different methods. On functions covered by the internal model of VD-CMA and on the Rosenbrock function, VD-CMA outperforms CMA-ES (having quadratic internal time and space complexity) not only in internal complexity but also in number of function calls with increasing dimension.},
address = {New York, New York, USA},
author = {Akimoto, Youhei and Auger, Anne and Hansen, Nikolaus},
booktitle = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
nodoi = {10.1145/2576768.2598258},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14/Akimoto, Auger, Hansen{\_}2014.pdf:pdf},
isbn = {9781450326629},
keywords = {cmaes,covariance matrix adaptation,hessian,information geometric optimization,matrix,natural gradient,theory},
mendeley-tags = {cmaes},
pages = {373--380},
publisher = {ACM Press},
title = {{Comparison-based natural gradient optimization in high dimension}},
nourl = {http://dl.acm.org/citation.cfm?doid=2576768.2598258},
year = {2014}
}
@article{Tronicke2012,
abstract = {Particle swarm optimization (PSO) is a relatively new global optimization approach inspired by the social behavior of bird flocking and fish schooling. Although this approach has proven to provide excellent convergence rates in different optimization problems, it has seldom been applied to inverse geophysical problems. Until today, published geophysical applications mainly focus on finding an optimum solution for simple, 1D inverse problems. We have applied PSO-based optimization strategies to reconstruct 2D P-wave velocity fields from crosshole traveltime data sets. Our inversion strategy also includes generating and analyzing a representative ensemble of acceptable models, which allows us to appraise uncertainty and nonuniqueness issues. The potential of our strategy was tested on field data collected at a well-constrained test site in Horstwalde, Germany. At this field site, the shallow subsurface mainly consists of sand- and gravel-dominated glaciofluvial sediments, which, as known from several boreholes and other geophysical experiments, exhibit some well-defined layering at the scale of our crosshole seismic data. Thus, we have implemented a flexible, layer-based model parameterization, which, compared with standard cell-based parameterizations, allows for significantly reducing the number of unknown model parameters and for efficiently implementing a priori model constraints. Comparing the 2D velocity fields resulting from our PSO strategy to independent borehole and direct-push data illustrated the benefits of choosing an efficient global optimization approach. These include a straightforward and understandable appraisal of nonuniqueness issues as well as the possibility of an improved and also more objective interpretation.},
author = {Tronicke, Jens and Paasche, Hendrik and B{\"{o}}niger, Urs},
nodoi = {10.1190/geo2010-0411.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Tronicke, Paasche, B{\"{o}}niger{\_}2012.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {pso},
mendeley-tags = {pso},
month = {jan},
number = {1},
pages = {R19--R32},
title = {{Crosshole traveltime tomography using particle swarm optimization: A near-surface field example}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/geo2010-0411.1 http://library.seg.org/nodoi/10.1190/geo2010-0411.1},
volume = {77},
year = {2012}
}
@article{Sabbione2013,
abstract = {We present a robust method for the automatic detection and picking of microseismic events that consists of two steps. The first step provides accurate single-trace picks using three automatic phase pickers adapted from earthquake seismology. In the second step, a multi-channel strategy is implemented to associate (or not) the previous picks with actual microseismic signals by taking into account their expected alignment in all the available channels, thus reducing the false positive rate. As a result, the method provides the number of declared microseismic events, a confidence indicator associated with each of them, and the corresponding traveltime picks. Results using two field noisy data records demonstrate that the automatic detection and picking of microseismic events can be carried out with a relatively high confidence level and accuracy. ?? 2013.},
author = {Sabbione, Juan I. and Velis, Danilo R.},
nodoi = {10.1016/j.jappgeo.2013.07.011},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Applied Geophysics/Sabbione, Velis{\_}2013.pdf:pdf},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Automatic detection,Microseism,Picking,picking},
mendeley-tags = {picking},
month = {dec},
pages = {42--50},
title = {{A robust method for microseismic event detection based on automatic phase pickers}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/S092698511300150X},
volume = {99},
year = {2013}
}
@article{Gentili2006,
abstract = {The large amount of digital data recorded by permanent and temporary seismic networks makes automatic analysis of seismograms and automatic wave onset time picking schemes of great importance for timely and accurate event locations. We propose a fast and efficient P- and S-wave onset time, automatic detection method based on neural networks. The neural networks adopted here are particular neural trees, called IUANT2, characterized by a high generalization capability. Comparison between neural network automatic onset picking and standard, manual methods, shows that the technique presented here is generally robust and that it is capable to correctly identify phase-types while providing estimates of their accuracies. In addition, the automatic post processing method applied here can remove the ambiguity deriving from the incorrect association of events occurring closely in time. We have tested the methodology against standard STA/LTA phase picks and found that this neural approach performs better especially for low signal-to-noise ratios. We adopt the recall, precision and accuracy estimators to appraise objectively the results and compare them with those obtained with other methodologies.Tests of the proposed method are presented for 342 earthquakes recorded by 23 different stations (about 5000 traces). Our results show that the distribution of the differences between manual and automatic picking has a standard deviation of 0.064 s and 0.11 s for the P and the S waves, respectively. Our results show also that the number of false alarms deriving from incorrect detection is small and, thus, that the method is inherently robust.},
author = {Gentili, S. and Michelini, A.},
nodoi = {10.1007/s10950-006-2296-6},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Seismology/Gentili, Michelini{\_}2006.pdf:pdf},
isbn = {1383-4649},
issn = {1383-4649},
journal = {Journal of Seismology},
keywords = {Arrival time,Artificial neural networks,Automatic picking,Pattern recognition,Seismic network,machine learning,neural network,picking},
mendeley-tags = {machine learning,neural network,picking},
month = {jan},
number = {1},
pages = {39--63},
title = {{Automatic picking of P and S phases using a neural tree}},
nourl = {http://link.springer.com/10.1007/s10950-006-2296-6},
volume = {10},
year = {2006}
}
@article{Beleites2013,
abstract = {In biospectroscopy, suitably annotated and statistically independent samples (e.g. patients, batches, etc.) for classifier training and testing are scarce and costly. Learning curves show the model performance as function of the training sample size and can help to determine the sample size needed to train good classifiers. However, building a good model is actually not enough: the performance must also be proven. We discuss learning curves for typical small sample size situations with 5-25 independent samples per class. Although the classification models achieve acceptable performance, the learning curve can be completely masked by the random testing uncertainty due to the equally limited test sample size. In consequence, we determine test sample sizes necessary to achieve reasonable precision in the validation and find that 75-100 samples will usually be needed to test a good but not perfect classifier. Such a data set will then allow refined sample size planning on the basis of the achieved performance. We also demonstrate how to calculate necessary sample sizes in order to show the superiority of one classifier over another: this often requires hundreds of statistically independent test samples or is even theoretically impossible. We demonstrate our findings with a data set of ca. 2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and three tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive simulation that allows precise determination of the actual performance of the models in question. {\textcopyright} 2012 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1211.1323},
author = {Beleites, Claudia and Neugebauer, Ute and Bocklitz, Thomas and Krafft, Christoph and Popp, J{\"{u}}rgen},
nodoi = {10.1016/j.aca.2012.11.007},
eprint = {1211.1323},
file = {:D$\backslash$:/OneDrive/Articles/Analytica Chimica Acta/Beleites et al.{\_}2013.pdf:pdf},
isbn = {1873-4324 (Electronic)$\backslash$r0003-2670 (Linking)},
issn = {00032670},
journal = {Analytica Chimica Acta},
keywords = {Classification,Design of experiments,Learning curve,Multivariate,Small sample size,Training,Validation,machine learning},
mendeley-tags = {machine learning},
month = {jan},
pages = {25--33},
pmid = {23265730},
title = {{Sample size planning for classification models}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/S0003267012016479},
volume = {760},
year = {2013}
}
@article{Noble2014,
abstract = {Seismic traveltimes and their spatial derivatives are the basis of many imaging methods such as pre-stack depth migration and tomography. A common approach to compute these quantities is to solve the eikonal equation with a finite-difference scheme. If many recently published algorithms for resolving the eikonal equation do now yield fairly accurate traveltimes for most applications, the spatial derivatives of traveltimes remain very approximate. To address this accuracy issue, we develop a new hybrid eikonal solver that combines a spherical approximation when close to the source and a plane wave approximation when far away. This algorithm reproduces properly the spherical behaviour of wave fronts in the vicinity of the source. We implement a combination of 16 local operators that enables us to handle velocity models with sharp vertical and horizontal velocity contrasts. We associate to these local operators a global fast sweeping method to take into account all possible directions of wave propagation. Our formulation allows us to introduce a variable grid spacing in all three directions of space. We demonstrate the efficiency of this algorithm in terms of computational time and the gain in accuracy of the computed traveltimes and their derivatives on several numerical examples.},
author = {Noble, Mark and Gesret, Alexandrine and Belayouni, Nidhal},
nodoi = {10.1093/gji/ggu358},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Noble, Gesret, Belayouni{\_}2014.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Numerical solutions,Wave propagation,eikonal},
mendeley-tags = {eikonal},
number = {3},
pages = {1572--1585},
title = {{Accurate 3-D finite difference computation of traveltimes in strongly heterogeneous media}},
volume = {199},
year = {2014}
}
@article{Venter2006,
abstract = {A parallel Particle Swarm Optimization (PSO) algorithm is presented.$\backslash$nParticle swarm optimization$\backslash$n$\backslash$nis a fairly recent addition to the family of non-gradient based, probabilistic$\backslash$nsearch algorithms that is$\backslash$n$\backslash$nbased on a simpliﬁed social model and is closely tied to swarming$\backslash$ntheory. Although PSO algorithms$\backslash$n$\backslash$npresent several attractive properties to the designer, they are plagued$\backslash$nby high computational cost as$\backslash$n$\backslash$nmeasured by elapsed time. One approach to reduce the elapsed time$\backslash$nis to make use of coarse-grained$\backslash$n$\backslash$nparallelization to evaluate the design points. Previous parallel PSO$\backslash$nalgorithms were mostly implemented$\backslash$n$\backslash$nin a synchronous manner, where all design points within a design iteration$\backslash$nare evaluated before the next$\backslash$n$\backslash$niteration is started. This approach leads to poor parallel speedup$\backslash$nin cases where a heterogeneous parallel$\backslash$n$\backslash$nenvironment is used and/or where the analysis time depends on the$\backslash$ndesign point being analyzed. This$\backslash$n$\backslash$npaper introduces an asynchronous parallel PSO algorithm that greatly$\backslash$nimproves the parallel eﬃciency.$\backslash$n$\backslash$nThe asynchronous algorithm is benchmarked on a cluster assembled of$\backslash$nApple Macintosh G5 desktop$\backslash$n$\backslash$ncomputers, using the multi-disciplinary optimization of a typical$\backslash$ntransport aircraft wing as an example.},
author = {Venter, Gerhard and Sobieszczanski-Sobieski, Jaroslaw},
nodoi = {10.2514/1.17873},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Aerospace Computing, Information, and Communication/Venter, Sobieszczanski-Sobieski{\_}2006.pdf:pdf},
isbn = {1542-9423$\backslash$r1542-9423},
issn = {1542-9423},
journal = {Journal of Aerospace Computing, Information, and Communication},
keywords = {hpc,parallel,pso},
mendeley-tags = {hpc,parallel,pso},
number = {3},
pages = {123--137},
title = {{Parallel Particle Swarm Optimization Algorithm Accelerated by Asynchronous Evaluations}},
volume = {3},
year = {2006}
}
@article{Sambridge1992,
abstract = {Recently a new class of methods, to solve non-linear optimization problems, has generated considerable interest in the field of Artificial Intelligence. These methods, known as genetic algorithms, are able to solve highly non-linear and non-local optimization problems and belong to the class of global optimization techniques, which includes Monte Carlo and Simulated Annealing methods. Unlike local techniques, such as damped least squares or conjugate gradients, genetic algorithms avoid all use of curvature information on the objective function. This means that they do not require any derivative information and therefore one can use any type of misfit function equally well. Most iterative methods work with a single model and find improvements by perturbing it in some fashion. Genetic algorithms, however, work with a group of models simultaneously and use stochastic processes to guide the search for an optimal solution. Both Simulated Annealing and genetic algorithms are modelled on natural optimization systems. Simulated Annealing uses an analogy with thermodynamics; genetic algorithms have an analogy with biological evolution. This evolution leads to an efficient exchange of information between all models encountered, and allows the algorithm to rapidly assimilate and exploit the information gained to find better data fitting models. To illustrate the power of genetic algorithms compared to Monte Carlo, we consider a simple multidimensional quadratic optimization problem and show that its relative efficiency increases dramatically as the number of unknowns is increased. As an example of their use in a geophysical problem with real data we consider the non-linear inversion of marine seismic refraction waveforms. The results show that genetic algorithms are inherently superior to random search techniques and can also perform better than iterative matrix inversion which requires a good starting model. This is primarily because genetic algorithms are able to combine both local and global search mechanisms into a single efficient method. Since many forward and inverse problems involve solving an optimization problem, we expect that the genetic approach will find applications in many other geophysical problems; these include seismic ray tracing, earthquake location, non-linear data fitting and, possibly seismic tomography.},
author = {Sambridge, Malcolm and Drijkoningen, Guy},
nodoi = {10.1111/j.1365-246X.1992.tb00100.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Sambridge, Drijkoningen{\_}1992.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Genetic algorithms,Global optimization,Waveform inversion,ga,inversion},
mendeley-tags = {ga,inversion},
month = {may},
number = {2},
pages = {323--342},
publisher = {Geophysical Journal International},
title = {{Genetic algorithms in seismic waveform inversion}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.1992.tb00100.x},
volume = {109},
year = {1992}
}
@article{Diouane2016,
abstract = {In this paper we propose a new way to compute a rough approximation solution, to be later used as a warm starting point in a more refined optimization process, for a challenging global optimization problem related to Earth imaging in geophysics. The warm start con- sists of a velocity model that approximately solves a full-waveform inverse problem at low frequency. Our motivation arises from the availability of massively parallel computing plat- forms and the natural parallelization of evolution strategies as global optimization methods for continuous variables. Our first contribution consists of developing a new and efficient parametrization of the velocity models to significantly reduce the dimension of the original optimization space. Our second contribution is to adapt a class of evolution strategies to the specificity of the physical problem at hands where the objective function evaluation is known to be the most expen- sive computational part. A third contribution is the development of a parallel evolution strategy solver, taking advantage of a recently proposed modification of these class of evolu- tionary methods that ensures convergence and promotes better performance under moderate budgets. The numerical results presented demonstrate the effectiveness of the algorithm on a realistic 3D full-waveform inverse problem in geophysics. The developed numerical approach allows us to successfully solve an acoustic full-waveform inversion problem at low frequencies on a reasonable number of cores of a distributed memory computer.},
author = {Diouane, Y. and Gratton, S. and Vasseur, X. and Vicente, L. N. and Calandra, H.},
nodoi = {10.1007/s11081-015-9296-8},
file = {:D$\backslash$:/OneDrive/Articles/Optimization and Engineering/Diouane et al.{\_}2016.pdf:pdf},
issn = {1389-4420},
journal = {Optimization and Engineering},
keywords = {Earth imaging,Evolution strategy,Full-waveform inversion,Global convergence,High performance computing (HPC),Inverse problem,Search space reduction,cmaes,inversion},
mendeley-tags = {cmaes,inversion},
month = {mar},
number = {1},
pages = {3--26},
title = {{A parallel evolution strategy for an earth imaging problem in geophysics}},
nourl = {http://link.springer.com/10.1007/s11081-015-9296-8},
volume = {17},
year = {2016}
}
@article{Zhang2016,
abstract = {For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian Monte Carlo (HMC). The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effective approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an efficient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differently, our method can be related to other approaches for the construction of surrogate functions such as generalized additive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the art methods.},
archivePrefix = {arXiv},
arxivId = {1506.05555},
author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai},
nodoi = {10.1007/s11222-016-9699-1},
eprint = {1506.05555},
file = {:D$\backslash$:/OneDrive/Articles/Statistics and Computing/Zhang, Shahbaba, Zhao{\_}2016.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Hamiltonian dynamics,Markov chain Monte Carlo,Random bases,Surrogate method,mcmc},
mendeley-tags = {mcmc},
month = {sep},
pages = {1--18},
title = {{Hamiltonian Monte Carlo acceleration using surrogate functions with random bases}},
nourl = {http://link.springer.com/10.1007/s11222-016-9699-1},
year = {2016}
}
@article{Michelini2004,
abstract = {Michelini and Lomax [2004] (hereinafter ML2004) make some statements regarding the double-difference (DD) tomography method of Zhang and Thurber [2003] (hereinafter ZT2003) that are incorrect or misleading. In this comment, we indicate the ways in which ML2004 misrepresent characteristics of ZT2003's DD tomography algorithm and the associated code tomoDD. In the process, we clarify the ways in which tomoDD differs from the DD location code hypoDD of Waldhauser [2001] and Waldhauser and Ellsworth [2000] (hereinafter WE2000).},
author = {Michelini, A. and Lomax, A.},
nodoi = {10.1029/2004GL019682},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Research Letters/Michelini, Lomax{\_}2004.pdf:pdf},
isbn = {0094-8276},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {nodoi:10.102,http://dx.nodoi.org/10.1029/2004GL019682,location},
mendeley-tags = {location},
number = {9},
pages = {1--4},
title = {{The effect of velocity structure errors on double-difference earthquake location}},
volume = {31},
year = {2004}
}
@article{Kuperkoch2010a,
abstract = {We present an algorithm for automatic P-phase arrival time determination for local and regional seismic events based on higher order statistics (HOS). Using skewness or kurtosis a characteristic function is determined to which a new iterative picking algorithm is applied. For P-phase identification we apply the Akaike Information Criterion to the characteristic function, while for a precise determination of the P-phase arrival time a pragmatic picking algorithm is applied to a recalculated characteristic function. In addition, an automatic quality estimate is obtained, based on the slope and the signal-to-noise ratio, both calculated from the characteristic function. To get rid of erroneous picks, a Jackknife procedure and an envelope function analysis is used. The algorithm is applied to a large data set with very heterogeneous qualities of P-onsets acquired by a temporary, regional seismic network of the EGELADOS-project in the southern Aegean. The reliability and robustness of the proposed algorithm is tested by comparing more than 3000 manually derived P readings, serving as reference picks, with the corresponding automatically estimated P-wave arrival times. We find an average deviation from the reference picks of 0.26 ± 0.64 s when using kurtosis and 0.38 ± 0.75 s when using skewness. If automatically as excellent classified picks are considered only, the average difference from the reference picks is 0.07 ± 0.31 s and 0.07 ± 0.41 s, respectively. However, substantially more P-arrival times are determined when using kurtosis, indicating that the characteristic function derived from kurtosis estimation is to be preferred. Since the characteristic function is calculated recursively, the algorithm is very fast and hence suited for earthquake early warning purposes. Furthermore, a comparative study with automatically derived P-readings using Allen's and Baer {\&} Kradolfer's picking algorithms applied to the same data set demonstrates better quantitative and qualitative performance of the HOS approach. This study shows, that precise automatic P-onset determination is feasible, even when using data sets with very heterogeneous signal-to-noise ratio.},
author = {K{\"{u}}perkoch, L. and Meier, T. and Lee, J. and Friederich, W. and {Working Group}, EGELADOS},
nodoi = {10.1111/j.1365-246X.2010.04570.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/K{\"{u}}perkoch et al.{\_}2010(2).pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Body waves,Early warning,Time series analysis,picking},
mendeley-tags = {picking},
month = {mar},
title = {{Automated determination of P-phase arrival times at regional and local distances using higher order statistics}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.2010.04570.x},
year = {2010}
}
@article{Chu2011,
abstract = {Despite the fact that the popular particle swarm optimizer (PSO) is currently being extensively applied to many real-world problems that often have high-dimensional and complex fitness landscapes, the effects of boundary constraints on PSO have not attracted adequate attention in the literature. However, in accordance with the theoretical analysis in [11], our numerical experiments show that particles tend to fly outside of the boundary in the first few iterations at a very high probability in high-dimensional search spaces. Consequently, the method used to handle boundary violations is critical to the performance of PSO. In this study, we reveal that the widely used random and absorbing bound-handling schemes may paralyze PSO for high-dimensional and complex problems. We also explore in detail the distinct mechanisms responsible for the failures of these two bound-handling schemes. Finally, we suggest that using high-dimensional and complex benchmark functions, such as the composition functions in [19], is a prerequisite to identifying the potential problems in applying PSO to many real-world applications because certain properties of standard benchmark functions make problems inexplicit. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
author = {Chu, Wei and Gao, Xiaogang and Sorooshian, Soroosh},
nodoi = {10.1016/j.ins.2010.11.030},
file = {:D$\backslash$:/OneDrive/Articles/Information Sciences/Chu, Gao, Sorooshian{\_}2011.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Bound-handling strategy,Complex benchmark functions,High-dimensional optimization,Particle swarm optimization,Real-world applications,pso},
mendeley-tags = {pso},
month = {oct},
number = {20},
pages = {4569--4581},
publisher = {Elsevier Inc.},
title = {{Handling boundary constraints for particle swarm optimization in high-dimensional search space}},
nourl = {http://dx.nodoi.org/10.1016/j.ins.2010.11.030 http://linkinghub.elsevier.com/retrieve/pii/S0020025510005839},
volume = {181},
year = {2011}
}
@article{Knopoff1970,
abstract = {Models of earthquake sources that have no volume change, no net force, and no net torque as criteria for the radiation of first motions, have five degrees of freedom in their spatial orientation. The usual double-couple model has only three degrees of freedom. The most general source of high-frequency seismic motions must be a linear combination of a double couple and another source called the compensated linear-vector dipole. A radiation pattern of amplitudes of first motions on the focal sphere cannot be uniquely decomposed into the radiation patterns due to the two sources.},
author = {Knopoff, L and Randall, M J},
nodoi = {10.1029/JB075i026p04957},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research/Knopoff, Randall{\_}1970.pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research},
keywords = {moment tensor},
mendeley-tags = {moment tensor},
month = {sep},
number = {26},
pages = {4957--4963},
title = {{The compensated linear-vector dipole: A possible mechanism for deep earthquakes}},
nourl = {http://nodoi.wiley.com/10.1029/JB075i026p04957},
volume = {75},
year = {1970}
}
@article{Han2009,
abstract = {The particle swarm optimization (PSO) is an adaptive optimization based on swarm intelligence. The basic principle and the method of it being used in seismic location were introduced. To get a more accurate result, the objective function is the residual square sum of the observational travel-time and theoretical travel-time of the same earthquake return two stations. Compared with Genetic Algorithm on Seismic Location, PSO, after numerous experiments, proved its distinct superiority to locate the hypocenter more quickly and accurately. PSO is potentially useful for seismic location. {\textcopyright} 2009 IEEE.},
author = {Han, Dong-xue and Wang, Gai-yun},
nodoi = {10.1109/WGEC.2009.48},
file = {:D$\backslash$:/OneDrive/Articles/2009 Third International Conference on Genetic and Evolutionary Computing/Han, Wang{\_}2009.pdf:pdf},
isbn = {978-1-4244-5245-3},
journal = {2009 Third International Conference on Genetic and Evolutionary Computing},
keywords = {Double-difference,Genetic algorithm,Particle swarm optimization,Seismic location,pso},
mendeley-tags = {pso},
number = {1},
pages = {641--644},
title = {{Application of Particle Swarm Optimization to Seismic Location}},
nourl = {http://www.scopus.com/inward/record.nourl?eid=2-s2.0-77950654301{\&}partnerID=tZOtx3y1},
year = {2009}
}
@article{Allen1982,
abstract = {Automatic phase-picking algorithms are designed to detect a seismic signal on a single trace and to time the arrival of the signal precisely. Because of the requirement for precise timing, a phase-picking algorithm is inherently less sensitive than one designed only to detect the presence of a signal, but still can approach the performance of a skilled analyst. A typical algorithm filters the input data and then generates a function characterizing the seismic time series. This function may be as simple as the absolute value of the series, or it may be quite complex. Event detection is accomplished by comparing the function or its short-term average (STA) with a threshold value (THR), which is commonly some multiple of a long-term average (LTA) of a characteristic function. If the STA exceeds THR, a trigger is declared. If the event passes simple criteria, it is reported. Sensitivity, expected timing error, false-trigger rate, and false-report rate are interrelated measures of performance controlled by choice of the characteristic function and several operating parameters. At present, computational power limits most systems to one-pass, time-domain algorithms. Rapidly advancing semi-conductor technology, however, will make possible much more powerful multi-pass approaches incorporating frequency-domain detection and pseudo-offline timing. },
author = {Allen, Rex},
journal = {Bulletin of the Seismological Society of America },
month = {dec},
number = {6B },
pages = {S225--S242},
title = {{Automatic phase pickers: Their present use and future prospects}},
nourl = {http://www.bssaonline.org/content/72/6B/S225.abstract},
volume = {72 },
year = {1982}
}
@article{Kawakatsu2008,
abstract = {The time reversal operation in seismic source estimation is considered. We show that the time reversal operation, equally the adjoint operation, for seismic source imaging gives an approximate solution tomore conventional seismic source inverse problem through the ‘happy approximation' underlined by Claerbout. Practical applications of such methods in a long- period range to monitor earth's activities in realtime are also discussed},
author = {Kawakatsu, Hitoshi and Montagner, Jean Paul},
nodoi = {10.1111/j.1365-246X.2008.03926.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Kawakatsu, Montagner{\_}2008.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Theoretical seismology,moment tensor},
mendeley-tags = {moment tensor},
number = {2},
pages = {686--688},
title = {{Time-reversal seismic-source imaging and moment-tensor inversion}},
volume = {175},
year = {2008}
}
@book{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, Radford M},
booktitle = {Handbook of Markov Chain Monte Carlo},
nodoi = {10.1201/b10905},
editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
eprint = {arXiv:1206.1901v1},
file = {:D$\backslash$:/OneDrive/Articles/Handbook of Markov Chain Monte Carlo/Neal{\_}2011.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
keywords = {hamiltonian dynamics,mcmc},
month = {may},
pages = {113--162},
pmid = {25246403},
publisher = {Chapman and Hall/CRC},
series = {Chapman {\&} Hall/CRC Handbooks of Modern Statistical Methods},
title = {{Handbook of Markov Chain Monte Carlo}},
nourl = {http://www.crcnetbase.com/nodoi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@article{FernandezMartinez2012a,
abstract = {In this paper, we show how to design a powerful set of particle swarm optimizers to be applied in inverse modelling. The design is based on the interpretation of the swarm dynamics as a stochastic damped mass-spring system, the so-called particle swarm optimization (PSO) continuous model. Based on this idea we derived a family of PSO optimizers (GPSO, CC-PSO and CP-PSO) having different exploitation and exploration capabilities. Their convergence is related to the stability of their first (mean trajectories)- and second-order moments (variance and temporal covariance). Good parameter sets are located inside their first stability regions close to the upper border of their respective second stability regions where the attraction from the particles oscillation centre is very weak. In this region of weak attraction, both convergence to the global minimum and exploration of the search space are possible. Based on this idea, we have designed a particle–cloud algorithm where each particle in the swarm has different inertia (damping) and acceleration (rigidity) constants. We explored the performance of these algorithms for different PSO members using different benchmark functions, showing that the cloud algorithms have a very good balance between exploration and exploitation. Also, the cloud design helps to avoid two main drawbacks of the PSO algorithm: the tuning of the PSO parameters and the clamping of the particles velocities. We also present the lime and sand algorithm that changes the time step with iterations. This feature helps to avoid entrapment in local minima when the time step is increased, and enables exploration around the global best when the time step is decreased. All these designs are based on the theoretical analysis of the PSO dynamics. We explain how to use this knowledge to the solution and appraisal of inverse problems. Finally, we briefly introduce the combined use of PSO and model reduction techniques to allow posterior sampling in high dimensional spaces.},
author = {{Fern{\'{a}}ndez Mart{\'{i}}nez}, Juan Luis and {Garc{\'{i}}a Gonzalo}, Esperanza and {Fern{\'{a}}ndez Mu{\~{n}}iz}, Zulima and Mukerji, Tapan},
nodoi = {10.1177/0142331211402900},
file = {:D$\backslash$:/OneDrive/Articles/Transactions of the Institute of Measurement and Control/Fern{\'{a}}ndez Mart{\'{i}}nez et al.{\_}2012.pdf:pdf},
issn = {0142-3312},
journal = {Transactions of the Institute of Measurement and Control},
keywords = {cloud particle swarm,exploitation,global optimization,pso,sampling},
mendeley-tags = {pso},
number = {6},
pages = {705--719},
title = {{How to design a powerful family of particle swarm optimizers for inverse modelling}},
nourl = {http://journals.sagepub.com/nodoi/10.1177/0142331211402900},
volume = {34},
year = {2012}
}
@article{Yin2004,
abstract = {Polygonal approximation of digital curves is one of the crucial steps prior to many image analysis tasks. This paper presents a new polygonal approximation approach based on the particle swarm optimization (PSO) algorithm. Each particle represented as a binary vector corresponds to a candidate solution to the polygonal approximation problem. A swarm of particles are initiated and fly through the solution space for targeting the optimal solution. We also propose to use a hybrid version of PSO embedding a local optimizer to enhance the performance. The experimental results manifest that the proposed discrete PSO is comparable to the genetic algorithm, and it outperforms another discrete implementation of PSO in the literature. The proposed hybrid version of PSO can significantly improve the approximation results in terms of the compression ratio, and the results obtained in different runs are more consistent. ?? 2003 Elsevier Inc. All rights reserved.},
author = {Yin, Peng Yeng},
nodoi = {10.1016/j.jvcir.2003.12.001},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Visual Communication and Image Representation/Yin{\_}2004.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Genetic algorithm,Global optimal solution,Local optimal solution,Particle swarm optimization,Polygonal approximation,pso},
mendeley-tags = {pso},
number = {2},
pages = {241--260},
title = {{A discrete particle swarm algorithm for optimal polygonal approximation of digital curves}},
volume = {15},
year = {2004}
}
@article{Schutte2004,
abstract = {Present day engineering optimization problems often impose large computational demands, resulting in long solution times even on a modern high-end processor. To obtain enhanced computational throughput and global search capability, we detail the coarse-grained parallelization of an increasingly popular global search method, the particle swarm optimization (PSO) algorithm. Parallel PSO performance was evaluated using two categories of optimization problems possessing multiple local minima-large-scale analytical test problems with computationally cheap function evaluations and medium-scale biomechanical system identification problems with computationally expensive function evaluations. For load-balanced analytical test problems formulated using 128 design variables, speedup was close to ideal and parallel efficiency above 95{\%} for up to 32 nodes on a Beowulf cluster. In contrast, for load-imbalanced biomechanical system identification problems with 12 design variables, speedup plateaued and parallel efficiency decreased almost linearly with increasing number of nodes. The primary factor affecting parallel performance was the synchronization requirement of the parallel algorithm, which dictated that each iteration must wait for completion of the slowest fitness evaluation. When the analytical problems were solved using a fixed number of swarm iterations, a single population of 128 particles produced a better convergence rate than did multiple independent runs performed using sub-populations (8 runs with 16 particles, 4 runs with 32 particles, or 2 runs with 64 particles). These results suggest that (1) parallel PSO exhibits excellent parallel performance under load-balanced conditions, (2) an asynchronous implementation would be valuable for real-life problems subject to load imbalance, and (3) larger population sizes should be considered when multiple processors are available.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Schutte, J. F. and Reinbolt, J. A. and Fregly, B. J. and Haftka, R. T. and George, A. D.},
nodoi = {10.1002/nme.1149},
eprint = {NIHMS150003},
file = {:D$\backslash$:/OneDrive/Articles/International Journal for Numerical Methods in Engineering/Schutte et al.{\_}2004.pdf:pdf},
isbn = {3143627344},
issn = {00295981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {Cluster computing,Parallel global optimization,Particle swarm},
number = {13},
pages = {2296--2315},
pmid = {17891226},
title = {{Parallel global optimization with the particle swarm algorithm}},
volume = {61},
year = {2004}
}
@article{Zhang2005,
abstract = {The constriction factor method (CFM) is a new variation of the basic particle swarm optimization (PSO), which has relatively better convergent nature. The effects of the major parameters on CFM were systematically investigated based on some benchmark functions. The constriction factor, velocity constraint, and population size all have significant impact on the performance of CFM for PSO. The constriction factor and velocity constraint have optimal values in practical application, and improper choice of these factors will lead to bad results. Increasing population size can improve the solution quality, although the computing time will be longer. The characteristics of CFM parameters are described and guidelines for determining parameter values are given in this paper.},
author = {Zhang, Li-ping and Yu, Huan-jun and Hu, Shang-xu},
nodoi = {10.1631/jzus.2005.A0528},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Zhejiang University SCIENCE/Zhang, Yu, Hu{\_}2005.pdf:pdf},
issn = {1009-3095},
journal = {Journal of Zhejiang University SCIENCE},
keywords = {Constriction factor method (CFM),Parameter selection,Particle swarm optimization (PSO),pso},
mendeley-tags = {pso},
number = {6},
pages = {528--534},
title = {{Optimal choice of parameters for particle swarm optimization}},
volume = {6A},
year = {2005}
}
@inproceedings{Engelbrecht2012,
abstract = {Since its birth in 1995, particle swarm optimization (PSO) has been well studied and successfully applied. While a better understanding of PSO and particle behaviors have been obtained through theoretical and empirical analysis, some issues about the beavior of particles remain unanswered. One such issue is how velocities should be initialized. Though zero initial velocities have been advocated, a popular initialization strategy is to set initial weights to random values within the domain of the optimization problem. This article first illustrates that particles tend to leave the boundaries of the search space irrespective of the initialization approach, resulting in wasted search effort. It is also shown that random initialization increases the number of roaming particles, and that this has a negative impact on convergence time. It is also shown that enforcing a boundary constraint on personal best positions does not help much to address this problem. The main objective of the article is to show that the best approach is to initialize particles to zero, or random values close to zero, without imposing a personal best bound.},
author = {Engelbrecht, Andries},
booktitle = {2012 IEEE Congress on Evolutionary Computation},
nodoi = {10.1109/CEC.2012.6256112},
file = {:D$\backslash$:/OneDrive/Articles/2012 IEEE Congress on Evolutionary Computation/Engelbrecht{\_}2012.pdf:pdf},
isbn = {978-1-4673-1509-8},
keywords = {pso},
mendeley-tags = {pso},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Particle swarm optimization: Velocity initialization}},
nourl = {http://ieeexplore.ieee.org/document/6256112/},
year = {2012}
}
@phdthesis{VanDenBergh2001,
abstract = {Many scientific, engineering and economic problems involve the optimisation of a set
of parameters. These problems include examples like minimising the losses in a power
grid by finding the optimal configuration of the components, or training a neural net-
work to recognise images of people's faces.  Numerous optimisation algorithms have
been proposed to solve these problems, with varying degrees of success. The Particle
Swarm Optimiser (PSO) is a relatively new technique that has been empirically shown
to perform well on many of these optimisation problems. This thesis presents a theo-
retical model that can be used to describe the long-term behaviour of the algorithm.
An enhanced version of the Particle Swarm Optimiser is constructed and shown to have
guaranteed convergence on local minima. This algorithm is extended further, resulting
in an algorithm with guaranteed convergence on global minima. A model for construct-
ing cooperative PSO algorithms is developed, resulting in the introduction of two new
PSO-based algorithms. Empirical results are presented to support the theoretical proper-
ties predicted by the various models, using synthetic benchmark functions to investigate
specific properties. The various PSO-based algorithms are then applied to the task of
training neural networks, corroborating the results obtained on the synthetic benchmark
functions.},
author = {{Van Den Bergh}, F.},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Van Den Bergh{\_}2001.pdf:pdf},
school = {University of Pretoria},
title = {{An analysis of particle swarm optimizers}},
year = {2001}
}
@book{Menke2012,
abstract = {MATLAB edition; third edition. Since 1984, Geophysical Data Analysis has filled the need for a short, concise reference on inverse theory for individuals who have an intermediate background in science and mathematics. The new edition maintains the accessible and succinct manner for which it is known, with the addition of: MATLAB examples and problem setsAdvanced color graphicsCoverage of new topics, including Adjoint Methods; Inversion by Steepest Descent, Monte Carlo and Simulated Annealing methods; and Bootstrap algorithm for determining empirical confidence intervalsOnline data sets and MATLAB scripts that can be used as an inverse theory tutorial. Additional material on probability, including Bayesian influence, probability density function, and metropolis algorithmDetailed discussion of application of inverse theory to tectonic, gravitational and geomagnetic studiesNumerous examples and end-of-chapter homework problems help you explore and further understand the ideas presentedUse as classroom text facilitated by a complete set of exemplary lectures in Microsoft PowerPoint format and homework problem solutions for instructors.},
author = {Menke, William},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Menke{\_}2012.pdf:pdf},
isbn = {978-0-12-397160-9},
month = {jan},
pages = {293},
publisher = {Elsevier/Academic Press},
title = {{Geophysical data analysis: Discrete inverse theory}},
nourl = {http://www.sciencedirect.com/science/book/9780123971609},
year = {2012}
}
@inproceedings{Bhat2006,
abstract = {The training of neural networks can be viewed as a problem of inference, which can be addressed from a Bayesian viewpoint. This perspective leads to a method, new to the field of particle physics, called Bayesian neural networks (BNN). After a brief overview of the method we illustrate how it can be usefully deployed in particle physics research.},
author = {Bhat, Pushpalatha C. and Prosper, Harrison B.},
booktitle = {Statistical Problems in Particle Physics, Astrophysics and Cosmology},
nodoi = {10.1142/9781860948985_0032},
file = {:D$\backslash$:/OneDrive/Articles/Statistical Problems in Particle Physics, Astrophysics and Cosmology/Bhat, Prosper{\_}2006.pdf:pdf},
isbn = {978-1-86094-649-3},
keywords = {bayesian,bayesian techniques,feedforward networks,machine learning,neural network,statistical pattern recognition},
mendeley-tags = {bayesian,machine learning,neural network},
month = {may},
number = {1},
pages = {151--154},
publisher = {PUBLISHED BY IMPERIAL COLLEGE PRESS AND DISTRIBUTED BY WORLD SCIENTIFIC PUBLISHING CO.},
title = {{Bayesian Neural Networks}},
nourl = {http://www.worldscientific.com/nodoi/abs/10.1142/9781860948985{\_}0032},
volume = {4},
year = {2006}
}
@incollection{SCHWAB1972,
author = {Schwab, F.A. and Knopoff, L.},
booktitle = {METHODS IN COMPUTATIONAL PHYSICS: Advances in Research and Applications: Volume 11: Seismology: Surface Waves and Earth Oscillations},
nodoi = {10.1016/B978-0-12-460811-5.50008-8},
file = {:D$\backslash$:/OneDrive/Articles/METHODS IN COMPUTATIONAL PHYSICS Advances in Research and Applications Volume 11 Seismology Surface Waves and Earth Oscillations/Schwab, Knopoff{\_}1972.pdf:pdf},
issn = {0076-6860},
number = {1960},
pages = {87--180},
publisher = {ACADEMIC PRESS, INC.},
title = {{Fast Surface Wave and Free Mode Computations}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/B9780124608115500088},
volume = {11},
year = {1972}
}
@article{Loshchilov2015,
abstract = {The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is often considered to be the method of choice for continuous optimization when first- and/or second- order information is available. However, the use of L-BFGS can be complicated in a black-box scenario where gradient information is not available and therefore should be numerically estimated. The accuracy of this estimation, obtained by finite difference methods, is often problem-dependent that may lead to premature convergence of the algorithm. In this paper, we demonstrate an alternative to L-BFGS, the limited memory Covariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov (2014). The LM-CMA is a stochastic derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems. Inspired by the L-BFGS, the LM-CMA samples candidate solutions according to a covariance matrix reproduced from {\$}m{\$} direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the memory complexity to {\$}O(mn){\$}, where {\$}n{\$} is the number of decision variables. The time complexity of sampling one candidate solution is also {\$}O(mn){\$}, but scales as only about 25 scalar-vector multiplications in practice. The algorithm has an important property of invariance w.r.t. strictly increasing transformations of the objective function, such transformations do not compromise its ability to approach the optimum. The LM-CMA outperforms the original CMA-ES and its large scale versions on non-separable ill-conditioned problems with a factor increasing with problem dimension. Invariance properties of the algorithm do not prevent it from demonstrating a comparable performance to L-BFGS on non-trivial large scale smooth and nonsmooth optimization problems.},
archivePrefix = {arXiv},
arxivId = {1511.00221},
author = {Loshchilov, Ilya},
eprint = {1511.00221},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Loshchilov{\_}2015.pdf:pdf},
keywords = {cmaes},
mendeley-tags = {cmaes},
month = {nov},
title = {{LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization}},
nourl = {http://arxiv.org/abs/1511.00221},
year = {2015}
}
@article{Got2003,
abstract = {We investigated the microseismicity recorded in an active volcano to infer information concerning the volcano structure and long-term dynamics, by using relative relocations and focal mechanisms of microearthquakes. There were 32,000 earthquakes of the Mauna Loa and Kilauea volcanoes recorded by more than eight stations of the Hawaiian Volcano Observatory seismic network between 1988 and 1999. We studied 17,000 of these events and relocated more than 70{\{}{\%}{\}}, with an accuracy ranging from 10 to 500 m. About 75{\{}{\%}{\}} of these relocated events are located in the vicinity of subhorizontal decollement planes, at a depth of 8--11 km. However, the striking features revealed by these relocation results are steep southeast dipping fault planes working as reverse faults, clearly located below the decollement plane and which intersect it. If this decollement plane coincides with the pre-Mauna Loa seafloor, as hypothesized by numerous authors, such reverse faults rupture the pre-Mauna Loa oceanic crust. The weight of the volcano and pressure in the magma storage system are possible causes of these ruptures, fully compatible with the local stress tensor computed by Gillard et al. [1996]. Reverse faults are suspected of producing scarps revealed by kilometer-long horizontal slip-perpendicular lineations along the decollement surface and therefore large-scale roughness, asperities, and normal stress variations. These are capable of generating stick-slip, large--magnitude earthquakes, the spatial microseismic pattern observed in the south flank of Kilauea volcano, and Hilina-type instabilities. Rupture intersecting the decollement surface, causing its large-scale roughness, may be an important parameter controlling the growth of Hawaiian volcanoes.},
author = {Got, J.-L. and Okubo, P.G.},
nodoi = {10.1029/2002JB002060},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research/Got, Okubo{\_}2003.pdf:pdf},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
keywords = {nodoi:10.102,http://dx.nodoi.org/10.1029/2002JB002060},
number = {B7},
pages = {1--13},
title = {{New insights into Kilauea's volcano dynamics brought by large-scale relative relocation of microearthquakes}},
volume = {108},
year = {2003}
}
@article{PerezSolano2014,
abstract = {In the context of near surface seismic imaging (a few hundreds of metres), we propose an alternative approach for inversion of surface waves in 2-D media with laterally varying velocities. It is based on Full Waveform Inversion (FWI) but using an alternative objective function formulated in the frequency–wavenumber f − k domain. The classical FWI objective function suffers from severe local minima problems in the presence of surface waves. It thus requires a very accurate initial model. The proposed objective function is similar to the one used in classical surface wave analysis. In this approach, the data are first split using sliding windows in the time–space t − x domain. For each window, the amplitude of the f − k spectrum is computed. The objective function measures the least-squares misfit between the amplitude of observed and modelled 2-D Fourier transformed data sets. We call this formulation the windowed-amplitude waveform inversion (w-AWI).The w-AWI objective function reduces some local minima problems as shown here through numerical examples. The global minimum basin is wider in the w-AWI approach than in FWI. Synthetic examples show that w-AWI may achieve convergence if the lowest data frequency content is twice higher than the one needed by FWI. For elastic inversion, w-AWI can be used to reconstruct a velocity model explaining surface waves. This surface wave inversion procedure can be used to retrieve near-surface model parameters in lateral-varying media.},
author = {{Perez Solano}, C. A. and Donno, D. and Chauris, H.},
nodoi = {10.1093/gji/ggu211},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Perez Solano, Donno, Chauris{\_}2014.pdf:pdf},
isbn = {0956-540X},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Inverse theory,Surface waves and free oscillations,Wave propagation,fwi,surface wave},
mendeley-tags = {fwi,surface wave},
month = {jul},
number = {3},
pages = {1359--1372},
title = {{Alternative waveform inversion for surface wave analysis in 2-D media}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggu211},
volume = {198},
year = {2014}
}
@article{Li2011,
abstract = {A new, relatively high frequency, full waveform matching method was used to study the focal mechanisms of small, local earthquakes induced in an oil field, which are monitored by a sparse near-surface network and a deep borehole network. The determined source properties are helpful for understanding the local stress regime in this field. During the waveform inversion, we maximize both the phase and amplitude matching between the observed and modeled waveforms. We also use the polarities of the first P-wave arrivals and the average S/P amplitude ratios to better constrain the matching. An objective function is constructed to include all four criteria. For different hypocenters and source types, comprehensive synthetic tests showed that our method is robust enough to determine the focal mechanisms under the current array geometries, even when there is considerable velocity inaccuracy. The application to several tens of induced microseismic events showed satisfactory waveform matching between modeled and observed seismograms. Most of the events have a strike direction parallel with the major northeast-southwest faults in the region, and some events trend parallel with the northwest-southeast conjugate faults. The results are consistent with the in situ well breakout measurements and the current knowledge on the stress direction of this region. The source mechanisms of the studied events, together with the hypocenter distribution, indicate that the microearthquakes are caused by the reactivation of preexisting faults. We observed that the faulting mechanism varies with depth, from strike-slip dominance at shallower depth to normal faulting dominance at greater depth.},
author = {Li, Junlun and {Sadi Kuleli}, H. and Zhang, Haijiang and {Nafi Toks{\"{o}}z}, M.},
nodoi = {10.1190/geo2011-0030.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Li et al.{\_}2011.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {moment tensor},
mendeley-tags = {moment tensor},
month = {nov},
number = {6},
pages = {WC87--WC101},
title = {{Focal mechanism determination of induced microearthquakes in an oil field using full waveforms from shallow and deep seismic networks}},
nourl = {http://library.seg.org/nodoi/10.1190/geo2011-0030.1},
volume = {76},
year = {2011}
}
@book{Sandham2003,
address = {Dordrecht},
nodoi = {10.1007/978-94-017-0271-3},
editor = {Sandham, William A. and Leggett, Miles},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Unknown{\_}2003.pdf:pdf},
isbn = {978-90-481-6476-9},
publisher = {Springer Netherlands},
series = {Modern Approaches in Geophysics},
title = {{Geophysical Applications of Artificial Neural Networks and Fuzzy Logic}},
nourl = {http://link.springer.com/10.1007/978-94-017-0271-3},
volume = {21},
year = {2003}
}
@article{Kuperkoch2010,
abstract = {We present an algorithm for automatic P-phase arrival time determination for local and regional seismic events based on higher order statistics (HOS). Using skewness or kurtosis a characteristic function is determined to which a new iterative picking algorithm is applied. For P-phase identification we apply the Akaike Information Criterion to the characteristic function, while for a precise determination of the P-phase arrival time a pragmatic picking algorithm is applied to a recalculated characteristic function. In addition, an automatic quality estimate is obtained, based on the slope and the signal-to-noise ratio, both calculated from the characteristic function. To get rid of erroneous picks, a Jackknife procedure and an envelope function analysis is used. The algorithm is applied to a large data set with very heterogeneous qualities of P-onsets acquired by a temporary, regional seismic network of the EGELADOS-project in the southern Aegean. The reliability and robustness of the proposed algorithm is tested by comparing more than 3000 manually derived P readings, serving as reference picks, with the corresponding automatically estimated P-wave arrival times. We find an average deviation from the reference picks of 0.26 ± 0.64 s when using kurtosis and 0.38 ± 0.75 s when using skewness. If automatically as excellent classified picks are considered only, the average difference from the reference picks is 0.07 ± 0.31 s and 0.07 ± 0.41 s, respectively. However, substantially more P-arrival times are determined when using kurtosis, indicating that the characteristic function derived from kurtosis estimation is to be preferred. Since the characteristic function is calculated recursively, the algorithm is very fast and hence suited for earthquake early warning purposes. Furthermore, a comparative study with automatically derived P-readings using Allen's and Baer {\&} Kradolfer's picking algorithms applied to the same data set demonstrates better quantitative and qualitative performance of the HOS approach. This study shows, that precise automatic P-onset determination is feasible, even when using data sets with very heterogeneous signal-to-noise ratio.},
author = {K{\"{u}}perkoch, L. and Meier, T. and Lee, J. and Friederich, W.},
nodoi = {10.1111/j.1365-246X.2010.04570.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/K{\"{u}}perkoch et al.{\_}2010.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Body waves,Early warning,Time series analysis},
number = {2},
pages = {1159--1170},
title = {{Automated determination of P-phase arrival times at regional and local distances using higher order statistics}},
volume = {181},
year = {2010}
}
@article{Bodin2012,
abstract = {We present a novel method for joint inversion of receiver functions and surface wave dispersion data, using a transdimensional Bayesian formulation. This class of algorithm treats the number of model parameters (e.g. number of layers) as an unknown in the problem. The dimension of the model space is variable and a Markov chain Monte Carlo (McMC) scheme is used to provide a parsimonious solution that fully quantifies the degree of knowledge one has about seismic structure (i.e constraints on the model, resolution, and trade-offs). The level of data noise (i.e. the covariance matrix of data errors) effectively controls the information recoverable from the data and here it naturally determines the complexity of the model (i.e. the number of model parameters). However, it is often difficult to quantify the data noise appropriately, particularly in the case of seismic waveform inversion where data errors are correlated. Here we address the issue of noise estimation using an extended Hierarchical Bayesian formulation, which allows both the variance and covariance of data noise to be treated as unknowns in the inversion. In this way it is possible to let the data infer the appropriate level of data fit. In the context of joint inversions, assessment of uncertainty for different data types becomes crucial in the evaluation of the misfit function. We show that the Hierarchical Bayes procedure is a powerful tool in this situation, because it is able to evaluate the level of information brought by different data types in the misfit, thus removing the arbitrary choice of weighting factors. After illustrating the method with synthetic tests, a real data application is shown where teleseismic receiver functions and ambient noise surface wave dispersion measurements from the WOMBAT array (South-East Australia) are jointly inverted to provide a probabilistic 1D model of shear-wave velocity beneath a given station.},
author = {Bodin, T. and Sambridge, M. and Tkal{\v{c}}i{\'{c}}, H. and Arroucau, P. and Gallagher, K. and Rawlinson, N.},
nodoi = {10.1029/2011JB008560},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Bodin et al.{\_}2012.pdf:pdf},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
month = {feb},
number = {B2},
title = {{Transdimensional inversion of receiver functions and surface wave dispersion}},
nourl = {http://nodoi.wiley.com/10.1029/2011JB008560},
volume = {117},
year = {2012}
}
@inproceedings{Loshchilov2014,
abstract = {We propose a computationally efficient limited memory Covariance Matrix Adaptation Evolution Strategy for large scale optimization, which we call the LM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems in continuous domain. Inspired by the limited memory BFGS method of Liu and Nocedal (1989), the LM-CMA-ES samples candidate solutions according to a covariance matrix reproduced from {\$}m{\$} direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the time and memory complexity of the sampling to {\$}O(mn){\$}, where {\$}n{\$} is the number of decision variables. When {\$}n{\$} is large (e.g., {\$}n{\$} {\textgreater} 1000), even relatively small values of {\$}m{\$} (e.g., {\$}m=20,30{\$}) are sufficient to efficiently solve fully non-separable problems and to reduce the overall run-time.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1404.5520},
author = {Loshchilov, Ilya},
booktitle = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
nodoi = {10.1145/2576768.2598294},
eprint = {1404.5520},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14/Loshchilov{\_}2014.pdf:pdf},
isbn = {9781450326629},
keywords = {cma-es,cmaes,evolution strategies,large scale optimization},
mendeley-tags = {cmaes},
pages = {397--404},
publisher = {ACM Press},
title = {{A computationally efficient limited memory CMA-ES for large scale optimization}},
nourl = {http://arxiv.org/abs/1404.5520 http://dl.acm.org/citation.cfm?doid=2576768.2598294},
year = {2014}
}
@inproceedings{Kennedy1999,
abstract = {The study manipulated the neighborhood topologies of particle swarms optimizing four test functions. Several social network structures were tested, with "small-world" randomization of a specified number of links. Sociometric structure and the small-world manipulation interacted with function to produce a significant effect on performance.},
author = {Kennedy, James},
booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},
nodoi = {10.1109/CEC.1999.785509},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)/Kennedy{\_}1999.pdf:pdf},
isbn = {0-7803-5536-9},
keywords = {pso},
mendeley-tags = {pso},
pages = {1931--1938},
publisher = {IEEE},
title = {{Small worlds and mega-minds: effects of neighborhood topology on particle swarm performance}},
nourl = {http://ieeexplore.ieee.org/document/785509/},
volume = {3},
year = {1999}
}
@article{Panagiotakis2008,
abstract = {In this paper, we propose a method for the automatic identification of P-phase arrival based on the distribution of local maxima (LM) in earthquake seismograms. The method efficiently combines energy and frequency characteristics of the LM distribution (LMD). The detection is mainly based on the energy of a seismic event in the case the earthquake has higher amplitude than seismic background noise. Otherwise, it is based on the frequency of LM. Thus, the method provides robust detection of P-phase arrival in any quality type of seismic data. Moreover, it uses two sequential sliding signal windows yielding very high accuracy on the P-phase estimation. A hierarchical P-phase detection algorithm dramatically reduces the computational cost, making possible a real-time implementation. Experimental results from a large database of more than 80 low, medium, and high signal-to-noise ratio seismic events and comparison with existing methods in the literature indicate the reliable performance of the proposed scheme.},
author = {Panagiotakis, Costas and Kokinou, Eleni and Vallianatos, Filippos},
nodoi = {10.1109/TGRS.2008.917272},
file = {:D$\backslash$:/OneDrive/Articles/IEEE Transactions on Geoscience and Remote Sensing/Panagiotakis, Kokinou, Vallianatos{\_}2008.pdf:pdf},
isbn = {0196-2892},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Automatic picking,P-phase arrival identification,Seismic-signal analysis,Signal segmentation,picking},
mendeley-tags = {picking},
month = {aug},
number = {8},
pages = {2280--2287},
title = {{Automatic P-Phase Picking Based on Local-Maxima Distribution}},
nourl = {http://ieeexplore.ieee.org/document/4554252/},
volume = {46},
year = {2008}
}
@article{Zhang2015,
abstract = {Particle swarm optimization (PSO) is a heuristic global optimization method, proposed originally by Kennedy and Eberhart in 1995. It is now one of the most commonly used optimization techniques. This survey presented a comprehensive investigation of PSO. On one hand, we provided advances with PSO, including its modifications (including quantum-behaved PSO, bare-bones PSO, chaotic PSO, and fuzzy PSO), population topology (as fully connected, von Neumann, ring, star, random, etc.), hybridization (with genetic algorithm, simulated annealing, Tabu search, artificial immune system, ant colony algorithm, artificial bee colony, differential evolution, harmonic search, and biogeography-based optimization), extensions (to multiobjective, constrained, discrete, and binary optimization), theoretical analysis (parameter selection and tuning, and convergence analysis), and parallel implementation (in multicore, multiprocessor, GPU, and cloud computing forms). On the other hand, we offered a survey on applications of PSO to the following eight fields: electrical and electronic engineering, automation control systems, communication theory, operations research, mechanical engineering, fuel and energy, medicine, chemistry, and biology. It is hoped that this survey would be beneficial for the researchers studying PSO algorithms.},
archivePrefix = {arXiv},
arxivId = {931256},
author = {Zhang, Yudong and Wang, Shuihua and Ji, Genlin},
nodoi = {10.1155/2015/931256},
eprint = {931256},
file = {:D$\backslash$:/OneDrive/Articles/Mathematical Problems in Engineering/Zhang, Wang, Ji{\_}2015.pdf:pdf},
isbn = {0300-5410},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
pages = {1--38},
title = {{A Comprehensive Survey on Particle Swarm Optimization Algorithm and Its Applications}},
nourl = {http://www.hindawi.com/journals/mpe/2015/931256/},
volume = {2015},
year = {2015}
}
@inproceedings{Clerc1999,
abstract = {A very simple particle swarm optimization iterative algorithm is presented, with just one equation and one social/confidence parameter. We define a “no-hope” convergence criterion and a “rehope” method so that, from time to time, the swarm re-initializes its position, according to some gradient estimations of the objective function and to the previous re-initialization (it means it has a kind of very rudimentary memory). We then study two different cases, a quite “easy” one (the Alpine function) and a “difficult” one (the Banana function), but both just in dimension two. The process is improved by taking into account the swarm gravity center (the “queen”) and the results are good enough so that it is certainly worthwhile trying the method on more complex problems},
author = {Clerc, Maurice},
booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},
nodoi = {10.1109/CEC.1999.785513},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)/Clerc{\_}1999.pdf:pdf},
isbn = {0-7803-5536-9},
keywords = {constriction,pso},
mendeley-tags = {constriction,pso},
pages = {1951--1957},
publisher = {IEEE},
title = {{The swarm and the queen: towards a deterministic and adaptive particle swarm optimization}},
nourl = {http://ieeexplore.ieee.org/document/785513/},
volume = {3},
year = {1999}
}
@article{Vassallo2012,
abstract = {In this paper we introduce an optimization scheme for choosing the most appropriate set of parameters for a picking algorithm by using real picks and data acquired by a specific seismic network. The optimal model is chosen through searching in the global parameter space of the maximum of an objective function that depends on the comparison between automatic picks and manual picks performed on a dataset representative for a seismic network. We show applications to two STA/LTA algorithms: the Allen (1978) picker and the new FilterPicker algorithm (Lomax et al. 2012). Automatic Picker Developments and Optimization: A Strategy for Improving the Performances of Automatic Phase Pickers (PDF Download Available).},
author = {Vassallo, Maurizio and Satriano, Claudio and Lomax, Anthony},
nodoi = {10.1785/gssrl.83.3.541},
file = {:D$\backslash$:/OneDrive/Articles/Seismological Research Letters/Vassallo, Satriano, Lomax{\_}2012.pdf:pdf},
issn = {0895-0695},
journal = {Seismological Research Letters},
keywords = {picking},
mendeley-tags = {picking},
month = {may},
number = {3},
pages = {541--554},
title = {{Automatic Picker Developments and Optimization: A Strategy for Improving the Performances of Automatic Phase Pickers}},
nourl = {http://srl.geoscienceworld.org/cgi/nodoi/10.1785/gssrl.83.3.541},
volume = {83},
year = {2012}
}
@article{FernandezMartinez2012,
abstract = {History matching provides to reservoir engineers an improved spatial distribution of physical properties to be used in forecasting the reservoir response for field management. The ill-posed character of the history-matching problem yields nonuniqueness and numerical instabilities that increase with the reservoir complexity. These features might cause local optimization methods to provide unpredictable results not being able to discriminate among the multiple models that fit the observed data (production history). Also, the high dimensionality of the inverse problem impedes estimation of uncertainties using classical Markov-chain Monte Carlo methods. We attenuated the ill-conditioned character of this history-matching inverse problem by reducing the model complexity using a spatial principal component basis and by combining as observables flow production measurements and time-lapse seismic crosswell tomographic images. Additionally the inverse problem was solved in a stochastic framework. For this purpose, we used a family of particle swarm optimization (PSO) optimizers that have been deduced from a physical analogy of the swarm system. For a synthetic sand-and-shale reservoir, we analyzed the performance of the different PSO optimizers, both in terms of exploration and convergence rate for two different reservoir models with different complexity and under the presence of different levels of white Gaussian noise added to the synthetic observed data. We demonstrated that PSO optimizers have a very good convergence rate for this example, and provide in addition, approximate measures of uncertainty around the optimum facies model. The PSO algorithms are robust in presence of noise, which is always the case for real data.},
author = {{Fern{\'{a}}ndez Mart{\'{i}}nez}, Juan Luis and Mukerji, Tapan and {Garc{\'{i}}a Gonzalo}, Esperanza and Suman, Amit},
nodoi = {10.1190/geo2011-0041.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Fern{\'{a}}ndez Mart{\'{i}}nez et al.{\_}2012.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {pso},
mendeley-tags = {pso},
month = {jan},
number = {1},
pages = {M1--M16},
title = {{Reservoir characterization and inversion uncertainty via a family of particle swarm optimizers}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/geo2011-0041.1 http://library.seg.org/nodoi/10.1190/geo2011-0041.1},
volume = {77},
year = {2012}
}
@article{Barros2015,
abstract = {ABSTRACTThe common-reflection surface (CRS) method is a sophisticated alternative to the traditional common-midpoint stacking because its traveltime approximation allows for the use of more traces than the normal moveout. This in turn requires more parameters for the moveout description, thus increasing the computational burden of the parameter estimation. In the literature, a suboptimal strategy is often used, which decreases the complexity but, as we found in this work, compromises the accuracy of the parameters in some cases. To cope with this problem, in this work, we have devised detailed information for efficient estimation of the CRS parameters using the differential evolution (DE) global optimization algorithm. Because we used data sets with low fold and low signal-to-noise ratio, from which no reliable velocity analysis could be easily performed, we applied this algorithm in a fully automatic global search, i.e., without any velocity guide. The results for a 2D real data set from Brazil indicated...},
author = {Barros, Tiago and Ferrari, Rafael and Krummenauer, Rafael and Lopes, Renato},
nodoi = {10.1190/geo2015-0032.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Barros et al.{\_}2015.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {common-midpoint (CMP),de,multiparameter,normal moveout (NMO),signal processing,statistics.},
mendeley-tags = {de},
number = {6},
pages = {WD189--WD200},
title = {{Differential evolution-based optimization procedure for automatic estimation of the common-reflection surface traveltime parameters}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/geo2015-0032.1},
volume = {80},
year = {2015}
}
@article{Moser1992,
abstract = {The shortest path method [Moser, 1991 ] for the calculation of seismic ray paths and travel times along them' can be applied directly in the hypocenter location method proposed by Tarantola and Valette [1982]. It uses the analogy between seismic rays in the Earth and shortest paths in networks to construct first arrival times from one point to all other points of a three-dimensional grid simultaneously in a fast, robust way, in Earth models of arbitrary complexity. Doing this for all stations of a seismic array, one can find the hypocenter location by minimizing the difference between the observed and the calculated travel times at the stations over the three-dimensional grid. The concept of probability density functions allows then for a fully nonlinear examination of the uncertainties in the hypocenter location, due to uncertainties in the travel time data, numerical errors in the calculated travel times and, to a limited extent, incomplete knowledge about the Earth model. The result is a three-dimensional contour map of regions of equal confidence for the earthquake location. The method becomes especially attractive when more than one event recorded by the same array is studied, because the calculation of the travel times, which is relatively the most time consuming operation, has to be done only once. The method is applied on the location of an event that occurred on January 18, 1989, in Israel.},
author = {Moser, T. J. and van Eck, T. and Nolet, G.},
nodoi = {10.1029/91JB03176},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research/Moser, van Eck, Nolet{\_}1992.pdf:pdf},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
keywords = {location},
mendeley-tags = {location},
number = {B5},
pages = {6563},
title = {{Hypocenter determination in strongly heterogeneous Earth models using the shortest path method}},
nourl = {http://nodoi.wiley.com/10.1029/91JB03176},
volume = {97},
year = {1992}
}
@article{Sambridge1998,
abstract = {A discussion of methodologies for nonlinear geophysical inverse problems is presented. Geophysical inverse problems are often posed as optimization problems in a finite- dimensional parameter space. An Earth model is usually described by a set of parameters representing one or more geophysical properties (e.g. the speed with which seismic waves travel through the Earth's interior). Earth models are sought by minimizing the discrepancies between observation and predictions from the model, possibly, together with some regularizing constraint. The resulting optimization problem is usually nonlinear and often highly so, which may lead to multiple minima in the misfit landscape. Global (stochastic) optimization methods have become popular in the past decade. A discussion of simulated annealing, genetic algorithms and evolutionary programming methods is presented in the geophysical context. Less attention has been paid to assessing how well constrained, or resolved, individual parameters are. Often this problem is poorly posed. A new class of method is presented which offers potential in both the optimization and the ‘error analysis' stage of the inversion. This approach uses concepts from the field of computational geometry. The search algorithm described here does not appear to be practical in problems with dimension much greater than 10.},
author = {Sambridge, Malcolm},
nodoi = {10.1088/0266-5611/14/3/005},
file = {:D$\backslash$:/OneDrive/Articles/Inverse Problems/Sambridge{\_}1998.pdf:pdf},
isbn = {0266-5611},
issn = {0266-5611},
journal = {Inverse Problems},
keywords = {inversion},
mendeley-tags = {inversion},
month = {jun},
number = {3},
pages = {427--440},
title = {{Exploring multidimensional landscapes without a map}},
nourl = {http://stacks.iop.org/0266-5611/14/i=3/a=005?key=crossref.bf74b2bb29a53426b1d85fee8e3bccd3},
volume = {14},
year = {1998}
}
@article{Akram2016,
abstract = {We have evaluated arrival-time picking algorithms for downhole microseismic data. The picking algorithms that we considered may be classified as window-based single-level methods (e.g., energy-ratio [ER] methods), nonwindow-based single-level methods (e.g., Akaike information criterion), multilevel-or array-based methods (e.g., crosscorrelation ap-proaches), and hybrid methods that combine a number of single-level methods (e.g., Akazawa's method). We have determined the key parameters for each algorithm and devel-oped recommendations for optimal parameter selection based on our analysis and experience. We evaluated the performance of these algorithms with the use of field examples from a downhole microseismic data set recorded in western Canada as well as with pseudo-synthetic microseismic data generated by adding 100 realizations of Gaussian noise to high signal-to-noise ratio microseismic waveforms. ER-based algorithms were found to be more efficient in terms of computational speed and were therefore recommended for real-time micro-seismic data processing. Based on the performance on pseudo-synthetic and field data sets, we found statistical, hybrid, and multilevel crosscorrelation methods to be more efficient in terms of accuracy and precision. Pick errors for S-waves are reduced significantly when data are preconditioned by ap-plying a transformation into ray-centered coordinates.},
author = {Akram, Jubran and Eaton, David W},
nodoi = {10.1190/geo2014-0500.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Akram, Eaton{\_}2016.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {picking},
mendeley-tags = {picking},
month = {mar},
number = {2},
pages = {KS71--KS91},
title = {{A review and appraisal of arrival-time picking methods for downhole microseismic data}},
nourl = {http://library.seg.org/nodoi/10.1190/geo2014-0500.1},
volume = {81},
year = {2016}
}
@article{DAgostino1990,
abstract = {For testing that an underlying population is normally distributed the skewness and kurtosis statistics, {\$}\backslashsqrt{\{}b{\_}1{\}}{\$} and b2, and the D'Agostino-Pearson K2 statistic that combines these two statistics have been shown to be powerful and informative tests. Their use, however, has not been as prevalent as their usefulness. We review these tests and show how readily available and popular statistical software can be used to implement them. Their relationship to deviations from linearity in normal probability plotting is also presented.},
author = {D'Agostino, Ralph B. and Belanger, Albert and D'Agostino, Ralph B.},
nodoi = {10.2307/2684359},
file = {:D$\backslash$:/OneDrive/Articles/The American Statistician/D'Agostino, Belanger, D'Agostino{\_}1990.pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
keywords = {statistics},
mendeley-tags = {statistics},
month = {nov},
number = {4},
pages = {316},
title = {{A Suggestion for Using Powerful and Informative Tests of Normality}},
nourl = {http://www.jstor.org/stable/2684359?origin=crossref},
volume = {44},
year = {1990}
}
@book{Tarantola2005,
abstract = {While the prediction of observations is a forward problem, the use of actual observations to infer the properties of a model is an inverse problem. Inverse problems are difficult because they may not have a unique solution. The description of uncertainties plays a central role in the theory, which is based on probability theory. This book proposes a general approach that is valid for linear as well as for nonlinear problems. The philosophy is essentially probabilistic and allows the reader to understand the basic difficulties appearing in the resolution of inverse problems. The book attempts to explain how a method of acquisition of information can be applied to actual real-world problems, and many of the arguments are heuristic. Prompted by recent developments in inverse theory, Inverse Problem Theory and Methods for Model Parameter Estimation is a completely rewritten version of a 1987 book by the same author. In this version there are lots of algorithmic details for Monte Carlo methods, least-squares discrete problems, and least-squares problems involving functions. In addition, some notions are clarified, the role of optimization techniques is underplayed, and Monte Carlo methods are taken much more seriously. The first part of the book deals exclusively with discrete inverse problems with a finite number of parameters while the second part of the book deals with general inverse problems.},
author = {Tarantola, Albert},
booktitle = {Society for Industrial and Applied Mathematics},
nodoi = {10.1137/1.9780898717921},
file = {:D$\backslash$:/OneDrive/Articles/Society for Industrial and Applied Mathematics/Tarantola{\_}2005.pdf:pdf},
isbn = {978-0-89871-572-9},
issn = {0001-4966},
keywords = {Inverse problems,inverse methods,inversion,least-squares,mcmc,probability,uncertainties},
mendeley-tags = {inversion,mcmc},
month = {jan},
pages = {1816--24},
pmid = {17069280},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Inverse Problem Theory and Methods for Model Parameter Estimation}},
nourl = {http://www.ncbi.nlm.nih.gov/pubmed/17069280 http://epubs.siam.org/nodoi/book/10.1137/1.9780898717921},
volume = {120},
year = {2005}
}
@article{Hansen2010,
abstract = {We present a novel method for handling uncertainty in evolutionary optimization. The method entails quantification and treatment of uncertainty and relies on the rank based selection operator of evolutionary algorithms. The proposed uncertainty handling is implemented in the context of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and verified on test functions. The present method is independent of the uncertainty distribution, prevents premature convergence of the evolution strategy and is well suited for online optimization as it requires only a small number of additional function evaluations. The algorithm is applied in an experimental set-up to the online optimization of feedback controllers of thermoacoustic instabilities of gas turbine combustors. In order to mitigate these instabilities, gain-delay or model-based Hinfty controllers sense the pressure and command secondary fuel injectors. The parameters of these controllers are usually specified via a trial and error procedure. We demonstrate that their online optimization with the proposed methodology enhances, in an automated fashion, the online performance of the controllers, even under highly unsteady operating conditions, and it also compensates for uncertainties in the model-building and design process.},
author = {Hansen, Nikolaus},
file = {:D$\backslash$:/OneDrive/Articles/IEEE Transactions on Evolutionary Computation/Hansen{\_}2010.pdf:pdf},
isbn = {0999999982},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {cmaes},
mendeley-tags = {cmaes},
number = {2},
pages = {180--197},
title = {{Errata / Addenda for A Method for Handling Uncertainty in Evolutionary Optimization With an Application to Feedback Control of Combustion}},
volume = {13},
year = {2010}
}
@article{Trelea2003,
abstract = {The particle swarm optimization algorithm is analyzed using standard results from the dynamic system theory. Graphical parameter selection guidelines are derived. The exploration-exploitation tradeoff is discussed and illustrated. Examples of performance on benchmark functions superior to previously published results are given. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Trelea, Ioan Cristian},
nodoi = {10.1016/S0020-0190(02)00447-7},
file = {:D$\backslash$:/OneDrive/Articles/Information Processing Letters/Trelea{\_}2003.pdf:pdf},
isbn = {0020-0190},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {Analysis of algorithms,Parallel algorithms,Particle swarm optimization,Stochastic optimization,pso},
mendeley-tags = {pso},
number = {6},
pages = {317--325},
title = {{The particle swarm optimization algorithm: Convergence analysis and parameter selection}},
volume = {85},
year = {2003}
}
@article{Poliannikov2011,
abstract = {Hydraulic fracturing is the process of injecting high-pressure fluids into a reservoir to induce fractures and thus improve reservoir productivity. Microseismic event localization is used to locate created fractures. Traditionally, events are localized individually. Available infor- mation about events already localized is not used to help estimate other source locations. Traditional localization methods yield an uncertainty that is inversely proportional to the square root of the number of receivers. However, in applications where multiple fractures are created, multiple sources in a reference fracture may provide redundant information about unknown events in subsequent fractures that can boost the signal-to-noise ratio, im- proving estimates of the event positions. We propose to use sources in fractures closer to the monitoring well to help localize events further away. It is known through seismic inter- ferometry that with a 2D array of receivers, the travel time between two sources may be recovered from a cross-correlogram of two common source gathers. This allows an event in the second fracture to be localized relative to an event in the reference fracture. A difficulty arises when receivers are located in a single monitoring well. When the receiver array is 1D, classical interferometry cannot be directly employed because the problem becomes un- derdetermined. In our approach, interferometry is used to partially redatum microseismic events from the second fracture onto the reference fracture so that they can be used as virtual receivers, providing additional information complementary to that provided by the physical receivers. Our error analysis shows that, in addition to the gain obtained by having multiple physical receivers, the location uncertainty is inversely proportional to the square root of the number of sources in the reference fracture. Since the number of microseism sources is usually high, the proposed method will usually result in more accurate location estimates as compared to the traditional methods.},
author = {Poliannikov, Oleg V. and {E. Malcolm}, Alison and Djikpesse, Hugues and Prange, Michael},
nodoi = {10.1190/geo2010-0325.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Poliannikov et al.{\_}2011.pdf:pdf},
issn = {00168033},
journal = {Geophysics},
number = {6},
pages = {WC27},
title = {{Interferometric hydrofracture microseism localization using neighboring fracture}},
volume = {76},
year = {2011}
}
@incollection{LeCun1998,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"{u}}ller, Klaus -Robert},
nodoi = {10.1007/3-540-49430-8_2},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/LeCun et al.{\_}1998.pdf:pdf},
keywords = {machine learning,neural network},
mendeley-tags = {machine learning,neural network},
pages = {9--50},
title = {{Efficient BackProp}},
nourl = {http://link.springer.com/10.1007/3-540-49430-8{\_}2},
volume = {75},
year = {1998}
}
@inproceedings{Rabenseifner2009,
abstract = {Today most systems in high-performance computing (HPC) feature a hierarchical hardware design: Shared memory nodes with several multi-core CPUs are connected via a network infrastructure. Parallel programming must combine distributed memory parallelization on the node interconnect with shared memory parallelization inside each node. We describe potentials and challenges of the dominant programming models on hierarchically structured hardware: Pure MPI (message passing interface), pure OpenMP (with distributed shared memory extensions) and hybrid MPI+OpenMP in several flavors. We pinpoint cases where a hybrid programming model can indeed be the superior solution because of reduced communication needs and memory consumption, or improved load balance. Furthermore we show that machine topology has a significant impact on performance for all parallelization strategies and that topology awareness should be built into all applications in the future. Finally we give an outlook on possible standardization goals and extensions that could make hybrid programming easier to do with performance in mind.},
author = {Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
booktitle = {2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
nodoi = {10.1109/PDP.2009.43},
file = {:D$\backslash$:/OneDrive/Articles/2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing/Rabenseifner, Hager, Jost{\_}2009.pdf:pdf},
isbn = {978-0-7695-3544-9},
issn = {1066-6192},
number = {c},
pages = {427--436},
publisher = {IEEE},
title = {{Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes}},
nourl = {http://ieeexplore.ieee.org/document/4912964/},
year = {2009}
}
@incollection{Arabas2010,
abstract = {In this paper we show that the technique of handling bound-ary constraints has a significant influence on the efficiency of the Differential Evolution method. We study the effects of applying several such techniques taken from the literature. The comparison is based on ex-periments performed for a standard DE/rand/1/bin strategy using the CEC2005 benchmark. The paper reports the results of experiments and provides their simple statistical analysis. Among several constraint han-dling methods, a winning approach is to repeat the differential mutation by resampling the population until a feasible mutant is obtained. Coupling the aforementioned method with a simple DE/rand/1/bin strategy allows to achieve results that outperform in many cases results of almost all other methods tested during the CEC2005 competition, including the original DE/rand/1/bin strategy.},
address = {Berlin, Heidelberg},
author = {Arabas, Jaros{\l}law and Szczepankiewicz, Adam and Wroniak, Tomasz},
booktitle = {Parallel Problem Solving from Nature, PPSN XI},
nodoi = {10.1007/978-3-642-15871-1_42},
file = {:D$\backslash$:/OneDrive/Articles/Parallel Problem Solving from Nature, PPSN XI/Arabas, Szczepankiewicz, Wroniak{\_}2010.pdf:pdf},
isbn = {3642158706},
issn = {03029743},
keywords = {de},
mendeley-tags = {de},
number = {PART 2},
pages = {411--420},
publisher = {Springer Berlin Heidelberg},
title = {{Experimental Comparison of Methods to Handle Boundary Constraints in Differential Evolution}},
nourl = {http://link.springer.com/10.1007/978-3-642-15871-1{\_}42},
volume = {6239 LNCS},
year = {2010}
}
@article{Bishop1995,
abstract = {Bayesian techniques have been developed over many years in a range of different fields, but have only recently been applied to the problem of learning in neural networks. As well as providing a consistent framework for statistical pattern recognition, the Bayesian approach offers a number of practical advantages including a potential solution to the problem of over-fitting. This chapter aims to provide an introductory overview of the application of Bayesian methods to neural networks. It assumes the reader is familiar with standard feed-forward network models and how to train them using conventional techniques.},
author = {Bishop, Christopher M},
file = {:D$\backslash$:/OneDrive/Articles/Machine Learning/Bishop{\_}1995.pdf:pdf},
journal = {Machine Learning},
keywords = {neural network},
mendeley-tags = {neural network},
number = {1},
pages = {1--11},
title = {{Bayesian methods for neural networks}},
nourl = {http://eprints.aston.ac.uk/1131/},
volume = {7},
year = {1995}
}
@article{Neal1992,
abstract = {It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the "Hybrid Monte Carlo" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of "free energy" differences, it should also be possible to compare the merits of different network architectures. The work described here should also be applicable to a wide variety of statistical models other than neural networks.},
author = {Neal, Radford M.},
nodoi = {10.1.1.53.5868},
file = {:D$\backslash$:/OneDrive/Articles/Dept. of Computer Science, University of Toronto, Tech. {\ldots}/Neal{\_}1992.pdf:pdf},
isbn = {CRG-TR-92-1},
journal = {Dept. of Computer Science, University of Toronto, Tech. {\ldots}},
keywords = {bayesian,mcmc,neural network},
mendeley-tags = {bayesian,mcmc,neural network},
pages = {1--21},
title = {{Bayesian training of backpropagation networks by the hybrid Monte Carlo method}},
nourl = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Bayesian+Training+of+Backpropagation+Networks+by+the+Hybrid+Monte+Carlo+Method{\#}0},
year = {1992}
}
@inproceedings{Gesret2011,
abstract = {Among many factors that contribute to microseismic location errors, the largest contribution is due to the lack of knowledge of the wave-propagation medium. In spite of efforts to build the " best " velocity model derived from surface seismic and/or logging data, these models are very often not adapted to the microseismic context and are characterized by numerous uncertainties. These uncertainties are often enhanced due to the poor aperture of the microseismic monitoring networks. Precise location of hypocenters requires deriving a very accurate velocity model using calibration shots; the inversion to obtain this model is a difficult task but cannot be neglected. We propose a tomography algorithm using calibrations shots that does not produce only a unique " best " velocity model but all velocity models that explain the observed data within the traveltime picking uncertainties. This approach allows deriving location uncertainties associated to velocity model uncertainties. These maps show that the commonly used probability associated to the picking uncertainties must not be used to represent the probability associated to the velocity model uncertainties.},
author = {Gesret, A. and Noble, M. and Desassis, N. and Romary, T.},
booktitle = {Third Passive Seismic Workshop – Actively Passive!},
file = {:D$\backslash$:/OneDrive/Articles/Third Passive Seismic Workshop – Actively Passive!/Gesret et al.{\_}2011.pdf:pdf},
isbn = {9781629937915},
number = {March 2011},
pages = {PAS31},
title = {{Microseismic Monitoring : Consequences of Velocity Model Uncertainties on Location Uncertainties}},
year = {2011}
}
@article{Saragiotis2013,
abstract = {Event picking is used in many steps of seismic processing. We present an automatic event picking method that is based on a new attribute of seismic signals, instantaneous travel-time. The calculation of the instantaneous traveltime con-sists of two separate but interrelated stages. First, a trace is mapped onto the time-frequency domain. Then the time-frequency representation is mapped back onto the time domain by an appropriate operation. The computed instan-taneous traveltime equals the recording time at those in-stances at which there is a seismic event, a feature that is used to pick the events. We analyzed the concept of the in-stantaneous traveltime and demonstrated the application of our automatic picking method on dynamite and Vibroseis field data.},
author = {Saragiotis, Christos and Alkhalifah, Tariq and Fomel, Sergey},
nodoi = {10.1190/geo2012-0026.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Saragiotis, Alkhalifah, Fomel{\_}2013.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {picking},
mendeley-tags = {picking},
month = {mar},
number = {2},
pages = {T53--T58},
title = {{Automatic traveltime picking using instantaneous traveltime}},
nourl = {http://library.seg.org/nodoi/10.1190/geo2012-0026.1},
volume = {78},
year = {2013}
}
@inproceedings{Poliannikov2016,
abstract = {We study the problem of the moment tensor inversion of a double-couple microseismic source from observed S/P amplitude ratios. The emphasis of this work is on uncertainty quantification that includes the effect of the uncertain event location. We use a Bayesian approach to quantify the uncertainty of the fault plane solution. The posterior distribution is effectively calculated by sampling from the posterior distribution of the event location, and performing a moment-tensor inversion using individual samples. The uncertainty in the reconstructed moment tensor depends on the receiver geometry, signal noise, and the true moment tensor. After a suitable transformation of the input data, the problem can be reduced to a classical least-squares estimation problem.},
author = {Poliannikov, Oleg and Fehler, Michael and Rodi, William},
booktitle = {SEG Technical Program Expanded Abstracts 2016},
nodoi = {10.1190/segam2016-13875371.1},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2016/Poliannikov, Fehler, Rodi{\_}2016.pdf:pdf},
keywords = {3D,earthquake,induced seismicity,microseismic,uncertainty quantification},
mendeley-tags = {uncertainty quantification},
month = {sep},
pages = {2545--2549},
publisher = {Society of Exploration Geophysicists},
title = {{Quantifying the uncertainty in fault plane solutions inferred from S/P amplitude ratios}},
nourl = {http://library.seg.org/nodoi/10.1190/segam2016-13875371.1},
year = {2016}
}
@article{Mikki2008,
author = {Mikki, Said M and Kishk, Ahmed A},
nodoi = {10.2200/S00110ED1V01Y200804CEM020},
file = {:D$\backslash$:/OneDrive/Articles/Synthesis Lectures on Computational Electromagnetics/Mikki, Kishk{\_}2008.pdf:pdf},
issn = {1932-1252},
journal = {Synthesis Lectures on Computational Electromagnetics},
month = {jan},
number = {1},
pages = {1--103},
title = {{Particle Swarm Optimization: A Physics-Based Approach}},
nourl = {http://www.morganclaypool.com/nodoi/abs/10.2200/S00110ED1V01Y200804CEM020},
volume = {3},
year = {2008}
}
@article{Reynen2017,
author = {Reynen, Andrew and Audet, Pascal},
nodoi = {10.1093/gji/ggx238},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Reynen, Audet{\_}2017.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Numerical approximations and analysis,Time-series analysis,machine learning},
mendeley-tags = {machine learning},
number = {3},
pages = {1394--1409},
title = {{Supervised machine learning on a network scale: Application to seismic event classification and detection}},
volume = {210},
year = {2017}
}
@inproceedings{Subbey2003,
abstract = {This paper will describe a strategy for rapid quantification of uncertainty in reservoir performance prediction. The strategy is based on a combination of streamline and conventional finite difference simulators. Our uncertainty framework uses the Neighbourhood Approximation algorithm to generate an ensemble of history match models, and has been described previously. A speedup in generating the misfit surface is essential since effective quantification of uncertainty can require thousands of reservoir model runs. Our speedup strategy for quantifying uncertainty in performance prediction involves using an approximate streamline simulator to rapidly explore the parameter space to identify good history matching regions, and to generate an approximate misfit surface. We then switch to a conventional, finite difference simulator, and selectively explore the identified parameter space regions. This paper will show results from a parallel version of the Neighbourhood Approximation algorithm on a Linux cluster, demonstrating the advantages of perfect parallelism. We show how it is possible to sample from the posterior probability distribution both to assess accuracy of the approximate misfit surface, and also to generate automatic history match models.},
author = {Subbey, Sam and Mike, Christie and Sambridge, Malcolm},
booktitle = {SPE Reservoir Simulation Symposium},
nodoi = {10.2118/79678-MS},
file = {:D$\backslash$:/OneDrive/Articles/SPE Reservoir Simulation Symposium/Subbey, Mike, Sambridge{\_}2003.pdf:pdf},
keywords = {uncertainty quantification},
mendeley-tags = {uncertainty quantification},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{A Strategy for Rapid Quantification of Uncertainty in Reservoir Performance Prediction}},
nourl = {http://www.onepetro.org/nodoi/10.2118/79678-MS},
year = {2003}
}
@inproceedings{Kennedy1997,
abstract = {The particle swarm algorithm adjusts the trajectories of a population of "particles" through a problem space on the basis of information about each particle's previous best performance and the best previous performance of its neighbors. Previous versions of the particle swarm have operated in continuous space, where trajectories are defined as changes in position on some number of dimensions. The paper reports a reworking of the algorithm to operate on discrete binary variables. In the binary version, trajectories are changes in the probability that a coordinate will take on a zero or one value. Examples, applications, and issues are discussed.},
author = {Kennedy, J. and Eberhart, R.C.},
booktitle = {1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation},
nodoi = {10.1109/ICSMC.1997.637339},
file = {:D$\backslash$:/OneDrive/Articles/1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation/Kennedy, Eberhart{\_}1997.pdf:pdf},
isbn = {0-7803-4053-1},
issn = {1062-922X},
keywords = {pso},
mendeley-tags = {pso},
pages = {4104--4108},
publisher = {IEEE},
title = {{A discrete binary version of the particle swarm algorithm}},
nourl = {http://ieeexplore.ieee.org/document/637339/},
volume = {5},
year = {1997}
}
@article{Maity2014,
abstract = {Microseismic monitoring is an increasingly common geophysical tool to monitor the changes in the subsurface. Autopicking involving phase arrival detection is a common element in microseismic data processing schemes and is necessary for accurate estimation of event locations as well as other workflows such as tomographic or moment tensor inversion, etc. The quality of first arrival picking is dependent on the actual seismic waveform, which in turn is related to the near surface and subsurface structure, source type, noise conditions, environmental factors, and monitoring array design, etc. We have developed a new hybrid autopicking workflow which makes use of multiple derived attributes from the seismic data and combines them within an artificial neural network framework. An evolutionary algorithm scheme is used as the network training algorithm. The autopicker has been tested and its applicability has been validated using a synthetically modelled seismic source, with promising results. In this work, we share the basic workflow and different attributes that have been tested with this algorithm for a synthetic data set to provide a framework for independent implementation, use and validation. We also compare the results obtained using the new neural network based autopicking routine with very robust contemporary autopicking algorithms in use within the industry.},
author = {Maity, Debotyam and Aminzadeh, Fred and Karrenbach, Martin},
nodoi = {10.1111/1365-2478.12125},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Prospecting/Maity, Aminzadeh, Karrenbach{\_}2014.pdf:pdf},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Automatic picking,Data processing,Neural network},
month = {jul},
number = {4},
pages = {834--847},
title = {{Novel hybrid artificial neural network based autopicking workflow for passive seismic data}},
nourl = {http://nodoi.wiley.com/10.1111/1365-2478.12125},
volume = {62},
year = {2014}
}
@article{Sen1996,
abstract = {The posterior probability density function (PPD), $\sigma$(m|dobs), of earth model m, where dobs are the measured data, describes the solution of a geophysical inverse problem, when a Bayesian inference model is used to describe the problem. In many applications, the PPD is neither analytically tractable nor easily approximated and simple analytic expressions for the mean and variance of the PPD are not available. Since the complete description of the PPD is impossible in the highly multi-dimensional model space of many geophysical applications, several measures such as the highest posterior density regions, marginal PPD and several orders of moments are often used to describe the solutions. Calculation of such quantities requires evaluation of multidimensional integrals. A faster alternative to enumeration and blind Monte-Carlo integration is importance sampling which may be useful in several applications. Thus how to draw samples of m from the PPD becomes an important aspect of geophysical inversion such that importance sampling can be used in the evaluation of these multi-dimensional integrals. Importance sampling can be carried out most efficiently by a Gibbs' sampler (GS). We also introduce a method which we called parallel Gibbs' sampler (PGS) based on genetic algorithms (GA) and show numerically that the results from the two samplers are nearly identical. We first investigate the performance of enumeration and several sampling based techniques such as a GS, PGS and several multiple maximum a posteriori (MAP) algorithms for a simple geophysical problem of inversion of resistivity sounding data. Several non-linear optimization methods based on simulated annealing (SA), GA and some of their variants can be devised which can be made to reach very close to the maximum of the PPD. Such MAP estimation algorithms also sample different points in the model space. By repeating these MAP inversions several times, it is possible to sample adequately the most significant portion(s) of the PPD and all these models can be used to construct the marginal PPD, mean) covariance, etc. We observe that the GS and PGS results are identical and indistinguishable from the enumeration scheme. Multiple MAP algorithms slightly underestimate the posterior variances although the correlation values obtained by all the methods agree very well. Multiple MAP estimation required 0.3{\%} of the computational effort of enumeration and 40{\%} of the effort of a GS or PGS for this problem. Next, we apply GS to the inversion of a marine seismic data set to quantify uncertainties in the derived model, given the prior distribution determined from several common midpoint gathers.},
author = {Sen, Mrinal K. and Stoffa, Paul L.},
nodoi = {10.1111/j.1365-2478.1996.tb00152.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Prospecting/Sen, Stoffa{\_}1996.pdf:pdf},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {uncertainty quantification},
mendeley-tags = {uncertainty quantification},
number = {2},
pages = {313--350},
title = {{Bayesian inference, Gibbs' sampler and uncertainty estimation in geophysical inversion}},
volume = {44},
year = {1996}
}
@article{Kikuchi1991,
abstract = {We have developed a method that inverts seismic body waves to determine the mechanism and rupture pattern of earthquakes. The rupture pattern is represented as a sequence of subevents distributed on the fault plane. This method is an extension of our earlier method in which the subevent mechanisms were fixed. In the new method, the subevent mechanisms are determined from the data and are allowed to vary during the sequence. When subevent mechanisms are allowed to vary, however, the inversion often becomes unstable because of the complex trade-offs between the mechanism, the timing, and the location of the subevents. Many different subevent sequences can explain the same data equally well, and it is important to determine the range of allowable solutions. Some constraints must be imposed on the solution to stabilize the inversion. We have developed a procedure to explore the range of allowable solutions and appropriate constraints. In this procedure, a network of grid points is constructed on the $\tau$ - I plane, where $\tau$ and I are, respectively, the onset time and the distance from the epicenter of a subevent; the best-fit subevent is determined at all grid points. Then the correlation is computed between the synthetic waveform for each subevent and the observed waveform. The correlation as a function of $\tau$ and I and the best-fit mechanisms computed at each $\tau$ - I grid point depict the character of allowable solutions and facilitate a decision on the appropriate constraints to be imposed on the solution. The method is illustrated using the data for the 1976 Guatemala earthquake.},
author = {Kikuchi, Masayuki and Kanamori, Hiroo},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Kikuchi, Kanamori{\_}1991.pdf:pdf},
issn = {00319201},
journal = {Bulletin of the Seismological Society of America},
keywords = {moment tensor},
mendeley-tags = {moment tensor},
month = {sep},
number = {6},
pages = {2335--2350},
title = {{Inversion of complex body waves-III}},
volume = {81},
year = {1991}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is $\Theta$(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
nodoi = {10.1145/1283383.1283494},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms/Arthur, Vassilvitskii{\_}2007.pdf:pdf},
isbn = {9780898716245},
issn = {978-0-898716-24-5},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
pages = {1027--1025},
pmid = {1000164511},
title = {k-means++: the advantages of careful seeding},
year = {2007}
}
@article{Murat1992,
abstract = {A back-propagation neural network is successfully applied to pick first arrivals (first breaks) in a background of noise. Network output is a decision whether each half-cycle on the trace is a first or not. 3D plots of the input attributes allow evaluation of the attributes for use in a neural network. Clustering and separation of first break from non-break data on the plots indicate that a neural network solution is possible, and therefore the attributes are suitable as network input. Application of the trained network to actual seismic data (Vibroseis and Poulter sources) demonstrates successful automated first-break selection for the following four attributes used as neural network input: (1) peak amplitude of a half-cycle; (2) amplitude difference between the peak value of the half-cycle and the previous (or following) half-cycle; (3) rms amplitude ratio for a data window (0.3 s) before and after the half-cycle; (4) rms amplitude ratio for a data window (0.06 s) on adjacent traces. The contribution of the attributes based on adjacent traces (4) was considered significant and future work will emphasize this aspect.},
author = {Murat, Michael E. and Rudman, Albert J.},
nodoi = {10.1111/j.1365-2478.1992.tb00543.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Prospecting/Murat, Rudman{\_}1992.pdf:pdf},
issn = {0016-8025},
journal = {Geophysical Prospecting},
month = {aug},
number = {6},
pages = {587--604},
title = {{Automated first arrival picking: a neural network approach}},
nourl = {http://nodoi.wiley.com/10.1111/j.1365-2478.1992.tb00543.x},
volume = {40},
year = {1992}
}
@inproceedings{Gong2011,
abstract = {Achieving good performance with a parallel genetic algorithm requires properly configuring control parameters such as mutation rate, crossover rate, and population size. We consider the problem of setting control parameter values in a standard, island-model distributed genetic algorithm. As an alternative to tuning parameters by hand or using a self-adaptive approach, we propose a very simple strategy which statically assigns random control parameter values to each processor. Experiments on benchmark problems show that this simple approach can yield results which are competitive with homogeneous distributed genetic algorithm using parameters tuned specifically for each of the benchmarks.},
author = {Gong, Yiyuan and Fukunaga, Alex},
booktitle = {2011 IEEE Congress of Evolutionary Computation (CEC)},
nodoi = {10.1109/CEC.2011.5949703},
file = {:D$\backslash$:/OneDrive/Articles/2011 IEEE Congress of Evolutionary Computation (CEC)/Gong, Fukunaga{\_}2011.pdf:pdf},
isbn = {978-1-4244-7834-7},
issn = {Pending},
keywords = {hpc},
mendeley-tags = {hpc},
month = {jun},
pages = {820--827},
publisher = {IEEE},
title = {{Distributed island-model genetic algorithms using heterogeneous parameter settings}},
nourl = {http://ieeexplore.ieee.org/document/5949703/},
year = {2011}
}
@article{Garcia-Nieto2011,
abstract = {Large scale continuous optimization problems are more relevant in current benchmarks since they are more representative of real-world problems (bioinformatics, data mining, etc.). Unfortunately, the performance of most of the available optimization algorithms deteriorates rapidly as the dimensionality of the search space increases. In particular, particle swarm optimization is a very simple and effective method for continuous optimization. Nevertheless, this algorithm usually suffers from unsuccessful performance on large dimension problems. In this work, we incorporate two new mechanisms into the particle swarm optimization with the aim of enhancing its scalability. First, a velocity modulation method is applied in the movement of particles in order to guide them within the region of interest. Second, a restarting mechanism avoids the early convergence and redirects the particles to promising areas in the search space. Experiments are carried out within the scope of this Special Issue to test scalability. The results obtained show that our proposal is scalable in all functions of the benchmark used, as well as numerically very competitive with regards to other compared optimizers.},
author = {Garc{\'{i}}a-Nieto, Jos{\'{e}} and Alba, Enrique},
nodoi = {10.1007/s00500-010-0648-1},
file = {:D$\backslash$:/OneDrive/Articles/Soft Computing/Garc{\'{i}}a-Nieto, Alba{\_}2011.pdf:pdf},
isbn = {1432-7643},
issn = {1432-7643},
journal = {Soft Computing},
keywords = {Continuous optimization,Large scale benchmarking,Particle swarm optimization,Scalability,pso},
mendeley-tags = {pso},
month = {nov},
number = {11},
pages = {2221--2232},
title = {{Restart particle swarm optimization with velocity modulation: a scalability test}},
nourl = {http://link.springer.com/10.1007/s00500-010-0648-1},
volume = {15},
year = {2011}
}
@article{Maraschini2010a,
abstract = {Higher-mode contribution is important in surface-wave inversion because it allows more information to be exploited, increases investigation depth, and improves model resolution. A new misfit function for multimodal inversion of surface waves, based on the Haskell-Thomson matrix method, allows higher modes to be taken into account without the need to associate experimental data points to a specific mode, thus avoiding mode-misidentification errors in the retrieved velocity profiles. Computing cost is reduced by avoiding the need for calculating synthetic apparent or modal dispersion curves. Based on several synthetic and real examples with inversion results from the classical and the proposed methods, we find that correct velocity models can be retrieved through the multimodal inversion when higher modes are superimposed in the apparent dispersion-curve or when it is not trivial to determine a priori to which mode each data point of the experimental dispersion curve belongs. The main drawback of the method is related to the presence of several local minima in the misfit function. This feature makes the choice of a consistent initial model very important.},
author = {Maraschini, Margherita and Ernst, Fabian and Foti, Sebastiano and Socco, Laura Valentina},
nodoi = {10.1190/1.3436539},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Maraschini et al.{\_}2010.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {field transformation,matrices,modes,rayleigh-waves,velocity},
month = {jul},
number = {4},
pages = {G31--G43},
title = {{A new misfit function for multimodal inversion of surface waves}},
nourl = {http://library.seg.org/nodoi/10.1190/1.3436539},
volume = {75},
year = {2010}
}
@article{Trojanowski2017,
abstract = {Microseismic monitoring in the oil and gas industry commonly uses migration-based methods to locate very weak microseismic events. The objective of this study is to compare the most popular migration-based methods on a synthetic dataset that simulates a strike-slip source mechanism event with a low signal-to-noise ratio recorded by surface receivers (vertical components). The results show the significance of accounting for the known source mechanism in the event detection and location procedures. For detection and location without such a correction, the ability to detect weak events is reduced. We show both numerically and theoretically that neglecting the source mechanism by using only absolute values of the amplitudes reduces noise suppression during stacking and, consequently, limits the possibility to retrieve weak microseismic events. On the other hand, even a simple correction to the data polarization used with otherwise ineffective methods can significantly improve detections and locations. A simple stacking of the data with a polarization correction provided clear event detection and location, but even better results were obtained for those data combined with methods that are based on semblance and cross-correlation.},
author = {Trojanowski, Jacek and Eisner, Leo},
nodoi = {10.1111/1365-2478.12366},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Prospecting/Trojanowski, Eisner{\_}2017.pdf:pdf},
isbn = {1365-2478},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Microseismic monitoring,Signal-to-noise ratio,Stacking,location,migration},
mendeley-tags = {location,migration},
month = {jan},
number = {1},
pages = {47--63},
title = {{Comparison of migration-based location and detection methods for microseismic events}},
nourl = {http://nodoi.wiley.com/10.1111/1365-2478.12366},
volume = {65},
year = {2017}
}
@article{Chawla2011,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
nodoi = {10.1613/jair.953},
eprint = {1106.1813},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Artificial Intelligence Research/Chawla et al.{\_}2011.pdf:pdf},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {jun},
pages = {321--357},
pmid = {18190633},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
nourl = {http://arxiv.org/abs/1106.1813 http://dx.nodoi.org/10.1613/jair.953},
volume = {16},
year = {2011}
}
@article{Curtis2004,
abstract = {Most general experimental design algorithms are either: (i) stochastic and hence give different designs each time they are run with finite computing power, or (ii) deterministic but converge to results that depend on an initial or reference design, taking little or no account of the range of all other possible designs. In this paper we introduce an approximation to standard measures of experimental design quality that enables a new algorithm to be used. The algorithm is simple, deterministic and the resulting experimental design is influenced by the full range of possible designs, thus addressing problems (i) and (ii) above. Although the designs produced are not guaranteed to be globally optimal, they significantly increase the magnitude of small eigenvalues in the model- data relationship (without requiring that these eigenvalues be calculated). This reduces the model uncertainties expected post- experiment. We illustrate the method on simple tomographic and microseismic location examples with varying degrees of seismic attenuation.},
author = {Curtis, Andrew and Michelini, Alberto and Leslie, David and Lomax, Anthony},
nodoi = {10.1111/j.1365-246X.2004.02114.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Curtis et al.{\_}2004.pdf:pdf},
isbn = {0956540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Inversion,Microseismicity,Tomography,location},
mendeley-tags = {location},
number = {2},
pages = {595--606},
title = {{A deterministic algorithm for experimental design applied to tomographic and microseismic monitoring surveys}},
volume = {157},
year = {2004}
}
@inproceedings{Lee2015,
abstract = {A novel particle swarm optimization (PSO) method for discrete parameters and its hybridized algorithm with multi-point geostatistics are presented. This stochastic algorithm is designed for complex geological models, which often require discrete facies modeling before simulating continuous reservoir properties. In this paper, we first develop a new PSO method for discrete parameters (Pro-DPSO) where particles move in the probability mass function (pmf) space instead of the parameter space. Then Pro-DPSO is hybridized with the single normal equation simulation algorithm (SNESIM), one of the popular multi-point geostatistics algorithms, to ensure the prior geological features. This hybridized algorithm (Pro-DPSO-SNESIM) is evaluated on a synthetic example of seismic inversion, and compared with a Markov chain Monte Carlo (McMC) method. The results show that the new algorithm generates multiple optimized models with the convergence rate much faster than the McMC method.},
author = {Lee, Jaehoon and Mukerji, Tapan},
booktitle = {SEG Technical Program Expanded Abstracts 2015},
nodoi = {10.1190/segam2015-5929157.1},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2015/Lee, Mukerji{\_}2015.pdf:pdf},
keywords = {algorithm,inversion,pso,reservoir chara,statistical},
mendeley-tags = {pso},
month = {aug},
pages = {2754--2759},
publisher = {Society of Exploration Geophysicists},
title = {{Probabilistic particle swarm optimization for discrete parameters (Pro-DPSO) hybridized with multi-point geostatistics: a new stochastic algorithm for complex geological models}},
nourl = {http://library.seg.org/nodoi/10.1190/segam2015-5929157.1{\%}5Cnhttp://library.seg.org/nodoi/pdf/10.1190/segam2015-5929157.1 http://library.seg.org/nodoi/10.1190/segam2015-5929157.1},
year = {2015}
}
@article{Sigaud2013,
abstract = {La r{\'{e}}solution de probl{\`{e}}mes {\`{a}} {\'{e}}tats et actions continus par l'optimisation de politiques param{\'{e}}triques est un sujet d'int{\'{e}}r{\^{e}}t r{\'{e}}cent en apprentissage par renforcement. L'algorithme PI2 est un exemple de cette approche, qui b{\'{e}}n{\'{e}}ficie de fondements math{\'{e}}matiques solides tir{\'{e}}s de la commande stochastique optimale et des outils de la th{\'{e}}orie de l'estimation statistique. Dans cet article, nous consid{\'{e}}rons PI2 en tant que membre de la famille plus vaste des m{\'{e}}thodes qui partagent le concept de moyenne pond{\'{e}}r{\'{e}}e par les probabilit{\'{e}}s pour mettre {\`{a}} jour it{\'{e}}rativement des param{\`{e}}tres afin d'optimiser une fonction de co{\^{u}}t. Nous comparons PI2 {\`{a}} d'autres membres de la m{\^{e}}me famille - la " m{\'{e}}thode d'entropie crois{\'{e}}e " et CMA-ES 1 - au niveau conceptuel et en termes de performance. La comparaison d{\'{e}}bouche sur la d{\'{e}}rivation d'un nouvel algorithme que nous appelons PI2-CMA pour " Path Integral Policy Improvement with Covariance Matrix Adaptation ". Le principal avantage de PI2-CMA est qu'il d{\'{e}}termine l'amplitude du bruit d'exploration automatiquement.},
author = {Sigaud, Olivier and Stulp, Freek},
nodoi = {10.3166/ria.27.243-263},
file = {:D$\backslash$:/OneDrive/Articles/Revue d'intelligence artificielle/Sigaud, Stulp{\_}2013.pdf:pdf},
issn = {0992499X},
journal = {Revue d'intelligence artificielle},
keywords = {Covariance matrix adaptation,Cross-entropy,Policy improvement,cmaes},
mendeley-tags = {cmaes},
month = {apr},
number = {2},
pages = {243--263},
title = {{Adaptation de la matrice de covariance pour l'apprentissage par renforcement direct}},
nourl = {http://ria.revuesonline.com/article.jsp?articleId=18378},
volume = {27},
year = {2013}
}
@inproceedings{Veezhinathan1990,
abstract = {First-break picking is an extremely time-consuming task for manual$\backslash$noperation since seismic data are so voluminous (e.g. a typical 3-D survey consists of more than half a million traces to be picked). Previous attempts to automate first-arrival picking have achieved only limited success. The authors describe a neural network (NN) solution to this problem using a back-propagation network. The NN-based application system achieved above 95{\%} accuracy on picking several seismic lines based on a single training using only a few seismic records. The level of performance exceeded that achieved by an existing automatic picking program. Job turnaround time (compared to manual picking) improved by 88{\%}. The approach appears robust and shows promise for automating other event-picking tasks in seismic velocity analysis and seismic tomography.},
author = {Veezhinathan, J. and Wagner, Don},
booktitle = {1990 IJCNN International Joint Conference on Neural Networks},
nodoi = {10.1109/IJCNN.1990.137575},
file = {:D$\backslash$:/OneDrive/Articles/1990 IJCNN International Joint Conference on Neural Networks/Veezhinathan, Wagner{\_}1990.pdf:pdf},
keywords = {neural network,picking},
mendeley-tags = {neural network,picking},
pages = {235--240 vol.1},
publisher = {IEEE},
title = {{A neural network approach to first break picking}},
nourl = {http://ieeexplore.ieee.org/document/5726535/},
year = {1990}
}
@article{Maraschini2010,
abstract = {The analysis of surface wave propagation is often used to estimate the S-wave velocity profile at a site. In this paper, we propose a stochastic approach for the inversion of surface waves, which allows apparent dispersion curves to be inverted. The inversion method is based on the integrated use of two-misfit functions. A misfit function based on the determinant of the Haskell-Thomson matrix and a classical Euclidean distance between the dispersion curves. The former allows all the modes of the dispersion curve to be taken into account with a very limited computational cost because it avoids the explicit calculation of the dispersion curve for each tentative model. It is used in a Monte Carlo inversion with a large population of profiles. In a subsequent step, the selection of representative models is obtained by applying a Fisher test based on the Euclidean distance between the experimental and the synthetic dispersion curves to the best models of the Monte Carlo inversion. This procedure allows the set of the selected models to be identified on the basis of the data quality. It also mitigates the influence of local minima that can affect the Monte Carlo results. The effectiveness of the procedure is shown for synthetic and real experimental data sets, where the advantages of the two-stage procedure are highlighted. In particular, the determinant misfit allows the computation of large populations in stochastic algorithms with a limited computational cost. {\textcopyright} 2010 The Authors Journal compilation {\textcopyright} 2010 RAS.},
author = {Maraschini, Margherita and Foti, Sebastiano},
nodoi = {10.1111/j.1365-246X.2010.04703.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Maraschini, Foti{\_}2010.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Elasticity and anelasticity,Inverse theory,Probability distributions,Site effects,Surface waves and free oscillations,Wave propagation},
number = {3},
pages = {1557--1566},
title = {{A Monte Carlo multimodal inversion of surface waves}},
volume = {182},
year = {2010}
}
@article{Gesret2015,
abstract = {Earthquake hypocentre locations are crucial in many domains of application (academic and industrial) as seismic event location maps are commonly used to delineate faults or fractures. The interpretation of these maps depends on location accuracy and on the reliability of the associated uncertainties. The largest contribution to location and uncertainty errors is due to the fact that the velocity model errors are usually not correctly taken into account. We propose a new Bayesian formulation that integrates properly the knowledge on the velocity model into the formulation of the probabilistic earthquake location. In this work, the velocity model uncertainties are first estimated with a Bayesian tomography of active shot data. We implement a sampling Monte Carlo type algorithm to generate velocity models distributed according to the posterior distribution. In a second step, we propagate the velocity model uncertainties to the seismic event location in a probabilistic framework. This enables to obtain more reliable hypocentre locations as well as their associated uncertainties accounting for picking and velocity model uncertainties. We illustrate the tomography results and the gain in accuracy of earthquake location for two synthetic examples and one real data case study in the context of induced microseismicity. 1 I N T RO D U C T I O N Earthquake hypocentre locations are central in seismogenic and tectonic interpretation. Indeed visual inspection of earthquake lo-cation maps is commonly used to delineate faults or fractures and mislocation can lead to an incorrect model of the Earth structure. For example, diffuse patterns that are sometimes observed can be due to unmapped or hidden faults but can also arise due to lo-cation inaccuracy. A precise location is particularly important in the microseismic context where the seismicity induced by mine or reservoir exploitation is often the only available tool to follow the fracturation. In addition, the spatial distribution of the uncertainties associated to the hypocentre location should also be estimated to interpret the results quantitatively. It is thus of primary importance to have accurate seismic event locations and their true associated uncertainties to obtain reliable interpretations. The location of earthquake hypocentre is a typical inverse prob-lem that can be solved with iterative linearized inversion approaches such as implemented in widely used softwares (e.g. Lee {\&} Lahr 1975; Lahr 1989, 2002). These algorithms aim to retrieve the max-imum likelihood solution that corresponds to the minimum of the misfit function between observed and computed P and S traveltimes. The major drawback of such local optimization methods is to give only a unique solution that can be inaccurate. Indeed as this prob-lem is non-linear, its solution can be multimodal and the algorithm can be stuck in a local minimum that does not correspond to the true hypocentre location. The shape of the misfit surface will be especially irregular for a poor network coverage and for a hetero-geneous medium (with velocity gradients and/or interfaces). Very often the uncertainties are estimated at the final hypocentre location under the Gaussian assumption, for example by scaling the partial derivatives of traveltime with respect to the hypocentre coordinates by the time residuals (Flinn 1965) or by prior estimates for picking and traveltime errors (Evernden 1969; Jordan {\&} Sverdrup 1981). These approaches lead to confidence regions of elliptical shape that can be erroneous due to the strong assumption that the misfit func-tion has a single optimum and a Gaussian shape. Since the work of Tarantola {\&} Valette (1982), the probabilistic Bayesian formulation is often preferred (e.g. Moser et al. 1992; Wittlinger et al. 1993; Lo-max et al. 2001; Husen et al. 2003) as it allows to retrieve the global minimum of the misfit function even for a non-linear/multimodal solution. With this Bayesian approach, the final solution is not a single point anymore but the complete posterior probability density function (pdf) of the event location. This formulation allows for},
author = {Gesret, A. and Desassis, N. and Noble, M. and Romary, T. and Maisons, C.},
nodoi = {10.1093/gji/ggu374},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Gesret et al.{\_}2015.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Inverse theory,Probability distribution,Seismic tomography,Theoretical seismology,location,mcmc,microseismic,sa},
mendeley-tags = {location,mcmc,microseismic,sa},
number = {1},
pages = {52--66},
title = {{Propagation of the velocity model uncertainties to the seismic event location}},
volume = {200},
year = {2015}
}
@article{Mohamed2012,
abstract = {This paper presents the application of a population Markov Chain Monte Carlo (MCMC) technique to generate history-matched models. The technique has been developed and successfully adopted in challenging domains such as computational biology but has not yet seen application in reservoir modelling. In population MCMC, multiple Markov chains are run on a set of response surfaces that form a bridge from the prior to posterior. These response surfaces are formed from the product of the prior with the likelihood raised to a varying power less than one. The chains exchange positions, with the probability of a swap being governed by a standard Metropolis accept/reject step, which allows for large steps to be taken with high probability. We show results of Population MCMC on the IC Fault Model-a simple three-parameter model that is known to have a highly irregular misfit surface and hence be difficult to match. Our results show that population MCMC is able to generate samples from the complex, multi-modal posterior probability distribution of the IC Fault model very effectively. By comparison, previous results from stochastic sampling algorithms often focus on only part of the region of high posterior probability depending on algorithm settings and starting points. {\textcopyright} 2011 Springer Science+Business Media B.V.},
author = {Mohamed, Linah and Calderhead, Ben and Filippone, Maurizio and Christie, Mike and Girolami, Mark},
nodoi = {10.1007/s10596-011-9232-8},
file = {:D$\backslash$:/OneDrive/Articles/Computational Geosciences/Mohamed et al.{\_}2012.pdf:pdf},
isbn = {1059601192},
issn = {1420-0597},
journal = {Computational Geosciences},
keywords = {History matching,MCMC methods,Population MCMC,Uncertainty quantification,mcmc,uncertainty quantification},
mendeley-tags = {mcmc,uncertainty quantification},
month = {mar},
number = {2},
pages = {423--436},
title = {{Population MCMC methods for history matching and uncertainty quantification}},
nourl = {http://link.springer.com/10.1007/s10596-011-9232-8},
volume = {16},
year = {2012}
}
@article{Poliannikov2013,
abstract = {We study the problem of determining an unknown microseismic event location relative to previously located events using a single monitoring array in a monitoring well. We show that using the available information about the previously located events for locating new events is advantageous compared to locating each event independently. By analysing confidence regions, we compare the performance of two previously proposed location methods, double-difference and interferometry, for varying signal-to-noise ratio and uncertainty in the velocity model. We show that one method may have an advantage over another depending on the experiment geometry, assumptions about uncertainty in velocity and recorded signal, etc. We propose a unified approach to relative event location that includes double-difference and interferometry as special cases, and is applicable to velocity models and well geometries of arbitrary complexity, producing location estimators that are superior to those of double-difference and interferometry.},
author = {Poliannikov, Oleg V. and Prange, Michael and Malcolm, Alison and Djikpesse, Hugues},
nodoi = {10.1093/gji/ggt119},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Poliannikov et al.{\_}2013.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Instability analysis,Interferometry,Numerical solutions,Theoretical seismology,bayesian},
mendeley-tags = {bayesian},
number = {1},
pages = {557--571},
title = {{A unified Bayesian framework for relative microseismic location}},
volume = {194},
year = {2013}
}
@article{Tibshirani2001,
abstract = {We propose a method (the ‘gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
nodoi = {10.1111/1467-9868.00293},
file = {:D$\backslash$:/OneDrive/Articles/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Tibshirani, Walther, Hastie{\_}2001.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
month = {may},
number = {2},
pages = {411--423},
pmid = {306526},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
nourl = {http://onlinelibrary.wiley.com/nodoi/10.1111/1467-9868.00293/abstract http://nodoi.wiley.com/10.1111/1467-9868.00293},
volume = {63},
year = {2001}
}
@article{Taillandier2009,
abstract = {Classical algorithms used for traveltime tomography are not necessarily well suited for handling very large seismic data sets or for taking advantage of current supercomputers. The classical approach of first-arrival traveltime tomography was revisited with the proposal of a simple gradient-based approach that avoids ray tracing and estimation of the Fr{\'{e}}chet derivative matrix. The key point becomes the derivation of the gradient of the misfit function obtained by the adjoint-state technique. The adjoint-state method is very attractive from a numerical point of view because the associated cost is equivalent to the solution of the forward-modeling problem, whatever the size of the input data and the number of unknown velocity parameters. An application on a 2D synthetic data set demonstrated the ability of the algorithm to image near-surface velocities with strong vertical and lateral variations and revealed the potential of the method.},
author = {Taillandier, C{\'{e}}dric and Noble, Mark and Chauris, Herv{\'{e}} and Calandra, Henri},
nodoi = {10.1190/1.3250266},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Taillandier et al.{\_}2009.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
title = {{First-arrival traveltime tomography based on the adjoint-state method}},
year = {2009}
}
@inproceedings{Piccand2008,
author = {Piccand, Sebastien and O'Neill, Michael and Walker, Jacqueline},
booktitle = {2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
nodoi = {10.1109/CEC.2008.4631134},
file = {:D$\backslash$:/OneDrive/Articles/2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)/Piccand, O'Neill, Walker{\_}2008.pdf:pdf},
isbn = {978-1-4244-1822-0},
keywords = {pso},
mendeley-tags = {pso},
month = {jun},
pages = {2505--2512},
publisher = {IEEE},
title = {{On the scalability of particle swarm optimisation}},
nourl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4631134 http://ieeexplore.ieee.org/document/4631134/},
year = {2008}
}
@inproceedings{Hansen1996,
abstract = {A new formulation for coordinate system independent adaptation of arbitrary normal mutation distributions with zero mean is presented. This enables the evolution strategy (ES) to adapt the correct scaling of a given problem and also ensures invariance with respect to any rotation of the fitness function (or the coordinate system). Especially rotation invariance, here resulting directly from the coordinate system independent adaptation of the mutation distribution, is an essential feature of the ES with regard to its general applicability to complex fitness functions. Compared to previous work on this subject, the introduced formulation facilitates an interpretation of the resulting mutation distribution, making sensible manipulation by the user possible (if desired). Furthermore it enables a more effective control of the overall mutation variance (expected step length).},
author = {Hansen, N. and Ostermeier, A.},
booktitle = {Proceedings of IEEE International Conference on Evolutionary Computation},
nodoi = {10.1109/ICEC.1996.542381},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of IEEE International Conference on Evolutionary Computation/Hansen, Ostermeier{\_}1996.pdf:pdf},
isbn = {0-7803-2902-3},
keywords = {adaptation,cmaes,covariance matrix,derandomized,evolution strategy,evolutionary algorithms,mutation distribution,self-adaptation,strategy parameters},
mendeley-tags = {cmaes},
pages = {312--317},
publisher = {IEEE},
title = {{Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation}},
nourl = {http://ieeexplore.ieee.org/document/542381/},
year = {1996}
}
@article{Grayver2016,
abstract = {This paper presents a methodology to sample equivalence domain (ED) in nonlinear partial differential equation (PDE)-constrained inverse problems. For this purpose, we first applied state-of-the-art stochastic optimization algorithm called Covariance Matrix Adaptation Evolution Strategy (CMAES) to identify low-misfit regions of the model space. These regions were then randomly sampled to create an ensemble of equivalent models and quantify uncertainty. CMAES is aimed at exploring model space globally and is robust on very ill-conditioned problems. We show that the number of iterations required to converge grows at a moderate rate with respect to number of unknowns and the algorithm is embarrassingly parallel. We formulated the problem by using the generalized Gaussian distribution. This enabled us to seamlessly use arbitrary norms for residual and regularization terms. We show that various regularization norms facilitate studying different classes of equivalent solutions. We further show how performance of the standard Metropolis-Hastings Markov chain Monte Carlo algorithm can be substantially improved by using information CMAES provides. This methodology was tested by using individual and joint inversions of magneotelluric, controlled-source electromagnetic (EM) and global EM induction data.},
author = {Grayver, Alexander V. and Kuvshinov, Alexey V.},
nodoi = {10.1093/gji/ggw063},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Grayver, Kuvshinov{\_}2016.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Geomagnetic induction,Inverse theory,Magnetotellurics,Marine electromagnetics,Numerical solutions,cmaes},
mendeley-tags = {cmaes},
number = {2},
pages = {971--987},
title = {{Exploring equivalence domain in nonlinear inverse problems using Covariance Matrix Adaption Evolution Strategy (CMAES) and random sampling}},
volume = {205},
year = {2016}
}
@article{Shao1993,
abstract = {We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion (AIC), the Cp, and the bootstrap, is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n {\^{a}}†' {\^{a}}ˆ{\v{z}}. We show that the inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n{\^{I}}½-out cross-validation with n{\^{I}}½, the number of observations reserved for validation, satisfying n{\^{I}}½/n {\^{a}}†' 1 as n {\^{a}}†' {\^{a}}ˆ{\v{z}}. This is a somewhat shocking discovery, because n{\^{I}}½/n {\^{a}}†' 1 is totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some practical aspects of the use of the leave-n{\^{I}}½-out cross-validation method are provided, and results from a simulation study are presented.},
author = {Shao, Jun},
nodoi = {10.2307/2290328},
file = {:D$\backslash$:/OneDrive/Articles/Journal of the American Statistical Association/Shao{\_}1993.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Balanced incomplete,Consistency,Data splitting,Model assessment,Monte Carlo,Prediction,machine learning},
mendeley-tags = {machine learning},
month = {jun},
number = {422},
pages = {486},
title = {{Linear Model Selection by Cross-Validation}},
nourl = {http://www.jstor.org/stable/2290328 http://about.jstor.org/terms http://www.jstor.org/stable/2290328?origin=crossref},
volume = {88},
year = {1993}
}
@article{Ramirez2011,
abstract = {In this research, we consider the supervised learning problem of seismic phase classification. In seismology, knowledge of the seismic activity arrival time and phase leads to epicenter localization and surface velocity estimates useful in developing seismic early warning systems and detecting man-made seismic events. Formally, the activity arrival time refers to the moment at which a seismic wave is first detected and the seismic phase classifies the physics of the wave. We propose a new perspective for the classification of seismic phases in three-channel seismic data collected within a network of regional recording stations. Our method extends current techniques and incorporates concepts from machine learning. Machine learning techniques attempt to leverage the concept of "learning'' the patterns associated with different types of data characteristics. In this case, the data characteristics are the seismic phases. This concept makes sense because the characteristics of the phase types are dictated by the physics of wave propagation. Thus by "learning'' a signature for each type of phase, we can apply classification algorithms to identify the phase of incoming data from a database of known phases observed over the recording network. Our method first uses a multi-scale feature extraction technique for clustering seismic data on low-dimensional manifolds. We then apply kernel ridge regression on each feature manifold for phase classification. In addition, we have designed an information theoretic measure used to merge regression scores across the multi-scale feature manifolds. Our approach complements current methods in seismic phase classification and brings to light machine learning techniques not yet fully examined in the context of seismology. We have applied our technique to a seismic data set from the Idaho, Montana, Wyoming, and Utah regions collected during 2005 and 2006. This data set contained compression wave and surface wave seismic phases. Through - ross-validation, our method achieves a 74.6{\%} average correct classification rate when compared to analyst classifications.},
author = {Ramirez, Juan and Meyer, Francois G.},
nodoi = {10.1109/ICMLA.2011.91},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings - 10th International Conference on Machine Learning and Applications, ICMLA 2011/Ramirez, Meyer{\_}2011.pdf:pdf},
isbn = {9780769546070},
journal = {Proceedings - 10th International Conference on Machine Learning and Applications, ICMLA 2011},
keywords = {Manifold Signal Processing,Seismology,Supervised Learning},
pages = {382--388},
title = {{Machine learning for seismic signal processing: Seismic phase classification on a manifold}},
volume = {1},
year = {2011}
}
@article{Poliannikov2014,
abstract = {The locations of seismic events are used to infer reservoir properties and to guide future production activity, as well as to determine and understand the stress field. Thus, locating seismic events with uncertainty quantification remains an important problem. Using Bayesian analysis, a joint probability density function of all event locations was constructed from prior information about picking errors in kinematic data and explicitly quantified velocity model uncertainty. Simultaneous location of all seismic events captured the absolute event locations and the relative locations of some events with respect to others, along with their associated uncertainties. We found that the influence of an uncertain velocity model on location uncertainty under many realistic scenarios can be significantly reduced by jointly locating events. Many quantities of interest that are estimated from multiple event locations, such as fault sizes and fracture spacing or orientation, can be better estimated in practice using the proposed approach.},
author = {Poliannikov, Oleg V and Prange, Michael and Malcolm, Alison E and Djikpesse, Hugues},
nodoi = {10.1190/geo2013-0390.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Poliannikov et al.{\_}2014.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {earthquake,fractures,location,microseismic,traveltime,uncertainty quantification},
mendeley-tags = {location,uncertainty quantification},
month = {nov},
number = {6},
pages = {KS51--KS60},
title = {{Joint location of microseismic events in the presence of velocity uncertainty}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/geo2013-0390.1 http://library.seg.org/nodoi/10.1190/geo2013-0390.1},
volume = {79},
year = {2014}
}
@article{Tarantola1982,
abstract = {We examine the general non-linear inverse problem with a finite number of parameters. In order to permit the incorporation of any a priori information about parameters and any distribution of data (not only of gaussian type) we propose to formulate the problem not using single quantities (such as bounds, means, etc.) but using probability density functions for data and parameters. We also want our formulation to allow for the incorporation of theoretical errors, i.e. non-exact theoretical relationships between data and parameters (due to discretization, or incomplete theoretical knowledge); to do that in a natural way we propose to define general theoretical relationships also as probability density functions. We show then that the inverse problem may be formulated as a problem of combination of information: the experimental information about data, the a priori information about parameters, and the theoretical information. With this approach, the general solution of the non-linear inverse problem is unique and consistent (solving the same problem, with the same data, but with a different system of parameters does not change the solution).},
author = {Tarantola, Albert and Valette, Bernard},
nodoi = {10.1038/nrn1011},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysics/Tarantola, Valette{\_}1982.pdf:pdf},
isbn = {0340-062X},
issn = {0340062X},
journal = {Journal of Geophysics},
keywords = {Information,Inverse problems,Pattern recognition,Probability,inversion,location},
mendeley-tags = {inversion,location},
number = {3},
pages = {159--170},
pmid = {12511864},
title = {{Inverse Problems = Quest for Information}},
nourl = {http://www.ipgp.fr/{~}tarantola/Files/Professional/Papers{\_}PDF/IP{\_}QI{\_}latex.pdf},
volume = {50},
year = {1982}
}
@article{PianaAgostinetti2015,
abstract = {Local earthquake tomography is a non-linear and non-unique inverse problem that uses event arrival times to solve for the spatial distribution of elastic properties. The typical approach is to apply iterative linearization and derive a preferred solution, but such solutions are biased by a number of subjective choices: the starting model that is iteratively adjusted, the degree of regularization used to obtain a smooth solution, and the assumed noise level in the arrival time data. These subjective choices also affect the estimation of the uncertainties in the inverted parameters. The method presented here is developed in a Bayesian framework where a priori information and measurements are combined to define a posterior probability density of the parameters of interest: elastic properties in a subsurface 3-D model, hypocentre coordinates and noise level in the data. We apply a trans-dimensional Markov chain Monte Carlo algorithm that asymptotically samples the posterior distribution of the investigated parameters. This approach allows us to overcome the issues raised above. First, starting a number of sampling chains from random samples of the prior probability distribution lessens the dependence of the solution from the starting point. Secondly, the number of elastic parameters in the 3-D subsurface model is one of the unknowns in the inversion, and the parsimony of Bayesian inference ensures that the degree of detail in the solution is controlled by the information in the data, given realistic assumptions for the error statistics. Finally, the noise level in the data, which controls the uncertainties of the solution, is also one of the inverted parameters, providing a first-order estimate of the data errors. We apply our method to both synthetic and field arrival time data. The synthetic data inversion successfully recovers velocity anomalies, hypocentre coordinates and the level of noise in the data. The Bayesian inversion of field measurements gives results comparable to those obtained independently by linearized inversion, reconstructing the geometry of the main seismic velocity anomalies. The quantification of the posterior uncertainties, a crucial output of Bayesian inversion, allows for visualizing regions where elastic properties are closely constrained by the data and is used here to directly compare our results to the ones obtained with the linearized inversion. In the case we examined the results of two inversion techniques are not significantly different.},
author = {{Piana Agostinetti}, N. and Giacomuzzi, Genny and Malinverno, Alberto},
nodoi = {10.1093/gji/ggv084},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Piana Agostinetti, Giacomuzzi, Malinverno{\_}2015.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Crustal structure,Seismic tomography,location,mcmc},
mendeley-tags = {location,mcmc},
month = {apr},
number = {3},
pages = {1598--1617},
title = {{Local three-dimensional earthquake tomography by trans-dimensional Monte Carlo sampling}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggv084},
volume = {201},
year = {2015}
}
@article{Wang1996,
abstract = {An artificial neural network-based pattern classification system is applied to seismic event detection. We have designed two types of Artificial Neural Detector (AND) for real-time earthquake detection. Type A artificial neural detector (AND-A) uses the recursive STA/LTA time series as input data, and type B (AND-B) uses moving window spectrograms as input data to detect earth-quake signals. The two AND's are trained under supervised learning by using a set of seismic recordings, and then the trained AND's are applied to another set of recordings for testing. Results show that the accuracy of the artificial neural network-based seismic detectors is better than that of the conventional algorithms solely based on the STA/LTA threshold. This is especially true for signals with either low signal-to-noise ratio or spikelike noises.},
author = {Wang, Jin and Teng, Ta-Liang},
nodoi = {10.1016/0148-9062(96)86904-X},
file = {:D$\backslash$:/OneDrive/Articles/International Journal of Rock Mechanics and Mining Sciences {\&} Geomechanics Abstracts/Wang, Teng{\_}1996.pdf:pdf},
isbn = {0037-1106},
issn = {01489062},
journal = {International Journal of Rock Mechanics and Mining Sciences {\&} Geomechanics Abstracts},
keywords = {neural network,picking},
mendeley-tags = {neural network,picking},
month = {apr},
number = {3},
pages = {A107},
title = {{Artificial neural network-based seismic detector}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/014890629686904X},
volume = {33},
year = {1996}
}
@article{Font2004,
abstract = {The maximum intersection (MAXI) method, which derives from the master station method (MSM), determines within a 3-D velocity model the absolute hypocentral location based on observed arrival times. First, the spatial node that better satisfies the arrival time differences computed at all station pairs, plus or minus an error tolerance value (in seconds), is defined as the preliminary hypocentral solution (PRED). Second, because PRED depends neither on the estimate of origin time nor on the residual root mean square (rms), residual outliers are objectively detected and cleaned out from the original data set without any iterative process or weighting. Third, a statistical minimization (residual rms) is conducted in a small domain around the PRED node, which results in a unique FINAL solution. The MAXI method is applied to the determination of earthquake hypocentres (with the proper station correction terms) in the southernmost extremity of the Ryukyu subduction zone, where several dense seismic clusters occur near the seismogenic plate interface. The location of earthquakes, recorded at both the Taiwanese and Japanese networks, is obtained for about a thousand events (between 1992 and 1997). The process uses a detailed 3-D velocity model based on multiple geophysical data sources obtained in the junction area between subduction and collision (east of Taiwan). The earthquake clustering and the significant drop in residual statistics (1.20, 0.80 and 0.35 s, for Taiwanese catalogue, MSM and MAXIM solutions respectively) indicate the accuracy of the method, which can be used to routinely determine absolute hypocentre location based on observed arrival times.},
author = {Font, Yvonne and Kao, Honn and Lallemand, Serge and Liu, Char Shine and Chiao, Ling Yun},
nodoi = {10.1111/j.1365-246X.2004.02317.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Font et al.{\_}2004.pdf:pdf},
isbn = {0956540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {3-D,Earthquake,Hypocentre,Location,Outliner,Taiwan,location},
mendeley-tags = {location},
number = {2},
pages = {655--675},
title = {{Hypocentre determination offshore of eastern Taiwan using the maximum intersection method}},
volume = {158},
year = {2004}
}
@article{Wolfe2002,
abstract = {This article examines the properties of difference operators that are used to relocate earthquakes and remove path anomaly biases. There are presently three established algorithms based on such techniques: (1) the method of Jordan and Sverdrup (1981), (2) the double-difference method of Got et al. (1994), and (3) the modified double-difference method of Waldhauser and Ellsworth (2001). We show that the underlying mathematics of these three methods are similar, although there are distinct contrasts in how each is adapted. Our results provide insight into the performance of individual methods. Both the Jordan and Sverdrup (1981) and double difference methods (Got et al., 1994; Waldhauser and Ellsworth, 2001) remove the average path anomaly bias in a set of events, but the equation weighting is more ideal in the first method. Distance dependent weighting in the Waldhauser and Ellsworth (2001) method does not reduce earthquake location-dependent path anomaly bias unless damping is applied, but damping causes the locations between earthquakes spaced far apart to be less well resolved. Alternatively, the results using Jordan and Sverdrup (1981) and Got et al. (1994) only remove a constant bias across a model subregion and cannot resolve the relative locations between subregions. The results of this study indicate that differencing operators contain the fundamental limitation that when the path anomalies from velocity heterogeneity change stongly with earthquake position, the bias effects can be reduced in the relative locations between closely spaced earthquakes, but the effects cannot be reduced in the relative locations between earthquakes spaced far apart.},
author = {Wolfe, Cecily J.},
nodoi = {10.1785/0120010189},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Wolfe{\_}2002.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
keywords = {location},
mendeley-tags = {location},
number = {8},
pages = {2879--2892},
title = {{On the mathematics of using difference operators to relocate earthquakes}},
volume = {92},
year = {2002}
}
@article{Tape2013,
abstract = {A seismic moment tensor is a description of an earthquake source, but the description is indirect. The moment tensor describes seismic radiation rather than the actual physical process that initiates the radiation. A moment tensor 'model' then ties the physical process to the moment tensor. The model is not unique, and the physical process is therefore not unique. In the classical moment tensor model, an earthquake arises from slip along a planar fault, but with the slip not necessarily in the plane of the fault. The model specifies the resulting moment tensor in terms of the slip vector, the fault normal vector and the Lam{\'{e}} elastic parameters, assuming isotropy. We review the classical model in the context of the fundamental lune. The lune is closely related to the space of moment tensors, and it provides a setting that is conceptually natural as well as pictorial. In addition to the classical model, we consider a crack plus double-couple model (CDC model) in which a moment tensor is regarded as the sum of a crack tensor and a double couple.},
author = {Tape, W. and Tape, Carl},
nodoi = {10.1093/gji/ggt302},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Tape, Tape{\_}2013.pdf:pdf},
isbn = {1365-246X},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Earthquake source observations,Theoretical seismology,moment tensor},
mendeley-tags = {moment tensor},
month = {dec},
number = {3},
pages = {1701--1720},
title = {{The classical model for moment tensors}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggt302},
volume = {195},
year = {2013}
}
@article{Mittal2017,
abstract = {Evolutionary Algorithms (EAs) are being routinely applied for a variety of optimization tasks, and real-parameter optimization in the presence of constraints is one such important area. During constrained optimization EAs often create solutions that fall outside the feasible region; hence a viable constraint- handling strategy is needed. This paper focuses on the class of constraint-handling strategies that repair infeasible solutions by bringing them back into the search space and explicitly preserve feasibility of the solutions. Several existing constraint-handling strategies are studied, and two new single parameter constraint-handling methodologies based on parent-centric and inverse parabolic probability (IP) distribution are proposed. The existing and newly proposed constraint-handling methods are first studied with PSO, DE, GAs, and simulation results on four scalable test-problems under different location settings of the optimum are presented. The newly proposed constraint-handling methods exhibit robustness in terms of performance and also succeed on search spaces comprising up-to 500 variables while locating the optimum within an error of 10{\$}{\^{}}{\{}-10{\}}{\$}. The working principle of the IP based methods is also demonstrated on (i) some generic constrained optimization problems, and (ii) a classic `Weld' problem from structural design and mechanics. The successful performance of the proposed methods clearly exhibits their efficacy as a generic constrained-handling strategy for a wide range of applications.},
archivePrefix = {arXiv},
arxivId = {1504.04421},
author = {Padhye, Nikhil and Mittal, Pulkit and Deb, Kalyanmoy},
eprint = {1504.04421},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Padhye, Mittal, Deb{\_}2015.pdf:pdf},
keywords = {pso},
mendeley-tags = {pso},
month = {apr},
title = {{Feasibility Preserving Constraint-Handling Strategies for Real Parameter Evolutionary Optimization}},
nourl = {http://arxiv.org/abs/1504.04421},
year = {2015}
}
@inproceedings{Rastogi2009,
abstract = {Seismic traveltime tomography provides method for direct estimation of velocity distribution in the subsurface from the P or S-wave first arrival traveltime data. Some of the seismic tomographic algorithms like - SIRT, ART and LSQR are widely used for their low compute intensiveness. But they may not yield the better image of the complex subsurface and especially with poor initial model. Also, these methods belong to the category of local optimization techniques, which are likely to get stuck in local minima and are derivative based. Genetic Algorithms (GA), in turn, one of the most popular global optimization method which works even with poor or no initial guess and without derivatives too. It can work well in various operator and coding modifications, e.g., binary and real number coding etc, with any global optimization problem. Any inversion algorithm consists of two important modules; one is the optimization technique and other is the forward modeler scheme. We have to start with certain model, which is nothing but the set of parameter values to be inverted, do a forward modeling, compare the results with the experimental data, update the guess using the optimization method and repeat the process until the match is satisfactory. In current investigation, we are dealing with a problem of inverting the first arrival traveltime data to reconstruct the subsurface velocity model which is nothing but a tomographic inversion problem. Our aim of the present study, is to develop and demonstrate an efficient and robust tool for traveltime tomographic inversion. To achieve this, we have developed an inversion algorithm taking real-coded genetic algorithm (RCGA) as optimization technique and used its multiple realization (run), in order to make sure that we are really near to the global minima. Here, the forward modeler is an efficient finite difference based scheme for 2D first arrival traveltime calculation of a given velocity model. The developed algorithm is highly compute intensive, hence parallelized using Hybrid Island model. The results and the performance of the algorithm are presented and discussed here for a complex synthetic velocity model.},
author = {Rastogi, Richa and Srivastava, Abhishek and Majumder, Satyajit and Gholap, Sachin},
booktitle = {SEG Technical Program Expanded Abstracts 2009},
nodoi = {10.1190/1.3255362},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2009/Rastogi et al.{\_}2009.pdf:pdf},
isbn = {10523812 (ISSN)},
issn = {10523812},
keywords = {genetic algorithm,inversion},
mendeley-tags = {genetic algorithm,inversion},
month = {jan},
pages = {2491--2495},
publisher = {Society of Exploration Geophysicists},
title = {{Multiple realisation of real‐coded genetic algorithm: A tool for 2D traveltime tomographic inversion}},
nourl = {http://www.scopus.com/inward/record.nourl?eid=2-s2.0-84857240174{\&}partnerID=40{\&}md5=b0207329e9c7173d6f5a865b42ac96af http://library.seg.org/nodoi/abs/10.1190/1.3255362},
volume = {28},
year = {2009}
}
@article{Hirata1987,
abstract = {A new algorithm is applied to inverting arrival time data for hypocenter location. The algorithm incorporates both observed and prior data from a Bayesian point of view. We define marginal probability density function (pdf) to eliminate the origin time from the location problem; the posterior pdf of hypocenter parameters is integrated over the whole range of the origin time. The best estimate of the hypocenter is defined as a set of spatial coordinates which maximizes the marginal pdf. Assuming Gaussian errors in both observed and prior data, we obtain a simple algorithm. Estimation errors of parameters are evaluated by an asymptotic covariance matrix, with which an asymptotic posterior pdf is computed. The algorithm is applied to observed data and is tested. An example of analysis is given for aftershocks of the 1969 Gifuken-chubu earthquake (M = 6.6) reported by the Japan Meteorological Agency (JMA). The spatial distribution of the aftershocks is supposed to be Gaussian with standard deviation of 15 km. A center of the aftershock distribution, which gives the prior estimates of hypocenters, is also estimated from observed data. Results of the nonlinear inversion of arrival time data are examined in terms of the asymptotic posterior pdf. We found that relocated hypocenters of the aftershocks are concentrated in a narrow region of 2-3 km in width, while the hypocenters previously reported by JMA have a wide distribution of 5-7 km. ?? 1987.},
author = {Hirata, Naoshi and Matsu'ura, Mitsuhiro},
nodoi = {10.1016/0031-9201(87)90066-5},
file = {:D$\backslash$:/OneDrive/Articles/Physics of the Earth and Planetary Interiors/Hirata, Matsu'ura{\_}1987.pdf:pdf},
isbn = {0031-9201},
issn = {00319201},
journal = {Physics of the Earth and Planetary Interiors},
keywords = {location},
mendeley-tags = {location},
number = {C},
pages = {50--61},
title = {{Maximum-likelihood estimation of hypocenter with origin time eliminated using nonlinear inversion technique}},
volume = {47},
year = {1987}
}
@inproceedings{Price1996,
abstract = {Differential evolution (DE) is a powerful yet simple evolutionary algorithm for optimizing real-valued multi-modal functions. Function parameters are encoded as floating-point variables and mutated with a simple arithmetic operation. During mutation, a variable-length, one-way crossover operation splices perturbed best-so-far parameter values into existing population vectors. A novel sampling technique adaptively scales the step-size of perturbations as the population evolves. DE's selection criterion demands that improved vectors always be accepted. The performance of DE on a testbed of 15 functions is compared with a variety of recently published results encompassing many different methods. DE converged for all 15 functions and was the fastest method for solving 11 of them. DE's performance on the remaining 4 functions was competitive ER -},
author = {Price, K.V.},
booktitle = {Proceedings of North American Fuzzy Information Processing},
nodoi = {10.1109/NAFIPS.1996.534790},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of North American Fuzzy Information Processing/Price{\_}1996.pdf:pdf},
isbn = {0-7803-3225-3},
keywords = {de},
mendeley-tags = {de},
pages = {524--527},
publisher = {IEEE},
title = {{Differential evolution: a fast and simple numerical optimizer}},
nourl = {papers2://publication/uuid/CC402C6E-3DE4-444F-B7E5-48EAC30623DB http://ieeexplore.ieee.org/document/534790/},
year = {1996}
}
@inproceedings{Rada-Vilela2011,
abstract = {This work provides a further study on the difference between synchronous and asynchronous updates in Particle Swarm Optimization with different neighborhood sizes ranging from local best to global best. Ten well-known functions are used as benchmarks on both variants. Statistical tests performed on the results provide strong evidence to claim that syn-chronous updates yield in general better results with similar or even faster speed of convergence than its asynchronous counterpart, contrary to observations and conclusions of pre-vious studies based solely on descriptive statistics.},
address = {New York, New York, USA},
author = {Rada-Vilela, Juan and Zhang, Mengjie and Seah, Winston},
booktitle = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
nodoi = {10.1145/2001576.2001581},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11/Rada-Vilela, Zhang, Seah{\_}2011.pdf:pdf},
isbn = {9781450305570},
keywords = {Particle swarm optimization,particle swarm optimization,pso,speed of convergence,synchronous and async,synchronous and asynchronous,updates},
mendeley-tags = {pso},
pages = {21},
publisher = {ACM Press},
title = {{A performance study on synchronous and asynchronous updates in particle swarm optimization}},
nourl = {http://portal.acm.org/citation.cfm?doid=2001576.2001581},
year = {2011}
}
@article{Angeline1998,
abstract = {This paper investigates the philosophical and performance differences of particle swarm and evolutionary optimization. The method of processing employed in each technique are first reviewed followed by a summary of their philosophical differences. Comparison experiments involving four non-linear functions well studied in the evolutionary optimization literature are used to highlight some performance differences between the techniques.},
author = {Angeline, Peter J},
nodoi = {10.1007/BFb0040811},
file = {:D$\backslash$:/OneDrive/Articles/Lecture Notes in Computer Science Evolutionary Programming VII/Angeline{\_}1998.pdf:pdf},
isbn = {3540648917},
issn = {16113349},
journal = {Lecture Notes in Computer Science: Evolutionary Programming VII},
keywords = {pso},
mendeley-tags = {pso},
pages = {601--610},
title = {{Evolutionary optimization versus particle swarm optimization: Philosophy and performance differences}},
volume = {1447},
year = {1998}
}
@article{Schaff2005,
abstract = {We processed the complete digital seismogram database for northern California to measure accurate differential travel times for correlated earthquakes observed at common stations. Correlated earthquakes are earthquakes that occur within a few kilometers of one another and have similar focal mechanisms, thus generating similar waveforms, allowing measurements to be made via cross-corre- lation analysis. The waveform database was obtained from the Northern California Earthquake Data Center and includes about 15 million seismograms from 225,000 local earthquakes between 1984 and 2003. A total of 26 billion cross-correlation measurements were performed on a 32-node (64 processor) Linux cluster, using improved analysis tools. All event pairs with separation distances of 5 km or less were processed at all stations that recorded the pair. We computed a total of about 1.7 billion P-wave differential times from pairs of waveforms that had cross- correlation coefficients (CC) of 0.6 or larger. The P-wave differential times are often on the order of a factor of ten to a hundred times more accurate than those obtained from routinely picked phase onsets. 1.2 billion S-wave differential times were mea- sured with CC?0.6, a phase not routinely picked at the Northern California Seismic Network because of the noise level of remaining P coda. We found that approxi- mately 95{\%} of the seismicity includes events that have cross-correlation coefficients of CC ? 0.7 with at least one other event recorded at four or more stations. At some stations more than 40{\%} of the recorded events are similar at the CC ? 0.9 level, indicating the potential existence of large numbers of repeating earthquakes. Large numbers of correlated events occur in different tectonic regions, including the San Andreas Fault, Long Valley caldera, Geysers geothermal field and Mendocino triple junction. Future research using these data may substantially improve earthquake lo- cations and add insight into the velocity structure in the crust.},
author = {Schaff, David P. and Waldhauser, Felix},
nodoi = {10.1785/0120040221},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Schaff, Waldhauser{\_}2005.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
number = {6},
pages = {2446--2461},
title = {{Waveform cross-correlation-based differential travel-time measurements at the northern California seismic network}},
volume = {95},
year = {2005}
}
@article{McCormack1993,
abstract = {Interactive seismic processing systems for editing noisy seismic traces and picking first-break refraction events have been developed using a neural network learning algorithm. We employ a backpropagation neural network (BNN) paradigm modified to improve the convergence rate of the BNN. The BNN is interactively “trained” to edit seismic data or pick first breaks by a human processor who judiciously selects and presents to the network examples of trace edits or refraction picks. The network then iteratively adjusts a set of internal weights until it can accurately duplicate the examples provided by the user. After the training session is completed, the BNN system can then process new data sets in a manner that mimics the human processor. Synthetic modeling studies indicate that the BNN uses many of the same subjective criteria that humans employ in editing and picking seismic data sets. Automated trace editing and first‐break picking based on the modified BNN paradigm achieve 90 to 98 percent agreement wit...},
author = {McCormack, Michael D. and Zaucha, David E and Dushek, Dennis W},
nodoi = {10.1190/1.1443352},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/McCormack, Zaucha, Dushek{\_}1993.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {machine learning,neural network,picking},
mendeley-tags = {machine learning,neural network,picking},
month = {jan},
number = {1},
pages = {67--78},
title = {{First-break refraction event picking and seismic data trace editing using neural networks}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/1.1443352 http://library.seg.org/nodoi/10.1190/1.1443352},
volume = {58},
year = {1993}
}
@article{Storn1997,
abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
author = {Storn, Rainer and Price, Kenneth},
nodoi = {10.1023/A:1008202821328},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Global Optimization/Storn, Price{\_}1997.pdf:pdf},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Stochastic optimization,de,evolution strategy,genetic algorithm,global optimization,nonlinear optimization},
mendeley-tags = {de},
number = {4},
pages = {341--359},
pmid = {2015},
title = {{Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces}},
nourl = {http://dx.nodoi.org/10.1023/A:1008202821328 http://link.springer.com/10.1023/A:1008202821328},
volume = {11},
year = {1997}
}
@article{Carlisle2001,
abstract = {What attributes and settings of the Particle Swarm Optimizer constants result in a good, off-the-shelf, PSO implementation? There are many parameters, both explicit and implicit, associated with the Particle Swarm Optimizer that may affect its performance. There are the social and cognitive learning rates and magnitudes, the population size, the neighborhood size (including global neighborhoods), synchronous or asynchronous updates, and various additional controls, such as inertia and constriction factors. For any given problem, the values and choices for some of these parameters may have significant impact on the efficiency and reliability of the PSO, and yet varying other parameters may have little or no effect. What set of values, then, constitutes a good, general purpose PSO? While some of these factors have been investigated in the literature, others have not. In this paper we use existing literature and a selection of benchmark problems to determine a set of starting values suitable for an off the shelf PSO.},
author = {Carlisle, Anthony and Dozier, Gerry},
file = {:D$\backslash$:/OneDrive/Articles/Population English Edition/Carlisle, Dozier{\_}2001.pdf:pdf},
journal = {Population English Edition},
keywords = {pso},
mendeley-tags = {pso},
pages = {1--6},
title = {{An Off-The-Shelf PSO}},
nourl = {http://antho.huntingdon.edu/publications/Off-The-Shelf{\_}PSO.pdf},
volume = {1},
year = {2001}
}
@article{Shi1999,
abstract = {We empirically study the performance of the particle swarm optimizer (PSO). Four different benchmark functions with asymmetric initial range settings are selected as testing functions. The experimental results illustrate the advantages and disadvantages of the PSO. Under all the testing cases, the PSO always converges very quickly towards the optimal positions but may slow its convergence speed when it is near a minimum. Nevertheless, the experimental results show that the PSO is a promising optimization method and a new approach is suggested to improve PSO's performance near the optima, such as using an adaptive inertia weight},
author = {Shi, Yuhui and Eberhart, R C},
nodoi = {10.1109/CEC.1999.785511},
file = {:D$\backslash$:/OneDrive/Articles/Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on/Shi, Eberhart{\_}1999.pdf:pdf},
isbn = {VO - 3},
issn = {1089-778X},
journal = {Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on},
keywords = {Benchmark testing,Convergence,Equations,Evolutionary computation,Genetic algorithms,Genetic mutations,Genetic programming,Optimization methods,PSO,Particle swarm optimization,Space technology,adaptive inertia weight,adaptive systems,asymmetric initial range settings,benchmark functions,convergence,convergence speed,evolutionary computation,optimal positions,optimization method,particle swarm optimization,particle swarm optimizer,pso,testing,testing functions},
mendeley-tags = {pso},
pages = {1--1950 Vol. 3},
pmid = {20371407},
title = {{Empirical study of particle swarm optimization}},
volume = {3},
year = {1999}
}
@article{Cipolla2011,
abstract = {Thousands of hydraulic fracture treatments have been monitored in the past ten years using microseismic mapping, providing a wealth of measurements that show a surprising degree of diversity in event patterns. Interpreting the microseismic data to determine the geometry and complexity of hydraulic fractures continues to be focused on visualization of the event patterns and qualitative estimates of the “stimulated volume”, which has led to wide variations and inconsistencies in interpretations. Comparing the energy input during a hydraulic fracture treatment and resultant energy released by microseismic events demonstrates that the seismic deformation is a very small portion of the total deformation. The vast majority of the energy is consumed in aseismic deformation (tensile opening) and fluid friction (Maxwell et al. 2008). Proper interpretation of microseismic measurements should account for both seismic and aseismic deformation, coupling the geomechanics of fracture opening and propagation with the shear failures that generate microseisms. Interpretation of microseismic measurements begins with an evaluation of location uncertainty, using signal-to-noise ratios and error ellipsoids, along with event moment magnitude. In some cases, microseismic event location uncertainty is erroneously interpreted as fracture complexity. The next step is to normalize the data and correct for observation well bias, both distance and azimuth, including use of seismic radiation patterns. Without these corrections fracture behavior from well to well or stage to stage (especially in horizontal wells) can easily be misinterpreted. Advanced geophysical processing that describes the failure mechanisms in more detail may also aid in the interpretation. The final step in the interpretation is to include the geomechanics of the overall process, accounting for the fracture treatment volumes injected, the net pressure in the hydraulic fracture(s) and the shear failures that generated the microseisms. This final, critical step is often overlooked when interpreting microseismic measurements. The paper provides a comprehensive, yet practical guide to the interpretation of microseismic measurements.},
author = {Cipolla, C and Maxwell, S and Mack, M and Downie, R},
nodoi = {10.2118/144067},
file = {:D$\backslash$:/OneDrive/Articles/SPE North American Unconventional Gas Conference and Exhibition/Cipolla et al.{\_}2011.pdf:pdf},
isbn = {9781618398109},
journal = {SPE North American Unconventional Gas Conference and Exhibition},
keywords = {SPE 144067,microseismic},
mendeley-tags = {microseismic},
number = {June},
pages = {1--28},
title = {{A Practical Guide to Interpreting Microseismic Measurements}},
year = {2011}
}
@article{Auger2006,
abstract = {We present a comprehensive processing tool for the real-time analysis of the source mechanism of very long period (VLP) seismic data based on waveform inversions performed in the frequency domain for a point source. A search for the source providing the best-fitting solution is conducted over a three-dimensional grid of assumed source locations, in which the Green's functions associated with each point source are calculated by finite differences using the reciprocal relation between source and receiver. Tests performed on 62 nodes of a Linux cluster indicate that the waveform inversion and search for the best-fitting signal over 100,000 point sources require roughly 30 s of processing time for a 2-min-long record. The procedure is applied to post-processing of a data archive and to continuous automatic inversion of real-time data at Stromboli, providing insights into different modes of degassing at this volcano.},
author = {Auger, Emmanuel and D'Auria, Luca and Martini, Marcello and Chouet, Bernard and Dawson, Phillip},
nodoi = {10.1029/2005GL024703},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Research Letters/Auger et al.{\_}2006.pdf:pdf},
isbn = {0094-8276},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {inversion,moment tensor},
mendeley-tags = {inversion,moment tensor},
number = {4},
pages = {1--5},
pmid = {117},
title = {{Real-time monitoring and massive inversion of source parameters of very long period seismic signals: An application to Stromboli Volcano, Italy}},
volume = {33},
year = {2006}
}
@article{Koh2006,
abstract = {The high computational cost of complex engineering optimization problems has motivated the development of parallel optimization algorithms. A recent example is the parallel particle swarm optimization (PSO) algorithm, which is valuable due to its global search capabilities. Unfortunately, because existing parallel implementations are synchronous (PSPSO), they do not make efficient use of computational resources when a load imbalance exists. In this study, we introduce a parallel asynchronous PSO (PAPSO) algorithm to enhance computational efficiency. The performance of the PAPSO algorithm was compared to that of a PSPSO algorithm in homogeneous and heterogeneous computing environments for small- to medium-scale analytical test problems and a medium-scale biomechanical test problem. For all problems, the robustness and convergence rate of PAPSO were comparable to those of PSPSO. However, the parallel performance of PAPSO was significantly better than that of PSPSO for heterogeneous computing environments or heterogeneous computational tasks. For example, PAPSO was 3.5 times faster than was PSPSO for the biomechanical test problem executed on a heterogeneous cluster with 20 processors. Overall, PAPSO exhibits excellent parallel performance when a large number of processors (more than about 15) is utilized and either (1) heterogeneity exists in the computational task or environment, or (2) the computation-to-communication time ratio is relatively small.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Koh, Byung-Il and George, Alan D. and Haftka, Raphael T. and Fregly, Benjamin J.},
nodoi = {10.1002/nme.1646},
eprint = {NIHMS150003},
file = {:D$\backslash$:/OneDrive/Articles/International Journal for Numerical Methods in Engineering/Koh et al.{\_}2006.pdf:pdf},
isbn = {1-4244-0719-2},
issn = {0029-5981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {game theory,learning (artificial intelligence),par,pso},
mendeley-tags = {pso},
month = {jul},
number = {4},
pages = {578--595},
pmid = {17224972},
publisher = {IEEE},
title = {{Parallel asynchronous particle swarm optimization}},
nourl = {http://ieeexplore.ieee.org/document/4298806/ http://nodoi.wiley.com/10.1002/nme.1646},
volume = {67},
year = {2006}
}
@article{Smyth1996,
author = {Smyth, Padhraic},
file = {:D$\backslash$:/OneDrive/Articles/KDD/Smyth{\_}1996.pdf:pdf},
isbn = {1-57735-004-9},
journal = {KDD},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
pages = {126--133},
title = {{Clustering Using Monte Carlo Cross-Validation.}},
nourl = {http://www.aaai.org/Papers/KDD/1996/KDD96-021.pdf},
year = {1996}
}
@article{Zhu1994,
abstract = {A master station (MS) method is presented in this paper to rapidly determine hypocenters in three-dimensional (3-D) heterogeneous velocities. An equal differential time (EDT) surface is defined as the collection of all spatial points that satisfy the time difference between two arrivals, which can be two picks at two stations or two different phase picks at one station. The EDT surface is independent of the origin time and will contain the hypocenter. For an event with J arrivals, there are (J-1) independent EDT surfaces. The MS method determines the hypocenter that satisfies two types of constraints: to be traversed by most EDT surfaces and to yield minimum travel time residual statistics. The statistics include both the residual variance and the amplitude of the origin time error. The combined use of the EDT surfaces and residual statistics allows for a unique determination of the hypocenter and origin time using different types of phase arrivals. In principle, only three arrivals are minimally required to constrain a unique solution if three different stations are used. For a 3-D velocity model, the EDT surfaces and the residual statistics can be computed efficiently using a reference file created by Moser's (1991) ray tracing method. An illustration of the MS method is given for 27 small events that occurred in southern California, using a P wave velocity model modified from that of Magistrale et al. (1992). The average misfit between the bulletin hypocenters and the new solutions is 3.8 km. If a 3-D velocity model is accurate, the MS method can be a viable means of hypocenter determination.},
author = {Zhu, Hua-Wei},
nodoi = {10.1029/94JB00934},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research/Zhu{\_}1994.pdf:pdf},
journal = {Journal of Geophysical Research},
keywords = {location},
mendeley-tags = {location},
pages = {439--455},
title = {{Rapid three-dimensional hypocentral determination using a master station method}},
volume = {99},
year = {1994}
}
@article{VanderBaan2000,
abstract = {Neural networks are increasingly popular in geophysics. Because they are universal approximators, these tools can approximate any continuous function with an arbitrary precision. Hence, they may yield important contributions to finding solutions to a variety of geo physical applications. However, knowledge of many methods and techniques recently developed to increase the performance and to facilitate the use of neural networks does not seem to be widespread in the geophysical community. Therefore, the power of these tools has not yet been explored to their full extent. In this paper, techniques are described for faster training, better overall performance, i.e., generalization, and the automatic estimation of network size and architecture.},
author = {van der Baan, Mirko and Jutten, Christian},
nodoi = {10.1190/1.1444797},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/van der Baan, Jutten{\_}2000.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {machine learning,neural network},
mendeley-tags = {machine learning,neural network},
month = {jul},
number = {4},
pages = {1032--1047},
title = {{Neural networks in geophysical applications}},
nourl = {http://library.seg.org/nodoi/10.1190/1.1444797},
volume = {65},
year = {2000}
}
@article{Clerc2002,
abstract = {The particle swarm is an algorithm for finding optimal regions of complex search spaces through the interaction of individuals in a population of particles. This paper analyzes a particle's trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). A five-dimensional depiction is developed, which describes the system completely. These analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system's convergence tendencies. Some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the ability of the particle swarm to find optima of some well-studied test functions.},
author = {Clerc, M. and Kennedy, J.},
nodoi = {10.1109/4235.985692},
file = {:D$\backslash$:/OneDrive/Articles/IEEE Transactions on Evolutionary Computation/Clerc, Kennedy{\_}2002.pdf:pdf},
isbn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {pso},
mendeley-tags = {pso},
number = {1},
pages = {58--73},
pmid = {21738602},
title = {{The particle swarm - explosion, stability, and convergence in a multidimensional complex space}},
volume = {6},
year = {2002}
}
@article{Jordan1981,
abstract = {Improved methods for single-and multiple-event hypocenter determinations are developed and applied to the problem of locating earthquake clusters in the South-Central Pacific Ocean. Bayesian statistical methods are used to incorporate a priori information about arrival-time variance into the derivation of hypocenter confidence ellipsoids, permitting a more realistic calculation of critical parameters in the case where the number of stations is small. The diagonal elements of certain projection operators, called “data importances” by Minster et al. (1974), are used to evaluate network balance. The hypocentroid of an event cluster is defined to be the average location of events within the cluster, and the deviations of individual hypocenters from the hypocentroid are called cluster vectors. The problem of estimating the cluster vectors can be decoupled from the problem of estimating the hypocentroid by a simple but fundamental mathematical result, here termed the hypocentroidal decomposition theorem. The algorithm based on this analysis appears to have many advantages over other published methods for multiple-event location, both in its efficient use of available information and its computational speed. The application of this method to three clusters of shallow intraplate seismicity in the South-Central Pacific, designated Regions A, B, and C, demonstrates that the seismicity within each cluster is very localized; the rms lengths of the cluster vectors for each group of epicenters are estimated to be only 9, 6, and 12 km, respectively. Estimates of the epicentroids are},
author = {Jordan, Thomas H and Sverdrup, Keith A},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Jordan, Sverdrup{\_}1981.pdf:pdf},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
keywords = {location},
mendeley-tags = {location},
number = {4},
pages = {1105--1130},
title = {{Teleseismic location techniques and their application to earthquake clusters in the South-Central Pacific}},
nourl = {http://www.bssaonline.org/content/71/4/1105.abstract},
volume = {71},
year = {1981}
}
@article{Frechet1985,
abstract = {La premi{\`{e}}re partie de la th{\`{e}}se analyse les relations entre s{\'{e}}ismes et contraintes. Apr{\`{e}}s un rappel sur la m{\'{e}}canique des milieux continus et son application aux contraintes dans le globe, on pr{\'{e}}sente le calcul des d{\'{e}}placements engendr{\'{e}}s par une faille rectangulaire de jeu et de pendage quelconques, dans un demi-espace {\'{e}}lastique homog{\`{e}}ne. La deuxi{\`{e}}me partie est consacr{\'{e}}e {\`{a}} l'{\'{e}}tude des doublets sismiques, c'est-{\`{a}}-dire des paires de s{\'{e}}ismes ayant des formes d'onde identiques. On distingue des doublets temporels, situ{\'{e}}s au m{\^{e}}me endroit mais {\`{a}} des dates diff{\'{e}}rentes, et des doublets spatiaux situ{\'{e}}s {\`{a}} quelque distance l'un de l'autre (au plus quelques centaines de m{\`{e}}tres), mais {\`{a}} des dates tr{\`{e}}s rapproch{\'{e}}es. L'{\'{e}}tude de ces donn{\'{e}}es est fond{\'{e}}e sur une analyse interspectrale par fen{\^{e}}tre mobile. Pour une fen{\^{e}}tre donn{\'{e}}e, on mesure le d{\'{e}}lai (d{\'{e}}calage en temps) entre les deux signaux en calculant la pente du d{\'{e}}phasage en fonction de la fr{\'{e}}quence. On calcule aussi la coh{\'{e}}rence et le rapport spectral pour chaque fr{\'{e}}quence. Le d{\'{e}}lai est mesur{\'{e}} avec une pr{\'{e}}cision atteignant 1 ms, et ceci en tout point du sismogramme. L'{\'{e}}tude porte sur 24 doublets et multiplets (67 s{\'{e}}ismes au total) situ{\'{e}}s en Californie centrale, {\`{a}} l'aide d'une soixantaine de stations du r{\'{e}}seau CALNET. La relocalisation relative des s{\'{e}}ismes d'un multiplet est faite avec une pr{\'{e}}cision atteignant 1 m{\`{e}}tre. On a pu ainsi d{\'{e}}montrer la simplicit{\'{e}} et l'unicit{\'{e}} locale des plans de faille en profondeur. Le d{\'{e}}lai mesur{\'{e}} dans la cauda permet de pr{\'{e}}ciser l'origine de celle-ci, profonde pour les stations {\`{a}} l'{\'{e}}cart des failles, superficielle pour les stations {\`{a}} l'aplomb des grandes failles. L'{\'{e}}tude des doublets temporels permet de mesurer les variations de vitesse des ondes dans la croate avec une grande pr{\'{e}}cision. Une variation r{\'{e}}gionale induit un " {\'{e}}tirement de sismogramme ". La mesure de ce dernier permet une r{\'{e}}solution de 0,01 {\%} de variation de vitesse des ondes S. Les variations ainsi mesur{\'{e}}es atteignent 0,4 {\%} dans la r{\'{e}}gion de Hollister et sont attribu{\'{e}}es au s{\'{e}}isme de Coyote Lake (M=5,6).},
author = {Fr{\'{e}}chet, J.},
file = {:D$\backslash$:/OneDrive/Articles/Th{\`{e}}se d'Etat, Universit{\'{e}} Scientifique et M{\'{e}}dicale de Grenoble/Fr{\'{e}}chet{\_}1985.pdf:pdf},
journal = {Th{\`{e}}se d'Etat, Universit{\'{e}} Scientifique et M{\'{e}}dicale de Grenoble},
keywords = {location},
mendeley-tags = {location},
pages = {206},
title = {{Sismogen{\`{e}}se et doublets sismiques}},
year = {1985}
}
@article{Kirkpatrick1983,
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kirkpatrick, S and Gelatt, C. D. and Vecchi, M. P.},
nodoi = {10.1126/science.220.4598.671},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/OneDrive/Articles/Science/Kirkpatrick, Gelatt, Vecchi{\_}1983.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
keywords = {sa},
mendeley-tags = {sa},
month = {may},
number = {4598},
pages = {671--680},
pmid = {17813860},
title = {{Optimization by Simulated Annealing}},
nourl = {http://www.sciencemag.org/cgi/nodoi/10.1126/science.220.4598.671},
volume = {220},
year = {1983}
}
@article{Corciulo2012,
abstract = {In the study, we investigate the use of ambient-noise data to locate microseismic sources at the exploration scale. We develop a multiscale matched-field processing (MFP) approach to localize seismic sources at frequencies below 10 Hz. An application to an actual data set acquired over a hydrocarbon field is presented to determine the reliability of the MFP procedure. The data used were continuously recorded over five days, at a total of 397 stations on a 1-km-per-side square seismic network. The MFP results show: (1) a dominant and stable surface source associated with human activities (road and exploration platforms) around the reservoir; and (2) weaker sources at depth below the seismic network that are related to the injection/extraction process.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Corciulo, Margherita and Roux, Philippe and Campillo, Michel and Dubucq, Dominique and Kuperman, W A},
nodoi = {10.1190/geo2011-0438.1},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Corciulo et al.{\_}2012.pdf:pdf},
isbn = {9788578110796},
issn = {0016-8033},
journal = {Geophysics},
month = {sep},
number = {5},
pages = {KS33--KS41},
pmid = {25246403},
title = {{Multiscale matched-field processing for noise-source localization in exploration geophysics}},
nourl = {http://library.seg.org/nodoi/10.1190/geo2011-0438.1},
volume = {77},
year = {2012}
}
@article{Davies1979,
abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
author = {Davies, D L and Bouldin, D W},
nodoi = {10.1109/TPAMI.1979.4766909},
file = {:D$\backslash$:/OneDrive/Articles/IEEE transactions on pattern analysis and machine intelligence/Davies, Bouldin{\_}1979.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
number = {2},
pages = {224--227},
pmid = {21868852},
title = {{A cluster separation measure}},
volume = {1},
year = {1979}
}
@article{Duane1987,
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
author = {Duane, Simon and Kennedy, A.D. and Pendleton, Brian J. and Roweth, Duncan},
nodoi = {10.1016/0370-2693(87)91197-X},
file = {:D$\backslash$:/OneDrive/Articles/Physics Letters B/Duane et al.{\_}1987.pdf:pdf},
isbn = {0370-2693},
issn = {03702693},
journal = {Physics Letters B},
month = {sep},
number = {2},
pages = {216--222},
title = {{Hybrid Monte Carlo}},
nourl = {http://www.sciencedirect.com/science/article/pii/037026938791197X http://linkinghub.elsevier.com/retrieve/pii/037026938791197X},
volume = {195},
year = {1987}
}
@inproceedings{Kennedy1995,
abstract = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described},
author = {Kennedy, J and Eberhart, R},
booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
nodoi = {10.1109/ICNN.1995.488968},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of ICNN'95 - International Conference on Neural Networks/Kennedy, Eberhart{\_}1995.pdf:pdf},
isbn = {0-7803-2768-3},
issn = {19353812},
keywords = {Artificial neural networks,Birds,Educational institutions,Genetic algorithms,Humans,Marine animals,Optimization methods,Particle swarm optimization,Performance evaluation,Testing,artificial intelligence,artificial life,evolution,genetic algorithms,multidimensional search,neural nets,neural network,nonlinear functions,optimization,particle swarm,pso,search problems,simulation,social metaphor},
mendeley-tags = {pso},
pages = {1942--1948},
pmid = {20371407},
publisher = {IEEE},
title = {{Particle swarm optimization}},
nourl = {http://ieeexplore.ieee.org/document/488968/},
volume = {4},
year = {1995}
}
@article{MacKay1992,
abstract = {Three Bayesian ideas are presented for supervised adaptive classiers. First, it is argued that the output of a classier should be obtained by marginalising over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves {\`{a}}moderation' of the most probable classi-er's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in (MacKay, 1992a, 1992b) can also be applied to classication problems. This framework success-fully chooses the magnitude of weight decay terms, and ranks solutions found using dierent numbers of hidden units. Third, an information{\{}based data selection criterion is derived and demonstrated within this framework. 1 Introduction A quantitative Bayesian framework has been described for learning of mappings in feedfor-ward networks (MacKay, 1992a, 1992b). It was demonstrated that thievidence' framework could successfully choose the magnitude and type of weight decay terms, and could choose between solutions using dierent numbers of hidden units. The framework also gives quan-tied error bars expressing the uncertainty in the network's outputs and its parameters. In (MacKay, 1992c) information{\{}based objective functions for active learning were discussed within the same framework. These three papers concentrated on interpolation (regression) problems. Neural net-works can also be trained to perform classication tasks.}}},
author = {MacKay, David J. C.},
nodoi = {10.1162/neco.1992.4.5.720},
file = {:D$\backslash$:/OneDrive/Articles/Neural Computation/MacKay{\_}1992.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {bayesian,neural network},
mendeley-tags = {bayesian,neural network},
month = {sep},
number = {5},
pages = {720--736},
title = {{The Evidence Framework Applied to Classification Networks}},
nourl = {http://www.mitpressjournals.org/nodoi/10.1162/neco.1992.4.5.720},
volume = {4},
year = {1992}
}
@article{Rousseeuw1987,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects he well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate' number of clusters.},
author = {Rousseeuw, Peter J},
nodoi = {10.1016/0377-0427(87)90125-7},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Computational and Applied Mathematics/Rousseeuw{\_}1987.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Graphical display,classification,cluster analysis,clustering validity},
mendeley-tags = {cluster analysis},
month = {nov},
pages = {53--65},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/0377042787901257},
volume = {20},
year = {1987}
}
@article{Sambridge1999a,
abstract = {This paper presents a new derivative-free search method for finding models of acceptable data fit in a multidimensional parameter space. It falls into the same class of method as simulated annealing and genetic algorithms, which are commonly used for global optimization problems. The objective here is to find an ensemble of models that preferentially sample the good data-fitting regions of parameter space, rather than seeking a single optimal model. (A related paper deals with the quantitative appraisal of the ensemble.) The new search algorithm makes use of the geometrical constructs known as Voronoi cells to derive the search in parameter space. These are nearest neighbour regions defined under a suitable distance norm. The algorithm is conceptually simple, requires just two ‘tuning parameters', and makes use of only the rank of a data fit criterion rather than the numerical value. In this way all difficulties associated with the scaling of a data misfit function are avoided, and any combination of data fit criteria can be used. It is also shown how Voronoi cells can be used to enhance any existing direct search algorithm, by intermittently replacing the forward modelling calculations with nearest neighbour calculations. The new direct search algorithm is illustrated with an application to a synthetic problem involving the inversion of receiver functions for crustal seismic structure. This is known to be a non-linear problem, where linearized inversion techniques suffer from a strong dependence on the starting solution. It is shown that the new algorithm produces a sophisticated type of ‘self-adaptive' search behaviour, which to our knowledge has not been demonstrated in any previous technique of this kind.},
author = {Sambridge, Malcolm},
nodoi = {10.1046/j.1365-246X.1999.00876.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Sambridge{\_}1999.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {inversion,numerical techniques,receiver functions,waveform inversion},
mendeley-tags = {inversion},
month = {aug},
number = {2},
pages = {479--494},
title = {{Geophysical inversion with a neighbourhood algorithm-I. Searching a parameter space}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1046/j.1365-246x.1999.00900.x https://academic.oup.com/gji/article-lookup/nodoi/10.1046/j.1365-246X.1999.00876.x},
volume = {138},
year = {1999}
}
@article{Green1995,
abstract = {Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the param-eter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with appli-cations to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.},
author = {Green, Peter J.},
nodoi = {10.2307/2337340},
file = {:D$\backslash$:/OneDrive/Articles/Biometrika/Green{\_}1995.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Change-point analysis,Image segmentation,Jump diffusion,Markov chain Monte Carlo,Multiple binomial experiments,Multiple shrinkage,Some key words,Step function,Voronoi tessellation,rjmcmc},
mendeley-tags = {rjmcmc},
month = {dec},
number = {4},
pages = {711},
title = {{Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination}},
nourl = {http://biomet.oxfordjournals.org/content/82/4/711.short http://www.jstor.org/stable/2337340?origin=crossref},
volume = {82},
year = {1995}
}
@article{Bodin2012a,
abstract = {Interpolation of spatial data is a widely used technique across the Earth sciences. For example, the thickness of the crust can be estimated by different active and passive seismic source surveys, and seismologists reconstruct the topography of the Moho by interpolating these different estimates. Although much research has been done on improving the quantity and quality of observations, the interpolation algorithms utilized often remain standard linear regression schemes, with three main weaknesses: (1) the level of structure in the surface, or smoothness, has to be predefined by the user; (2) different classes of measurements with varying and often poorly constrained uncertainties are used together, and hence it is difficult to give appropriate weight to different data types with standard algorithms; (3) there is typically no simple way to propagate uncertainties in the data to uncertainty in the estimated surface. Hence the situation can be expressed by Mackenzie (2004): “We use fantastic telescopes, the best physical models, and the best computers. The weak link in this chain is interpreting our data using 100 year old mathematics”. Here we use recent developments made in Bayesian statistics and apply them to the problem of surface reconstruction. We show how the reversible jump Markov chain Monte Carlo (rj-McMC) algorithm can be used to let the degree of structure in the surface be directly determined by the data. The solution is described in probabilistic terms, allowing uncertainties to be fully accounted for. The method is illustrated with an application to Moho depth reconstruction in Australia.},
author = {Bodin, T. and Salmon, M. and Kennett, B. L N and Sambridge, M.},
nodoi = {10.1029/2012JB009547},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Bodin et al.{\_}2012(2).pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {Australia,Bayesian inference,Moho discontinuity,Monte Carlo inversion,spatial data analysis,surface reconstruction},
number = {10},
pages = {1--13},
title = {{Probabilistic surface reconstruction from multiple data sets: An example for the Australian Moho}},
volume = {117},
year = {2012}
}
@article{Husen2010,
abstract = {Earthquake location catalogs are not an exact representation of the true earthquake locations. They contain random error, for example from errors in the arrival time picks, as well as systematic biases. The most important source of systematic errors in earthquake locations is the inherent dependence of earthquake locations on the assumed seismic velocity structure of the Earth. Random errors may be accounted for in formal uncertainty estimates, but systematic biases are not, and they must be considered based on knowledge about how the earthquakes were located. In this article we discuss earthquake location methods and methods for estimating formal uncertainties; we consider systematic biases in earthquake location catalogs; and we give readers guidance on how to identify good-quality earthquake locations.},
author = {Husen, S and Hardebeck, J},
nodoi = {10.5078/corssa-55815573},
file = {:D$\backslash$:/OneDrive/Articles/Community Online Resource for Statistical Seismicity Analysis/Husen, Hardebeck{\_}2010.pdf:pdf},
journal = {Community Online Resource for Statistical Seismicity Analysis},
keywords = {Earthquake location accuracy,location},
mendeley-tags = {location},
number = {September},
pages = {1--35},
title = {{Understanding Seismicity Catalogs and their Problems}},
year = {2010}
}
@article{Li2013,
abstract = {An improved particle swarm optimization (PSO) algorithm with a neighborhood-redispatch (NR) technique is presented to design an ultrawideband (UWB) antenna in this letter. Based on the conventional version of PSO algorithm, some new parameters are introduced in the NR-PSO algorithm to improve optimization performance. In order to illustrate the effectiveness of the algorithm in representative topology problems, several benchmark functions are used to test the performance of the improved PSO algorithm. The results show that it is better than the conventional PSO algorithm for multimodal and unimodal functions.Combined with theHFSS solver, one UWBantenna with a respective notched band is designed by using the proposed NR-PSO algorithm. It not only covers theUWBband (3.1–10.6GHz) and Bluetooth passband (2.40–2.484 GHz), but also realizes a stopband (5.15–5.825 GHz) to avoid potential interferences.},
author = {Li, Yan-liang and Shao, Wei and You, Long and Wang, Bing-zhong},
nodoi = {10.1109/LAWP.2013.2283375},
file = {:D$\backslash$:/OneDrive/Articles/IEEE Antennas and Wireless Propagation Letters/Li et al.{\_}2013.pdf:pdf},
issn = {1536-1225},
journal = {IEEE Antennas and Wireless Propagation Letters},
keywords = {Neighborhood redispatch,particle swarm optimization,pso,slot antenna,ultrawideband.},
mendeley-tags = {pso},
number = {3},
pages = {1236--1239},
title = {{An Improved PSO Algorithm and Its Application to UWB Antenna Design}},
nourl = {http://ieeexplore.ieee.org/document/6607154/},
volume = {12},
year = {2013}
}
@article{Sculley2010,
abstract = {We present two modifications to the popular k-means clus- tering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast ϵ- accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml},
author = {Sculley, D},
nodoi = {10.1145/1772690.1772862},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 19th international conference on World wide web WWW 10/Sculley{\_}2010.pdf:pdf},
isbn = {9781605587998},
issn = {1605587990},
journal = {Proceedings of the 19th international conference on World wide web WWW 10},
pages = {1177},
title = {{Web-scale k-means clustering}},
nourl = {http://portal.acm.org/citation.cfm?doid=1772690.1772862},
year = {2010}
}
@inproceedings{Mohamed2010a,
abstract = {Reservoir modelling is frequently used in the oil industry to measure the risk associated with alternative production scenarios. However, reservoir models themselves contain a high level of uncertainty because of the typically very limited, sparse and multi-scaled reservoir knowledge. The effect of this uncertainty can be assessed by producing a set of diverse models that match the production data reasonably well and using these models to quantify uncertainty in predicting the future performance of the reservoir. Evolutionary and swarm intelligence algorithms have become very popular for history matching due to their simplicity and parallel implementation capacity. This paper focuses on the application of Particle Swarm Optimisation (PSO) for history matching the Brugge field (a recent SPE benchmark case study). The parameterisation of the model is based on principal component analysis (PCA) for modelling spatially correlated random fields (e.g. porosity, net-to-gross and permeability) applied to the set of initial realisations which describe the range of prior beliefs. The PSO is then used to find the set of possible combinations of parameters, represented by the PCA eigenvalues, which match the historical data and honour the static data from the wells present in the initial realisations. We show that PSO is able to find multiple good and diverse history matched models for the Brugge reservoir without exhaustive sampling of the parameter space. Uncertainty of production forecasts are quantified by P10-P50-P90 uncertainty envelope obtained from the ensemble of PSO models. The history matching results are compared with the ones obtained with Ensemble Kalman Filter data assimilation method. These results show the ability of PSO to handle large history matching problems and obtain results comparable to the EnKF for this case study.},
author = {Mohamed, Lina and Christie, Mike A. and Demyanov, Vasily and Robert, Emmanuel and Kachuma, Dick},
booktitle = {SPE Annual Technical Conference and Exhibition},
nodoi = {10.2118/135264-MS},
file = {:D$\backslash$:/OneDrive/Articles/SPE Annual Technical Conference and Exhibition/Mohamed et al.{\_}2010.pdf:pdf},
isbn = {9781555633004},
keywords = {pso},
mendeley-tags = {pso},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Application of Particle Swarms for History Matching in the Brugge Reservoir}},
nourl = {http://www.onepetro.org/nodoi/10.2118/135264-MS},
year = {2010}
}
@article{Sambridge1999b,
abstract = {Monte Carlo direct search methods, such as genetic algorithms, simulated annealing, etc., are often used to explore a finite-dimensional parameter space. They require the solving of the forward problem many times, that is, making predictions of observables from an earth model. The resulting ensemble of earth models represents all ‘information' collected in the search process. Search techniques have been the subject of much study in geophysics; less attention is given to the appraisal of the ensemble. Often inferences are based on only a small subset of the ensemble, and sometimes a single member. This paper presents a new approach to the appraisal problem. To our knowledge this is the first time the general case has been addressed, that is, how to infer information from a complete ensemble, previously generated by any search method. The essence of the new approach is to use the information in the available ensemble to guide a resampling of the parameter space. This requires no further solving of the forward problem, but from the new ‘resampled' ensemble we are able to obtain measures of resolution and trade-off in the model parameters, or any combinations of them. The new ensemble inference algorithm is illustrated on a highly non-linear wave-form inversion problem. It is shown how the computation time and memory requirements scale with the dimension of the parameter space and size of the ensemble. The method is highly parallel, and may easily be distributed across several computers. Since little is assumed about the initial ensemble of earth models, the technique is applicable to a wide variety of situations. For example, it may be applied to perform ‘error analysis' using the ensemble generated by a genetic algorithm, or any other direct search method.},
author = {Sambridge, Malcolm},
nodoi = {10.1046/j.1365-246x.1999.00900.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Sambridge{\_}1999(2).pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {numerical techniques,receiver functions,uncertainty quantification,waveform inversion},
mendeley-tags = {uncertainty quantification},
month = {sep},
number = {3},
pages = {727--746},
title = {{Geophysical inversion with a neighbourhood algorithm-II. Appraising the ensemble}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1046/j.1365-246x.1999.00900.x},
volume = {138},
year = {1999}
}
@article{Hansen2001,
abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
author = {Hansen, Nikolaus and Ostermeier, Andreas},
nodoi = {10.1162/106365601750190398},
file = {:D$\backslash$:/OneDrive/Articles/Evolutionary Computation/Hansen, Ostermeier{\_}2001.pdf:pdf},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {cmaes,covariance matrix adaptation,cumulation,cumulative path length control,de-,derandomized self-adaptation,evolu-,evolution strategy,randomization,self-adaptation,step size control,strategy parameter control,tion path},
mendeley-tags = {cmaes},
number = {2},
pages = {159--195},
pmid = {11382355},
title = {{Completely Derandomized Self-Adaptation in Evolution Strategies}},
nourl = {http://www.mitpressjournals.org/nodoi/abs/10.1162/106365601750190398},
volume = {9},
year = {2001}
}
@article{Poormirzaee2016,
abstract = {The refraction microtremor method has been increasingly used as an appealing tool for investigating near surface S-wave structure. However, inversion, as a main stage in processing refraction microtremor data, is challenging for most local search methods due to its high nonlinearity. With the development of data optimization approaches, fast and easier techniques can be employed for processing geophysical data. Recently, particle swarm optimization algorithm has been used in many fields of studies. Use of particle swarm optimization in geophysical inverse problems is a relatively recent development which offers many advantages in dealing with the nonlinearity inherent in such applications. In this study, the reliability and efficiency of particle swarm optimization algorithm in the inversion of refraction microtremor data were investigated. A new framework was also proposed for the inversion of refraction microtremor Rayleigh wave dispersion curves. First, particle swarm optimization code in MATLAB was developed; then, in order to evaluate the efficiency and stability of proposed algorithm, two noise-free and two noise-corrupted synthetic datasets were inverted. Finally, particle swarm optimization inversion algorithm in refraction microtremor data was applied for geotechnical assessment in a case study in the area in city of Tabriz in northwest of Iran. The S-wave structure in the study area successfully delineated. Then, for evaluation, the estimated Vs profile was compared with downhole data available around of the considered area. It could be concluded that particle swarm optimization inversion algorithm is a suitable technique for inverting microtremor waves.},
author = {Poormirzaee, Rashed},
nodoi = {10.1007/s12517-016-2701-6},
file = {:D$\backslash$:/OneDrive/Articles/Arabian Journal of Geosciences/Poormirzaee{\_}2016.pdf:pdf},
issn = {1866-7511},
journal = {Arabian Journal of Geosciences},
keywords = {Inversion,Particle swarm optimization,Refraction microtremor,Shear wave,pso},
mendeley-tags = {pso},
month = {oct},
number = {16},
pages = {673},
publisher = {Arabian Journal of Geosciences},
title = {{S-wave velocity profiling from refraction microtremor Rayleigh wave dispersion curves via PSO inversion algorithm}},
nourl = {http://dx.nodoi.org/10.1007/s12517-016-2701-6 http://link.springer.com/10.1007/s12517-016-2701-6},
volume = {9},
year = {2016}
}
@book{Neal1996,
address = {New York, NY},
author = {Neal, Radford M},
nodoi = {10.1007/978-1-4612-0745-0},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Neal{\_}1996.pdf:pdf},
isbn = {978-0-387-94724-2},
pages = {341},
publisher = {Springer New York},
series = {Lecture Notes in Statistics},
title = {{Bayesian Learning for Neural Networks}},
nourl = {http://link.springer.com/10.1007/978-1-4612-0745-0},
volume = {118},
year = {1996}
}
@article{Dai1997,
abstract = {An automatic approach is developed to pick P and {\$} arrivals from single component (l-C) recordings of local earthquake data. In this approach a back propagation neural network (BPNN) accepts a normalized segment (window of 40 samples) of absolute amplitudes from the 1-C recordings as its input pattern, calculating two output values between 0 and 1. The outputs (0,1) or (1,0) correspond to the presence of an arrival or background noise within a moving window. The two outputs form a time series. The P and {\$} arrivals are then retrieved from this series by using a threshold and a local maximum rule. The BPNN is trained by only 10 pairs of P arrivals and background noise segments from the vertical component (V-C) recordings. It can also successfully pick seismic arrivals from the horizontal components (E-W and N-S). Its performance is different for each of the three components due to strong effects of ray path and source position on the seismic waveforms. For the data from two stations of TDP3 seismic network, the success rates are 93{\%}, 89{\%}, and 83{\%} for P arrivals and 75{\%}, 91{\%}, and 87{\%} for {\$} arrivals from the V-C, E-W, and N-S recordings, respectively. The accuracy of the onset times picked fxom each individual 1-C recording is similar. Adding a constraint on the error to be 10 ms (one sample increment), 66{\%}, 59{\%} and 63{\%} of the P arrivals and 53{\%}, 61{\%}, and 58{\%} of the {\$} arrivals are picked from the V-C, E-W and N-S recordings respectively. Its performance is lower than a similar three-component picking approach but higher than other 1-C picking methods.},
author = {Dai, Hengchang and MacBeth, Colin},
nodoi = {10.1029/97JB00625},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Dai, MacBeth{\_}1997.pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {neural network,picking},
mendeley-tags = {neural network,picking},
month = {jul},
number = {B7},
pages = {15105--15113},
title = {{The application of back-propagation neural network to automatic picking seismic arrivals from single-component recordings}},
nourl = {http://nodoi.wiley.com/10.1029/97JB00625},
volume = {102},
year = {1997}
}
@article{Monteiller2005,
abstract = {Improving our understanding of crustal processes requires a better knowledge of the geometry and the position of geological bodies. In this study we have designed a method based upon double-difference relocation and tomography to image, as accurately as possible, a heterogeneous medium containing seismogenic objects. Our approach consisted not only of incorporating double difference in tomography but also partly in revisiting tomographic schemes for choosing accurate and stable numerical strategies, adapted to the use of cross-spectral time delays. We used a finite difference solution to the eikonal equation for travel time computation and a Tarantola-Valette approach for both the classical and double-difference three-dimensional tomographic inversion to find accurate earthquake locations and seismic velocity estimates. We estimated efficiently the square root of the inverse model's covariance matrix in the case of a Gaussian correlation function. It allows the use of correlation length and a priori model variance criteria to determine the optimal solution. Double-difference relocation of similar earthquakes is performed in the optimal velocity model, making absolute and relative locations less biased by the velocity model. Double-difference tomography is achieved by using high-accuracy time delay measurements. These algorithms have been applied to earthquake data recorded in the vicinity of Kilauea and Mauna Loa volcanoes for imaging the volcanic structures. Stable and detailed velocity models are obtained: the regional tomography unambiguously highlights the structure of the island of Hawaii and the double-difference tomography shows a detailed image of the southern Kilauea caldera-upper east rift zone magmatic complex.},
author = {Monteiller, Vadim and Got, Jean Luc and Virieux, Jean and Okubo, Paul},
nodoi = {10.1029/2004JB003466},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Monteiller et al.{\_}2005.pdf:pdf},
isbn = {0148-0227},
issn = {21699356},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {location},
mendeley-tags = {location},
number = {12},
pages = {1--22},
title = {{An efficient algorithm for double-difference tomography and location in heterogeneous media, with an application to the Kilauea volcano}},
volume = {110},
year = {2005}
}
@article{Eberhart2001,
abstract = {This paper focuses on the engineering and computer science aspects of developments, applications, and resources related to particle swarm optimization. Developments in the particle swarm algorithm since its origin in 1995 are reviewed. Included are brief discussions of constriction factors, inertia weights, and tracking dynamic systems. Applications, both those already developed, and promising future application areas, are reviewed. Finally, resources related to particle swarm optimization are listed, including books, Web sites, and software. A particle swarm optimization bibliography is at the end of the paper},
author = {Eberhart, R.C. and {Yuhui Shi}},
nodoi = {10.1109/CEC.2001.934374},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)/Eberhart, Yuhui Shi{\_}2001.pdf:pdf},
isbn = {0-7803-6657-3},
journal = {Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)},
keywords = {Acceleration,Application software,Bibliographies,Books,Computer science,Evolutionary computation,Particle swarm optimization,Particle tracking,Power system dynamics,Writing,constriction factors,dynamic systems,evolutionary computation,inertia weights,particle swarm optimization,pso,tracking},
mendeley-tags = {pso},
pages = {81--86},
title = {{Particle swarm optimization: developments, applications and resources}},
nourl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=934374},
volume = {1},
year = {2001}
}
@misc{Gong2015,
abstract = {The increasing complexity of real-world optimization problems raises new challenges to evolutionary computation. Responding to these challenges, distributed evolutionary computation has received considerable attention over the past decade. This article provides a comprehensive survey of the state-of-the-art distributed evolutionary algorithms and models, which have been classified into two groups according to their task division mechanism. Population-distributed models are presented with master-slave, island, cellular, hierarchical, and pool architectures, which parallelize an evolution task at population, individual, or operation levels. Dimension-distributed models include coevolution and multi-agent models, which focus on dimension reduction. Insights into the models, such as synchronization, homogeneity, communication, topology, speedup, advantages and disadvantages are also presented and discussed. The study of these models helps guide future development of different and/or improved algorithms. Also highlighted are recent hotspots in this area, including the cloud and MapReduce-based implementations, GPU and CUDA-based implementations, distributed evolutionary multiobjective optimization, and real-world applications. Further, a number of future research directions have been discussed, with a conclusion that the development of distributed evolutionary computation will continue to flourish.},
author = {Gong, Yue Jiao and Chen, Wei Neng and Zhan, Zhi Hui and Zhang, Jun and Li, Yun and Zhang, Qingfu and Li, Jing Jing},
booktitle = {Applied Soft Computing Journal},
nodoi = {10.1016/j.asoc.2015.04.061},
file = {:D$\backslash$:/OneDrive/Articles/Applied Soft Computing Journal/Gong et al.{\_}2015.pdf:pdf},
issn = {15684946},
keywords = {Coevolutionary computation,Distributed evolutionary computation,Evolutionary algorithms,Global optimization,Multiobjective optimization,hpc},
mendeley-tags = {hpc},
month = {sep},
pages = {286--300},
title = {{Distributed evolutionary algorithms and their models: A survey of the state-of-the-art}},
nourl = {http://linkinghub.elsevier.com/retrieve/pii/S1568494615002987},
volume = {34},
year = {2015}
}
@article{Eyre2015,
abstract = {Understanding the source mechanisms of microseismic events is important for understanding the fracturing behavior and evolving stress field within a reservoir, knowledge of which can help to improve production and minimize seismic risk. The most common method for calculating the source mechanisms is momenttensor inversion, which can provide the magnitudes, modes, and orientations of fractures. An overview of three common methods includes their advantages and limitations: the first-arrival polarity method, amplitude methods, and the full-waveform method. The first-arrival method is the quickest to implement but also the crudest, likely producing the least reliable results. Amplitude methods are also relatively simple but can better constrain the inversion because of the increased number of observations, especially those using S/P amplitude ratios. Full-waveform methods can provide results of very good quality, including source-time functions, but involve much more complex and expensive calculations and rely on accurate seismic-velocity models.},
author = {Eyre, Thomas S and Baan, Mirko Van Der},
nodoi = {10.1190/tle34080882.1},
file = {:D$\backslash$:/OneDrive/Articles/The Leading Edge/Eyre, Baan{\_}2015.pdf:pdf},
isbn = {1070-485X$\backslash$r1938-3789},
issn = {1070-485X},
journal = {The Leading Edge},
keywords = {microseismic,moment tensor},
mendeley-tags = {microseismic,moment tensor},
number = {August},
pages = {882--888},
title = {{Overview of moment-tensor inversion of microseismic events}},
year = {2015}
}
@inproceedings{Eberhart2000,
abstract = {The performance of particle swarm optimization using an inertia weight is compared with performance using a constriction factor. Five benchmark functions are used for the comparison. It is concluded that the best approach is to use the constriction factor while limiting the maximum velocity Vmax to the dynamic range of the variable Xmax on each dimension. This approach provides performance on the benchmark functions superior to any other published results known by the authors},
author = {Eberhart, R.C. and Shi, Y.},
booktitle = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
nodoi = {10.1109/CEC.2000.870279},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)/Eberhart, Shi{\_}2000.pdf:pdf},
isbn = {0-7803-6375-2},
issn = {13272314},
keywords = {pso},
mendeley-tags = {pso},
number = {7},
pages = {84--88},
publisher = {IEEE},
title = {{Comparing inertia weights and constriction factors in particle swarm optimization}},
nourl = {http://ieeexplore.ieee.org/ielx5/6997/18852/00870279.pdf?tp={\&}arnumber=870279{\&}isnumber=18852{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=870279{\&}tag=1 http://ieeexplore.ieee.org/document/870279/},
volume = {1},
year = {2000}
}
@article{Whitley1994,
abstract = {This tutorial covers the canonical genetic algorithm as well as more experimental forms of genetic algorithms, including parallel island models and parallel cellular genetic algorithms. The tutorial also illustrates genetic search by hyperplane sampling. The theoretical foun- dations of genetic algorithms are reviewed, include the schema theorem as well as recently developed exact models of the canonical genetic algorithm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Whitley, Darrell},
nodoi = {10.1007/BF00175354},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/OneDrive/Articles/Statistics and Computing/Whitley{\_}1994.pdf:pdf},
isbn = {1101001100},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Genetic algorithms,ga,parallel algorithms,search},
mendeley-tags = {ga},
month = {jun},
number = {2},
pages = {65--85},
pmid = {848},
title = {{A genetic algorithm tutorial}},
nourl = {http://link.springer.com/10.1007/BF00175354},
volume = {4},
year = {1994}
}
@article{Zhang2003,
abstract = {We have developed an automatic P-wave arrival detection and picking algorithm based on the wavelet transform and Akaike information criteria (AIC) picker. Wavelet coefficients at high resolutions show the fine structure of the time series, and those at low resolutions characterize its coarse features. Primary features such as the P-wave arrival are retained over several resolution scales, whereas sec-ondary features such as scattered arrivals decay quickly at lower resolutions. We apply the discrete wavelet transform to single-component seismograms through a series of sliding time windows. In each window the AIC autopicker is applied to the thresholded absolute wavelet coefficients at different scales, and we compare the consistency of those picks to determine whether a P-wave arrival has been detected in the given time window. The arrival time is then determined using the AIC picker on the time window chosen by the wavelet transform. We test our method on regional earthquake data from the Dead Sea Rift region and local earthquake data from the Parkfield, California region. We find that 81{\%} of picks are within 0.2-sec of the corresponding analyst pick for the Dead Sea dataset, and 93{\%} of picks are within 0.1 sec of the analyst pick for the Parkfield dataset. We attribute the lower percentage of agreement for the Dead Sea dataset to the substantially lower signal-to-noise ratio of those data, and the likelihood that some percentage of the analyst picks are in error.},
author = {Zhang, Haijiang and Thurber, Clifford and Rowe, Charlotte},
nodoi = {10.1785/0120020241},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Zhang, Thurber, Rowe{\_}2003.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
keywords = {automatic picking,microseismic},
mendeley-tags = {automatic picking,microseismic},
number = {5},
pages = {1904--1912},
title = {{Automatic P-wave arrival detection and picking with multiscale wavelet analysis for single-component recordings}},
volume = {93},
year = {2003}
}
@inproceedings{Aminzadeh2011,
abstract = {Earthquake data collected from the Geysers geothermal field from the period 2006 to 2010 was studied. An artificial neural network (ANN) based autopicker was developed to study and compare the picks with available autopickers in use at Lawrence Berkeley National Lab (LBNL) as well as in house autopicker at USC. The results indicate the following: 1. The ANN autopicker is able to generate good picks in most situations where other autopickers work. 2. The ANN autopicker is able to pick in situations where the other autopickers fail to generate good picks or do not generate any picks at all due to noise. 3. STA/LTA ratio and frequency based attributes show highest weight in the trained ANN. Despite the low misclassification error observed in the final results (3.7{\%}), the ANN autopicker failed in some situations. The next step is to incorporate improved attribute sets and look at a hybrid neuro-fuzzy approach to tide over the limitations of the present network.},
author = {Aminzadeh, Fred and Maity, Debotyam and Tafti, Tayeb A and Brouwer, Friso},
booktitle = {SEG Technical Program Expanded Abstracts 2011},
nodoi = {10.1190/1.3627514},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2011/Aminzadeh et al.{\_}2011.pdf:pdf},
issn = {10523812},
keywords = {attributes,geothermal,machine learning,microseismic,neural network,neural networks,noise,picking},
mendeley-tags = {machine learning,neural network,picking},
month = {jan},
pages = {1623--1626},
publisher = {Society of Exploration Geophysicists},
title = {{Artificial neural network based autopicker for micro‐earthquake data}},
nourl = {http://library.seg.org/nodoi/abs/10.1190/1.3627514},
year = {2011}
}
@inproceedings{Mussi2011,
abstract = {This paper describes our latest implementation of Particle Swarm Optimization (PSO) with simple ring topology for modern Graphic Processing Units (GPUs). To achieve both the fastest execution time and the best performance, we designed a parallel version of the algorithm, as fine-grained as possible, without introducing explicit synchronization mechanisms among the particles' evolution processes. The results we obtained show a significant speed-up with respect to both the sequential version of the algorithm run on an up-to-date CPU and our previously developed parallel implementation within the nVIDIA™ CUDA™ architecture. Copyright 2011 ACM.},
address = {New York, New York, USA},
author = {Mussi, Luca and Nashed, Youssef S.G. and Cagnoni, Stefano},
booktitle = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
nodoi = {10.1145/2001576.2001786},
file = {:D$\backslash$:/OneDrive/Articles/Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11/Mussi, Nashed, Cagnoni{\_}2011.pdf:pdf},
isbn = {9781450305570},
keywords = {implementation,mization,parallelization,particle swarm opti-,pso,speed-up technique},
mendeley-tags = {pso},
pages = {1555},
publisher = {ACM Press},
title = {{GPU-based asynchronous particle swarm optimization}},
nourl = {http://portal.acm.org/citation.cfm?doid=2001576.2001786},
year = {2011}
}
@inproceedings{Galvis2016,
abstract = {This work is aimed to detect and classify surface waves by characterizing the movement of ground particles when seis-mic waves are produced and, based on the information from multicomponent seismic records, differentiating how seismic data behaves when surface waves is present. Seismic attributes were used in order to enhance the frequency and polarization information of seismic wave-fields in seismic records; these were computed from short-time windows of multicomponent seismic data in order to build the feature vectors. The unsupervised pattern recognition technique K-means was used to separate surface waves areas from reflection seismic data.},
author = {Galvis, Ivan Javier S{\'{a}}nchez and Villa-Acu{\~{n}}a, Yenni and Bueno, Daniel Alfonso Sierra and Gualdr{\'{o}}n, C{\'{e}}sar Antonio Duarte and Agudelo, William},
booktitle = {SEG Technical Program Expanded Abstracts 2016},
nodoi = {10.1190/segam2016-13961709.1},
file = {:D$\backslash$:/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2016/Galvis et al.{\_}2016.pdf:pdf},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
month = {sep},
pages = {4961--4965},
publisher = {Society of Exploration Geophysicists},
title = {{3-C surface-waves detection and classification via unsupervised pattern recognition from seismic attributes}},
nourl = {http://library.seg.org/nodoi/10.1190/segam2016-13961709.1},
year = {2016}
}
@article{Malinverno2004,
abstract = {A common way to account for uncertainty in inverse problems is to apply Bayes' rule and obtain a posterior distribution of the quantities of interest given a set of measurements. A conventional Bayesian treatment, however, requires assuming specific values for parameters of the prior distribution and of the distribution of the measurement errors (e.g., the standard deviation of the errors). In practice, these parameters are often poorly known a priori, and choosing a particular value is often problematic. Moreover, the posterior uncertainty is computed assuming that these parameters are fixed; if they are not well known a priori, the posterior uncertainties have dubious value.This paper describes extensions to the conventional Bayesian treatment that assign uncertainty to the parameters defining the prior distribution and the distribution of the measurement errors. These extensions are known in the statistical literature as “empirical Bayes” and “hierarchical Bayes.” We demonstrate the practical application of these approaches to a simple linear inverse problem: using seismic traveltimes measured by a receiver in a well to infer compressional wave slowness in a 1D earth model. These procedures do not require choosing fixed values for poorly known parameters and, at most, need a realistic range (e.g., a minimum and maximum value for the standard deviation of the measurement errors). Inversion is thus made easier for general users, who are not required to set parameters they know little about.},
author = {Malinverno, A. and Briggs, V. A.},
nodoi = {10.1190/1.1778243},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Malinverno, Briggs{\_}2004.pdf:pdf},
isbn = {00168033},
issn = {00168033},
journal = {Geophysics},
keywords = {bayesian},
mendeley-tags = {bayesian},
number = {4},
pages = {1005--10016},
title = {{Expanded uncertainty quantification in inverse problems: Hierarchical Bayes and empirical Bayes}},
volume = {69},
year = {2004}
}
@inproceedings{Evers2009,
abstract = {Particle swarm optimization (PSO) is known to suffer from stagnation once particles have prematurely converged to any particular region of the search space. The proposed regrouping PSO (RegPSO) avoids the stagnation problem by automatically triggering swarm regrouping when premature convergence is detected. This mechanism liberates particles from sub-optimal solutions and enables continued progress toward the true global minimum. Particles are regrouped within a range on each dimension proportional to the degree of uncertainty implied by the maximum deviation of any particle from the globally best position. This is a computationally simple yet effective addition to the computationally simple PSO algorithm. Experimental results show that the proposed RegPSO successfully reduces each popular benchmark tested to its approximate global minimum.},
author = {Evers, George I. and {Ben Ghalia}, Mounir},
booktitle = {2009 IEEE International Conference on Systems, Man and Cybernetics},
nodoi = {10.1109/ICSMC.2009.5346625},
file = {:D$\backslash$:/OneDrive/Articles/2009 IEEE International Conference on Systems, Man and Cybernetics/Evers, Ben Ghalia{\_}2009.pdf:pdf},
isbn = {978-1-4244-2793-2},
keywords = {automatic regrouping mechanism,convergence,maintaining,particle swarm optimization,premature,pso,stagnation},
mendeley-tags = {pso},
month = {oct},
pages = {3901--3908},
publisher = {IEEE},
title = {{Regrouping particle swarm optimization: A new global optimization algorithm with improved performance consistency across benchmarks}},
nourl = {http://ieeexplore.ieee.org/document/5346625/},
year = {2009}
}
@article{Bodin2009,
abstract = {The reversible jump algorithm is a statistical method for Bayesian inference with a variable number of unknowns. Here, we apply this method to the seismic tomography problem. The approach lets us consider the issue of model parametrization (i.e. the way of discretizing the velocity field) as part of the inversion process. The model is parametrized using Voronoi cells with mobile geometry and number. The size, position and shape of the cells defining the velocity model are directly determined by the data. The inverse problem is tackled within a Bayesian framework and explicit regularization of model parameters is not required. The mobile position and number of cells means that global damping procedures, controlled by an optimal regularization parameter, are avoided. Many velocity models with variable numbers of cells are generated via a transdimensional Markov chain and information is extracted from the ensemble as a whole. As an aid to interpretation we visualize the expected earth model that is obtained via Monte Carlo integration in a straightforward manner. The procedure is particularly adept at imaging rapid changes or discontinuities in wave speed. While each velocity model in the final ensemble consists of many discontinuities at cell boundaries, these are smoothed out in the averaged ensemble solution while those required by the data are reinforced. The ensemble of models can also be used to produce uncertainty estimates and experiments with synthetic data suggest that they represent actual uncertainty surprisingly well. We use the fast marching method in order to iteratively update the ray geometry and account for the non-linearity of the problem. The method is tested here with synthetic data in a 2-D application and compared with a subspace method that is a more standard matrix-based inversion scheme. Preliminary results illustrate the advantages of the reversible jump algorithm. A real data example is also shown where a tomographic image of Rayleigh wave group velocity for the Australian continent is constructed together with uncertainty estimates.},
author = {Bodin, Thomas and Sambridge, Malcolm},
nodoi = {10.1111/j.1365-246X.2009.04226.x},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Bodin, Sambridge{\_}2009.pdf:pdf},
isbn = {0956540X$\backslash$r1365246X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Australia,Computational seismology,Inverse theory,Probability distribution,Seismic tomography,Tomography,rjmcmc},
mendeley-tags = {rjmcmc},
month = {sep},
number = {3},
pages = {1411--1436},
title = {{Seismic tomography with the reversible jump algorithm}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1111/j.1365-246X.2009.04226.x},
volume = {178},
year = {2009}
}
@article{Ge2003,
abstract = {Iterative algorithms are of particular importance in source location as they provide a much more flexible means to solve nonlinear equations, which is essential in order to deal with a wide range of practical problems. The most important iterative algorithms are Geiger's method and the Simplex method. This article provides an overview of iterative algorithms as well as an in-depth analysis of several major methods.},
author = {Ge, Maochen},
file = {:D$\backslash$:/OneDrive/Articles/Journal of acoustic emission/Ge{\_}2003.pdf:pdf},
journal = {Journal of acoustic emission},
keywords = {location},
mendeley-tags = {location},
pages = {29--51},
title = {{Analysis of source location algorithms, Part II: Iterative methods}},
nourl = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:ANALYSIS+OF+SOURCE+LOCATION+ALGORITHMS+Part+II+:+Iterative+methods{\#}0},
volume = {21},
year = {2003}
}
@article{Whitley1999,
abstract = {Parallel Genetic Algorithms have often been reported to yield better performance than Genetic Algorithms which use a single large panmictic population. In the case of the Island Model genetic algorithm, it has been informally argued that having multiple subpopulations helps to preserve genetic diversity, since each island can potentially follow a different search trajectory through the search space. It is also possible that since linearly separable problems are often used to test Genetic Algorithms, that Island Models may simply be particularly well suited to exploiting the separable nature of the test problems. We explore this possibility by using the infinite population models of simple genetic algorithms to study how Island Models can track multiple search trajectories. We also introduce a simple model for better understanding when Island Model genetic algorithms may have an advantage when processing some test problems. We provide empirical results for both linearly separa...},
author = {Whitley, Darrell and Rana, Soraya and Heckendorn, Robert B},
nodoi = {10.1.1.36.7225},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Computing and Information Technology/Whitley, Rana, Heckendorn{\_}1999.pdf:pdf},
issn = {1330-1136},
journal = {Journal of Computing and Information Technology},
keywords = {hpc},
mendeley-tags = {hpc},
pages = {33--47},
title = {{The island model genetic algorithm: On separability, population size and convergence}},
nourl = {http://citeseerx.ist.psu.edu/viewdoc/download?nodoi=10.1.1.36.7225{\&}rep=rep1{\&}type=pdf},
volume = {7},
year = {1999}
}
@article{Bottero2016,
abstract = {Markov chain Monte Carlo sampling methods are widely used for non-linear Bayesian inver-sion where no analytical expression for the forward relation between data and model parameters is available. Contrary to the linear(ized) approaches, they naturally allow to evaluate the un-certainties on the model found. Nevertheless their use is problematic in high-dimensional model spaces especially when the computational cost of the forward problem is significant and/or the a posteriori distribution is multimodal. In this case, the chain can stay stuck in one of the modes and hence not provide an exhaustive sampling of the distribution of interest. We present here a still relatively unknown algorithm that allows interaction between several Markov chains at different temperatures. These interactions (based on importance resampling) ensure a robust sampling of any posterior distribution and thus provide a way to efficiently tackle complex fully non-linear inverse problems. The algorithm is easy to implement and is well adapted to run on parallel supercomputers. In this paper, the algorithm is first introduced and applied to a synthetic multimodal distribution in order to demonstrate its robustness and efficiency compared to a simulated annealing method. It is then applied in the framework of first arrival traveltime seismic tomography on real data recorded in the context of hydraulic fracturing. To carry out this study a wavelet-based adaptive model parametrization has been used. This allows to integrate the a priori information provided by sonic logs and to reduce optimally the dimension of the problem.},
author = {Bottero, Alexis and Gesret, Alexandrine and Romary, Thomas and Noble, Mark and Maisons, Christophe},
nodoi = {10.1093/gji/ggw272},
file = {:D$\backslash$:/OneDrive/Articles/Geophysical Journal International/Bottero et al.{\_}2016.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {inverse theory,mcmc,probability distributions,tomography,wavelet transform},
mendeley-tags = {mcmc},
month = {oct},
number = {1},
pages = {374--392},
title = {{Stochastic seismic tomography by interacting Markov chains}},
nourl = {https://academic.oup.com/gji/article-lookup/nodoi/10.1093/gji/ggw272},
volume = {207},
year = {2016}
}
@article{Sedlak2008,
abstract = {The information of first-arrival time of acoustic emission (AE) signal is important in event location, event identification and source mechanism analysis. Manual picks are time-consuming and sometimes subjective. Several approaches are used in practice. New first arrival automatic determination technique of AE signals in thin metal plates is presented. Based on Akaike infor-mation criterion (AIC), proposed algorithm of the first-arrival detection uses the specific charac-teristic function, which is sensitive to change of frequency in contrast to others such as envelope of the signal. The approach was tested on real AE data recorded by a four-channel recording system. The results were compared to manual picks and to other AIC approach. It is shown that our two-step AIC picker is a reliable tool to identify the arrival time for AE signals.},
author = {Sedlak, Petr and Hirose, Yuichiro and Enoki, Manabu and Sikula, Josef},
file = {:D$\backslash$:/OneDrive/Articles/Journal of Acoustic Emission/Sedlak et al.{\_}2008.pdf:pdf},
journal = {Journal of Acoustic Emission},
keywords = {akaike information criterion,arrival time detection,multi-layer plate,picking},
mendeley-tags = {picking},
pages = {182--188},
title = {{Arrival Time Detection in Thin Multilayer Plates on the Basis of Akaike Information Criterion}},
volume = {26},
year = {2008}
}
@article{Chmiel2016,
abstract = {We have investigated the use of ambient-noise data to extract phase and group velocities from surface-noise sources in a microseismic monitoring context. The data were continuously recorded on 44 patch arrays with an interpatch distance on the order of 1 km. Typically, a patch-array design consists of a few tens of patches, each containing 48 strings of 12 single-vertical-component geophones densely distributed within the patch area. The specificity of the patch-array design allows seismic analysis at two different scales. Within each patch, highly coherent signals at small distances provide phase information at high frequency (up to 10 Hz), from which surface-wave phase velocities can be extracted. Between the pairs of patches, surface-wave group-velocity maps can be built using correctly identified and localized surface-noise sources. The technique can be generalized to every patch pair using different noise sources identified at the surface. We note that the incoherent but localized noise sources accelerate the convergence of the noise-correlation functions. This opens the route to passive seismic monitoring of the near surface from repetitive inversion of phase- and group-velocity maps.},
author = {Chmiel, Malgorzata and Roux, Philippe and Bardainne, Thomas},
nodoi = {10.1190/geo2016-0027.1},
file = {:D$\backslash$:/OneDrive/Articles/Geophysics/Chmiel, Roux, Bardainne{\_}2016.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
month = {nov},
number = {6},
pages = {KS231--KS240},
title = {{Extraction of phase and group velocities from ambient surface noise in a patch-array configuration}},
nourl = {http://library.seg.org/nodoi/10.1190/geo2016-0027.1},
volume = {81},
year = {2016}
}
@article{Waldhauser2000,
abstract = {We have developed an efficient method to determine high-resolution hypocenter locations over large distances. The location method incorporates ordinary absolute travel-time measurements and/or cross-correlation P-and S-wave differential travel-time measurements. Residuals between observed and theoretical travel-time differences (or double-differences) are minimized for pairs of earthquakes at each station while linking together all observed event-station pairs. A least-squares solu- tion is found by iteratively adjusting the vector difference between hypocentral pairs. The double-difference algorithm minimizes errors due to unmodeled velocity struc- ture without the use of station corrections. Because catalog and cross-correlation data are combined into one system of equations, interevent distances within multiplets are determined to the accuracy of the cross-correlation data, while the relative lo- cations between multiplets and uncorrelated events are simultaneously determined to the accuracy of the absolute travel-time data. Statistical resampling methods are used to estimate data accuracy and location errors. Uncertainties in double-difference locations are improved by more than an order of magnitude compared to catalog locations. The algorithm is tested, and its performance is demonstrated on two clus- ters of earthquakes located on the northern Hayward fault, California. There it col- lapses the diffuse catalog locations into sharp images of seismicity and reveals hor- izontal lineations of hypocenters that define the narrow regions on the fault where stress is released by brittle failure.},
author = {Waldhauser, F. and Ellsworth, W. L.},
nodoi = {10.1785/0120000006},
file = {:D$\backslash$:/OneDrive/Articles/Bulletin of the Seismological Society of America/Waldhauser, Ellsworth{\_}2000.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
keywords = {double-difference,location},
mendeley-tags = {double-difference,location},
number = {6},
pages = {1353--1368},
title = {{A Double-difference Earthquake location algorithm: Method and application to the Northern Hayward Fault, California}},
volume = {90},
year = {2000}
}
@article{Drosinos2004,
abstract = {Summary form only given. We compare the performance of three programming paradigms for the parallelization of nested loop algorithms onto SMP clusters. More specifically, we propose three alternative models for tiled nested loop algorithms, namely a pure message passing paradigm, as well as two hybrid ones, that implement communication both through message passing and shared memory access. The hybrid models adopt an advanced hyperplane scheduling scheme, that allows both for minimal thread synchronization, as well as for pipelined execution with overlapping of computation and communication phases. We focus on the experimental evaluation of all three models, and test their performance against several iteration spaces and parallelization grains with the aid of a typical microkernel benchmark. We conclude that the hybrid models can in some cases be more beneficial compared to the monolithic pure message passing model, as they exploit better the configuration characteristics of an hierarchical parallel platform, such as an SMP cluster.},
author = {Drosinos, N. and Koziris, N.},
nodoi = {10.1109/IPDPS.2004.1302919},
file = {:D$\backslash$:/OneDrive/Articles/18th International Parallel and Distributed Processing Symposium, 2004. Proceedings/Drosinos, Koziris{\_}2004.pdf:pdf},
isbn = {0-7695-2132-0},
journal = {18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.},
keywords = {Clustering algorithms,Concurrent computing,Electronic mail,Laboratories,MPI,Message passing,OpenMP parallelization,Parallel programming,Processor scheduling,SMP clusters,Systems engineering and theory,Testing,Yarn,distributed shared memory systems,hybrid model,hyperplane scheduling scheme,message passing,message passing paradigm,microkernel benchmark,nested loop algorithm,open systems,pipeline processing,pipelined execution,shared memory access,synchronisation,thread synchronization,workstation clusters},
number = {C},
pages = {15--24},
title = {{Performance comparison of pure MPI vs hybrid MPI-OpenMP parallelization models on SMP clusters}},
nourl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1302919},
volume = {00},
year = {2004}
}
@phdthesis{Bakari2015,
author = {Bakari, Issam},
file = {:D$\backslash$:/OneDrive/Articles/Unknown/Bakari{\_}2015.pdf:pdf},
keywords = {microseismic},
mendeley-tags = {microseismic},
pages = {85},
school = {Universit{\'{e}} du Qu{\'{e}}bec},
title = {{Localisation des {\'{e}}picentres des munitions non explos{\'{e}}es dans les exercices de tirs}},
year = {2015}
}
